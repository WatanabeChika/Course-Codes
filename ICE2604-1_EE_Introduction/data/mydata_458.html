<!DOCTYPE html>
<html>
<head>
<title>heapsort</title>
</head>
<body>
<div class="mw-parser-output">
<p class="mw-empty-elt">
</p>
<style data-mw-deduplicate="TemplateStyles:r1066479718">.mw-parser-output .infobox-subbox{padding:0;border:none;margin:-3px;width:auto;min-width:100%;font-size:100%;clear:none;float:none;background-color:transparent}.mw-parser-output .infobox-3cols-child{margin:auto}.mw-parser-output .infobox .navbar{font-size:100%}body.skin-minerva .mw-parser-output .infobox-header,body.skin-minerva .mw-parser-output .infobox-subheader,body.skin-minerva .mw-parser-output .infobox-above,body.skin-minerva .mw-parser-output .infobox-title,body.skin-minerva .mw-parser-output .infobox-image,body.skin-minerva .mw-parser-output .infobox-full-data,body.skin-minerva .mw-parser-output .infobox-below{text-align:center}</style><table class="infobox"><caption class="infobox-title">Heapsort</caption><tbody><tr><td class="infobox-image" colspan="2"><img alt="Sorting heapsort anim.gif" data-file-height="214" data-file-width="280" decoding="async" height="214" src="//upload.wikimedia.org/wikipedia/commons/1/1b/Sorting_heapsort_anim.gif" width="280"/></td></tr><tr><th class="infobox-label" scope="row">Class</th><td class="infobox-data">Sorting algorithm</td></tr><tr><th class="infobox-label" scope="row">Data structure</th><td class="infobox-data">Array</td></tr><tr><th class="infobox-label" scope="row">Worst-case performance</th><td class="infobox-data"><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(n\log n)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(n\log n)}</annotation>
</semantics>
</math></span><img alt="O(n\log n)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9d2320768fb54880ca4356e61f60eb02a3f9d9f1" style="vertical-align: -0.838ex; width:10.118ex; height:2.843ex;"/></span></td></tr><tr><th class="infobox-label" scope="row">Best-case performance</th><td class="infobox-data"><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(n\log n)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(n\log n)}</annotation>
</semantics>
</math></span><img alt="O(n\log n)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9d2320768fb54880ca4356e61f60eb02a3f9d9f1" style="vertical-align: -0.838ex; width:10.118ex; height:2.843ex;"/></span> (distinct keys)<br/>or <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(n)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(n)}</annotation>
</semantics>
</math></span><img alt="O(n)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/34109fe397fdcff370079185bfdb65826cb5565a" style="vertical-align: -0.838ex; width:4.977ex; height:2.843ex;"/></span> (equal keys)</td></tr><tr><th class="infobox-label" scope="row">Average performance</th><td class="infobox-data"><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(n\log n)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(n\log n)}</annotation>
</semantics>
</math></span><img alt="O(n\log n)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9d2320768fb54880ca4356e61f60eb02a3f9d9f1" style="vertical-align: -0.838ex; width:10.118ex; height:2.843ex;"/></span></td></tr><tr><th class="infobox-label" scope="row">Worst-case space complexity</th><td class="infobox-data"><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(n)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(n)}</annotation>
</semantics>
</math></span><img alt="O(n)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/34109fe397fdcff370079185bfdb65826cb5565a" style="vertical-align: -0.838ex; width:4.977ex; height:2.843ex;"/></span> total <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(1)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(1)}</annotation>
</semantics>
</math></span><img alt="O(1)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e66384bc40452c5452f33563fe0e27e803b0cc21" style="vertical-align: -0.838ex; width:4.745ex; height:2.843ex;"/></span> auxiliary</td></tr></tbody></table>
<p>In computer science, <b>heapsort</b> is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like selection sort, heapsort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region in a heap data structure to more quickly find the largest element in each step.<sup class="reference" id="cite_ref-1">[1]</sup>
</p><p>Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case <span class="texhtml">O(<i>n</i> log <i>n</i>)</span> runtime (and as such is used by Introsort as a fallback should it detect that quicksort is becoming degenerate). Heapsort is an in-place algorithm, but it is not a stable sort.
</p><p>Heapsort was invented by J. W. J. Williams in 1964.<sup class="reference" id="cite_ref-2">[2]</sup> This was also the birth of the heap, presented already by Williams as a useful data structure in its own right.<sup class="reference" id="cite_ref-brass_3-0">[3]</sup> In the same year, Robert W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.<sup class="reference" id="cite_ref-brass_3-1">[3]</sup>
</p>

<h2><span class="mw-headline" id="Overview">Overview</span><span class="mw-editsection"></span></h2>
<p>The heapsort algorithm can be divided into two parts.
</p><p>In the first step, a heap is built out of the data (see Binary heap § Building a heap). The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions. For a zero-based array, the root node is stored at index 0; if <code>i</code> is the index of the current node, then
</p>
<pre>  iParent(i)     = floor((i-1) / 2) where floor functions map a real number to the largest leading integer.
  iLeftChild(i)  = 2*i + 1
  iRightChild(i) = 2*i + 2
</pre>
<p>In the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.
</p><p>Heapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed here. The heap's invariant is preserved after each extraction, so the only cost is that of extraction.
</p>
<h2><span class="mw-headline" id="Algorithm">Algorithm</span><span class="mw-editsection"></span></h2>
<p>The heapsort algorithm involves preparing the list by first turning it into a max heap. The algorithm then repeatedly swaps the first value of the list with the last value, decreasing the range of values considered in the heap operation by one, and sifting the new first value into its position in the heap. This repeats until the range of considered values is one value in length.
</p><p>The steps are:
</p>
<ol><li>Call the <code class="mw-highlight mw-highlight-lang-text mw-content-ltr" dir="ltr" id="" style="">buildMaxHeap()</code> function on the list. Also referred to as <code class="mw-highlight mw-highlight-lang-text mw-content-ltr" dir="ltr" id="" style="">heapify()</code>, this builds a heap from a list in <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(n)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(n)}</annotation>
</semantics>
</math></span><img alt="O(n)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/34109fe397fdcff370079185bfdb65826cb5565a" style="vertical-align: -0.838ex; width:4.977ex; height:2.843ex;"/></span> operations.</li>
<li>Swap the first element of the list with the final element. Decrease the considered range of the list by one.</li>
<li>Call the <code class="mw-highlight mw-highlight-lang-text mw-content-ltr" dir="ltr" id="" style="">siftDown()</code> function on the list to sift the new first element to its appropriate index in the heap.</li>
<li>Go to step (2) unless the considered range of the list is one element.</li></ol>
<p>The <code class="mw-highlight mw-highlight-lang-text mw-content-ltr" dir="ltr" id="" style="">buildMaxHeap()</code> operation is run once, and is <span class="texhtml">O(<i>n</i>)</span> in performance. The <code class="mw-highlight mw-highlight-lang-text mw-content-ltr" dir="ltr" id="" style="">siftDown()</code> function is <span class="texhtml">O(log <i>n</i>)</span>, and is called <span class="texhtml mvar" style="font-style:italic;">n</span> times. Therefore, the performance of this algorithm is <span class="texhtml">O(<i>n</i> + <i>n</i> log <i>n</i>) = O(<i>n</i> log <i>n</i>)</span>.
</p>
<h3><span class="mw-headline" id="Pseudocode">Pseudocode</span><span class="mw-editsection"></span></h3>
<p>The following is a simple way to implement the algorithm in pseudocode. Arrays are zero-based and <code>swap</code> is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at <code>a[0]</code>, while at the end of the sort, the largest element is in <code>a[end]</code>.
</p>
<pre><b>procedure</b> heapsort(a, count) <b>is</b>
    <b>input:</b> an unordered array <i>a</i> of length <i>count</i>
 
    <i>(Build the heap in array a so that largest value is at the root)</i>
    heapify(a, count)

    <i>(The following loop maintains the invariants that a[0:end] is a heap and every element</i>
     <i>beyond end is greater than everything before it (so a[end:count] is in sorted order))</i>
    end ← count - 1
    <b>while</b> end &gt; 0 <b>do</b>
        <i>(a[0] is the root and largest value. The swap moves it in front of the sorted elements.)</i>
        swap(a[end], a[0])
        <i>(the heap size is reduced by one)</i>
        end ← end - 1
        <i>(the swap ruined the heap property, so restore it)</i>
        siftDown(a, 0, end)
</pre>
<p>The sorting routine uses two subroutines, <code>heapify</code> and <code>siftDown</code>. The former is the common in-place heap construction routine, while the latter is a common subroutine for implementing <code>heapify</code>.
</p>
<pre><i>(Put elements of 'a' in heap order, in-place)</i>
<b>procedure</b> heapify(a, count) <b>is</b>
    <i>(start is assigned the index in 'a' of the last parent node)</i>
    <i>(the last element in a 0-based array is at index count-1; find the parent of that element)</i>
    start ← iParent(count-1)
    
    <b>while</b> start ≥ 0 <b>do</b>
        <i>(sift down the node at index 'start' to the proper place such that all nodes below</i>
        <i> the start index are in heap order)</i>
        siftDown(a, start, count - 1)
        <i>(go to the next parent node)</i>
        start ← start - 1
    <i>(after sifting down the root all nodes/elements are in heap order)</i>

<i>(Repair the heap whose root element is at index 'start', assuming the heaps rooted at its children are valid)</i>
<b>procedure</b> siftDown(a, start, end) <b>is</b>
    root ← start

    <b>while</b> iLeftChild(root) ≤ end <b>do</b>    <i>(While the root has at least one child)</i>
        child ← iLeftChild(root)   <i>(Left child of root)</i>
        swap ← root                <i>(Keeps track of child to swap with)</i>

        <b>if</b> a[swap] &lt; a[child] <b>then</b>
            swap ← child
        <i>(If there is a right child and that child is greater)</i>
        <b>if</b> child+1 ≤ end <b>and</b> a[swap] &lt; a[child+1] <b>then</b>
            swap ← child + 1
        <b>if</b> swap = root <b>then</b>
            <i>(The root holds the largest element. Since we assume the heaps rooted at the</i>
            <i> children are valid, this means that we are done.)</i>
            <b>return</b>
        <b>else</b>
            Swap(a[root], a[swap])
            root ← swap          <i>(repeat to continue sifting down the child now)</i>
</pre>
<p>The <code>heapify</code> procedure can be thought of as building a heap from the bottom up by successively sifting downward to establish the heap property. An alternative version (shown below) that builds the heap top-down and sifts upward may be simpler to understand. This <code>siftUp</code> version can be visualized as starting with an empty heap and successively inserting elements, whereas the <code>siftDown</code> version given above treats the entire input array as a full but "broken" heap and "repairs" it starting from the last non-trivial sub-heap (that is, the last parent node).
</p>

<p>Also, the <code>siftDown</code> version of heapify has <span class="texhtml"><i>O</i>(<i>n</i>)</span> time complexity, while the <code>siftUp</code> version given below has <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span> time complexity due to its equivalence with inserting each element, one at a time, into an empty heap.<sup class="reference" id="cite_ref-4">[4]</sup>
This may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never affects asymptotic analysis.
</p><p>To grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one <code class="mw-highlight mw-highlight-lang-text mw-content-ltr" dir="ltr" id="" style="">siftUp</code> call <i>increases</i> with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more "deep" nodes than there are "shallow" nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the "bottom" of the heap. On the other hand, the number of swaps that may occur during any one siftDown call <i>decreases</i> as the depth of the node on which the call is made increases. Thus, when the <code>siftDown</code> <code>heapify</code> begins and is calling <code>siftDown</code> on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the "height" (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to <code class="mw-highlight mw-highlight-lang-text mw-content-ltr" dir="ltr" id="" style="">siftDown</code> will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.
</p><p>The heapsort algorithm itself has <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span> time complexity using either version of heapify.
</p>
<pre> <b>procedure</b> heapify(a,count) is
     <i>(end is assigned the index of the first (left) child of the root)</i>
     end := 1
     
     <b>while</b> end &lt; count
         <i>(sift up the node at index end to the proper place such that all nodes above</i>
         <i> the end index are in heap order)</i>
         siftUp(a, 0, end)
         end := end + 1
     <i>(after sifting up the last node all nodes are in heap order)</i>
 
 <b>procedure</b> siftUp(a, start, end) <b>is</b>
     <b>input: </b> <i>start represents the limit of how far up the heap to sift.</i>
                   <i>end is the node to sift up.</i>
     child := end 
     <b>while</b> child &gt; start
         parent := iParent(child)
         <b>if</b> a[parent] &lt; a[child] <b>then</b> <i>(out of max-heap order)</i>
             swap(a[parent], a[child])
             child := parent <i>(repeat to continue sifting up the parent now)</i>
         <b>else</b>
             <b>return</b>
</pre>
<p>Note that unlike <code>siftDown</code> approach where, after each swap, you need to call only the <code>siftDown</code> subroutine to fix the broken heap; the <code>siftUp</code> subroutine alone cannot fix the broken heap. The heap needs to be built every time after a swap by calling the <code>heapify</code> procedure since "siftUp" assumes that the element getting swapped ends up in its final place, as opposed to "siftDown" allows for continuous adjustments of items lower in the heap until the invariant is satisfied. The adjusted pseudocode for using <code>siftUp</code> approach is given below.
</p>
<pre> <b>procedure</b> heapsort(a, count) <b>is</b>
    <b>input:</b> an unordered array <i>a</i> of length <i>count</i>
 
    <i>(Build the heap in array a so that largest value is at the root)</i>
    heapify(a, count)

    <i>(The following loop maintains the invariants that a[0:end] is a heap and every element</i>
     <i>beyond end is greater than everything before it (so a[end:count] is in sorted order))</i>
    end ← count - 1
    <b>while</b> end &gt; 0 <b>do</b>
        <i>(a[0] is the root and largest value. The swap moves it in front of the sorted elements.)</i>
        swap(a[end], a[0])
        <i>(rebuild the heap using siftUp after the swap ruins the heap property)</i>
        heapify(a, end)
        <i>(reduce the heap size by one)</i>
        end ← end - 1
</pre>
<h2><span class="mw-headline" id="Variations">Variations</span><span class="mw-editsection"></span></h2>
<h3><span id="Floyd.27s_heap_construction"></span><span class="mw-headline" id="Floyd's_heap_construction">Floyd's heap construction</span><span class="mw-editsection"></span></h3>
<p>The most important variation to the basic algorithm, which is included in all practical implementations, is a heap-construction algorithm by Floyd which runs in <span class="texhtml"><i>O</i>(<i>n</i>)</span> time and uses siftdown rather than siftup, avoiding the need to implement siftup at all.
</p><p>Rather than starting with a trivial heap and repeatedly adding leaves, Floyd's algorithm starts with the leaves, observing that they are trivial but valid heaps by themselves, and then adds parents. Starting with element <span class="texhtml"><i>n</i>/2</span> and working backwards, each internal node is made the root of a valid heap by sifting down. The last step is sifting down the first element, after which the entire array obeys the heap property.
</p><p>The worst-case number of comparisons during the Floyd's heap-construction phase of heapsort is known to be equal to <span class="texhtml">2<i>n</i> − 2<i>s</i><sub>2</sub>(<i>n</i>) − <i>e</i><sub>2</sub>(<i>n</i>)</span>, where <span class="texhtml"><i>s</i><sub>2</sub>(<i>n</i>)</span> is the number of 1 bits in the binary representation of <span class="texhtml mvar" style="font-style:italic;">n</span> and <span class="texhtml"><i>e</i><sub>2</sub>(<i>n</i>)</span> is number of trailing 0 bits.<sup class="reference" id="cite_ref-5">[5]</sup>
</p><p>The standard implementation of Floyd's heap-construction algorithm causes a large number of cache misses once the size of the data exceeds that of the CPU cache.<sup class="reference" id="cite_ref-LaMarca99_6-0">[6]</sup><sup class="reference nowrap"><span title="Page: 87">: 87 </span></sup> Much better performance on large data sets can be obtained by merging in depth-first order, combining subheaps as soon as possible, rather than combining all subheaps on one level before proceeding to the one above.<sup class="reference" id="cite_ref-Bojesen00_7-0">[7]</sup><sup class="reference" id="cite_ref-8">[8]</sup>
</p>
<h3><span class="mw-headline" id="Bottom-up_heapsort">Bottom-up heapsort</span><span class="mw-editsection"></span></h3>
<p>Bottom-up heapsort is a variant which reduces the number of comparisons required by a significant factor. While ordinary heapsort requires <span class="texhtml">2<i>n</i> log<sub>2</sub> <i>n</i> + <i>O</i>(<i>n</i>)</span> comparisons worst-case and on average,<sup class="reference" id="cite_ref-Wegener_9-0">[9]</sup> the bottom-up variant requires <span class="texhtml"><i>n</i> log<sub>2</sub><i>n</i> + <i>O</i>(1)</span> comparisons on average,<sup class="reference" id="cite_ref-Wegener_9-1">[9]</sup> and <span class="texhtml">1.5<i>n</i> log<sub>2</sub><i>n</i> + <i>O</i>(<i>n</i>)</span> in the worst case.<sup class="reference" id="cite_ref-fleischer_10-0">[10]</sup>
</p><p>If comparisons are cheap (e.g. integer keys) then the difference is unimportant,<sup class="reference" id="cite_ref-Melhorn_11-0">[11]</sup> as top-down heapsort compares values that have already been loaded from memory. If, however, comparisons require a function call or other complex logic, then bottom-up heapsort is advantageous.
</p><p>This is accomplished by improving the <code>siftDown</code> procedure. The change improves the linear-time heap-building phase somewhat,<sup class="reference" id="cite_ref-McDiarmid_12-0">[12]</sup> but is more significant in the second phase. Like ordinary heapsort, each iteration of the second phase extracts the top of the heap, <span class="texhtml"><i>a</i>[0]</span>, and fills the gap it leaves with <span class="texhtml"><i>a</i>[<i>end</i>]</span>, then sifts this latter element down the heap. But this element comes from the lowest level of the heap, meaning it is one of the smallest elements in the heap, so the sift-down will likely take many steps to move it back down.<sup class="reference" id="cite_ref-MacKay05_13-0">[13]</sup> In ordinary heapsort, each step of the sift-down requires two comparisons, to find the minimum of three elements: the new node and its two children.
</p><p>Bottom-up heapsort instead finds the path of largest children to the leaf level of the tree (as if it were inserting −∞) using only one comparison per level. Put another way, it finds a leaf which has the property that it and all of its ancestors are greater than or equal to their siblings. (In the absence of equal keys, this leaf is unique.) Then, from this leaf, it searches <i>upward</i> (using one comparison per level) for the correct position in that path to insert <span class="texhtml"><i>a</i>[<i>end</i>]</span>. This is the same location as ordinary heapsort finds, and requires the same number of exchanges to perform the insert, but fewer comparisons are required to find that location.<sup class="reference" id="cite_ref-fleischer_10-1">[10]</sup>
</p><p>Because it goes all the way to the bottom and then comes back up, it is called <b>heapsort with bounce</b> by some authors.<sup class="reference" id="cite_ref-14">[14]</sup>
</p>
<pre><b>function</b> leafSearch(a, i, end) <b>is</b>
    j ← i
    <b>while</b> iRightChild(j) ≤ end <b>do</b>
        <i>(Determine which of j's two children is the greater)</i>
        <b>if</b> a[iRightChild(j)] &gt; a[iLeftChild(j)] <b>then</b>
            j ← iRightChild(j)
        <b>else</b>
            j ← iLeftChild(j)
    <i>(At the last level, there might be only one child)</i>
    <b>if</b> iLeftChild(j) ≤ end <b>then</b>
        j ← iLeftChild(j)
    <b>return</b> j
</pre>
<p>The return value of the <code>leafSearch</code> is used in the modified <code>siftDown</code> routine:<sup class="reference" id="cite_ref-fleischer_10-2">[10]</sup>
</p>
<pre><b>procedure</b> siftDown(a, i, end) <b>is</b>
    j ← leafSearch(a, i, end)
    <b>while</b> a[i] &gt; a[j] <b>do</b>
        j ← iParent(j)
    x ← a[j]
    a[j] ← a[i]
    <b>while</b> j &gt; i <b>do</b>
        swap x, a[iParent(j)]
        j ← iParent(j)
</pre>
<p>Bottom-up heapsort was announced as beating quicksort (with median-of-three pivot selection) on arrays of size ≥16000.<sup class="reference" id="cite_ref-Wegener_9-2">[9]</sup>
</p><p>A 2008 re-evaluation of this algorithm showed it to be no faster than ordinary heapsort for integer keys, presumably because modern branch prediction nullifies the cost of the predictable comparisons which bottom-up heapsort manages to avoid.<sup class="reference" id="cite_ref-Melhorn_11-1">[11]</sup>
</p><p>A further refinement does a binary search in the path to the selected leaf, and sorts in a worst case of <span class="texhtml">(<i>n</i>+1)(log<sub>2</sub>(<i>n</i>+1) + log<sub>2</sub> log<sub>2</sub>(<i>n</i>+1) + 1.82) + <i>O</i>(log<sub>2</sub><i>n</i>)</span> comparisons, approaching the information-theoretic lower bound of <span class="texhtml"><i>n</i> log<sub>2</sub><i>n</i> − 1.4427<i>n</i></span> comparisons.<sup class="reference" id="cite_ref-Carlsson_15-1">[15]</sup>
</p><p>A variant which uses two extra bits per internal node (<i>n</i>−1 bits total for an <i>n</i>-element heap) to cache information about which child is greater (two bits are required to store three cases: left, right, and unknown)<sup class="reference" id="cite_ref-McDiarmid_12-1">[12]</sup> uses less than <span class="texhtml"><i>n</i> log<sub>2</sub><i>n</i> + 1.1<i>n</i></span> compares.<sup class="reference" id="cite_ref-16">[16]</sup>
</p>
<h3><span class="mw-headline" id="Other_variations">Other variations</span><span class="mw-editsection"></span></h3>
<ul><li>Ternary heapsort uses a ternary heap instead of a binary heap; that is, each element in the heap has three children. It is more complicated to program, but does a constant number of times fewer swap and comparison operations. This is because each sift-down step in a ternary heap requires three comparisons and one swap, whereas in a binary heap two comparisons and one swap are required. Two levels in a ternary heap cover 3<sup>2</sup> = 9 elements, doing more work with the same number of comparisons as three levels in the binary heap, which only cover 2<sup>3</sup> = 8.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><span title="This claim needs references to reliable sources. (September 2014)">citation needed</span></i>]</sup> This is primarily of academic interest, or as a student exercise,<sup class="reference" id="cite_ref-17">[17]</sup> as the additional complexity is not worth the minor savings, and bottom-up heapsort beats both.</li>
<li>Memory-optimized heapsort<sup class="reference" id="cite_ref-LaMarca99_6-1">[6]</sup><sup class="reference nowrap"><span title="Page: 87">: 87 </span></sup> improves heapsort's locality of reference by increasing the number of children even more. This increases the number of comparisons, but because all children are stored consecutively in memory, reduces the number of cache lines accessed during heap traversal, a net performance improvement.</li>
<li>Out-of-place heapsort<sup class="reference" id="cite_ref-18">[18]</sup><sup class="reference" id="cite_ref-19">[19]</sup><sup class="reference" id="cite_ref-MacKay05_13-1">[13]</sup> improves on bottom-up heapsort by eliminating the worst case, guaranteeing <span class="texhtml"><i>n</i> log<sub>2</sub><i>n</i> + <i>O</i>(<i>n</i>)</span> comparisons. When the maximum is taken, rather than fill the vacated space with an unsorted data value, fill it with a <span class="texhtml">−∞</span> sentinel value, which never "bounces" back up. It turns out that this can be used as a primitive in an in-place (and non-recursive) "QuickHeapsort" algorithm.<sup class="reference" id="cite_ref-20">[20]</sup> First, you perform a quicksort-like partitioning pass, but reversing the order of the partitioned data in the array. Suppose (without loss of generality) that the smaller partition is the one greater than the pivot, which should go at the end of the array, but our reversed partitioning step places it at the beginning. Form a heap out of the smaller partition and do out-of-place heapsort on it, exchanging the extracted maxima with values from the end of the array. These are less than the pivot, meaning less than any value in the heap, so serve as <span class="texhtml">−∞</span> sentinel values. Once the heapsort is complete (and the pivot moved to just before the now-sorted end of the array), the order of the partitions has been reversed, and the larger partition at the beginning of the array may be sorted in the same way. (Because there is no non-tail recursion, this also eliminates quicksort's <span class="texhtml"><i>O</i>(log <i>n</i>)</span> stack usage.)</li>
<li>The smoothsort algorithm<sup class="reference" id="cite_ref-21">[21]</sup> is a variation of heapsort developed by Edsger W. Dijkstra in 1981. Like heapsort, smoothsort's upper bound is <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span>. The advantage of smoothsort is that it comes closer to <span class="texhtml"><i>O</i>(<i>n</i>)</span> time if the input is already sorted to some degree, whereas heapsort averages <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span> regardless of the initial sorted state. Due to its complexity, smoothsort is rarely used.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><span title="This claim needs references to reliable sources. (November 2016)">citation needed</span></i>]</sup></li>
<li>Levcopoulos and Petersson<sup class="reference" id="cite_ref-22">[22]</sup> describe a variation of heapsort based on a heap of Cartesian trees. First, a Cartesian tree is built from the input in <span class="texhtml"><i>O</i>(<i>n</i>)</span> time, and its root is placed in a 1-element binary heap. Then we repeatedly extract the minimum from the binary heap, output the tree's root element, and add its left and right children (if any) which are themselves Cartesian trees, to the binary heap.<sup class="reference" id="cite_ref-23">[23]</sup> As they show, if the input is already nearly sorted, the Cartesian trees will be very unbalanced, with few nodes having left and right children, resulting in the binary heap remaining small, and allowing the algorithm to sort more quickly than <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span> for inputs that are already nearly sorted.</li>
<li>Several variants such as weak heapsort require <span class="texhtml"><i>n</i> log<sub>2</sub> <i>n</i>+<i>O</i>(1)</span> comparisons in the worst case, close to the theoretical minimum, using one extra bit of state per node. While this extra bit makes the algorithms not truly in-place, if space for it can be found inside the element, these algorithms are simple and efficient,<sup class="reference" id="cite_ref-Bojesen00_7-1">[7]</sup><sup class="reference nowrap"><span title="Page: 40">: 40 </span></sup> but still slower than binary heaps if key comparisons are cheap enough (e.g. integer keys) that a constant factor does not matter.<sup class="reference" id="cite_ref-Kat2014-11-14P_24-0">[24]</sup></li>
<li>Katajainen's "ultimate heapsort" requires no extra storage, performs <span class="texhtml"><i>n</i> log<sub>2</sub> <i>n</i>+<i>O</i>(1)</span> comparisons, and a similar number of element moves.<sup class="reference" id="cite_ref-25">[25]</sup> It is, however, even more complex and not justified unless comparisons are very expensive.</li></ul>
<h2><span class="mw-headline" id="Comparison_with_other_sorts">Comparison with other sorts</span><span class="mw-editsection"></span></h2>
<p>Heapsort primarily competes with quicksort, another very efficient general purpose in-place comparison-based sort algorithm.
</p><p>Heapsort's primary advantages are its simple, non-recursive code, minimal auxiliary storage requirement, and reliably good performance: its best and worst cases are within a small constant factor of each other, and of the theoretical lower bound on comparison sorts. While it cannot do better than <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span> for pre-sorted inputs, it does not suffer from quicksort's <span class="texhtml"><i>O</i>(<i>n</i><sup>2</sup>)</span> worst case, either. (The latter can be avoided with careful implementation, but that makes quicksort far more complex, and one of the most popular solutions, introsort, uses heapsort for the purpose.)
</p><p>Its primary disadvantages are its poor locality of reference and its inherently serial nature; the accesses to the implicit tree are widely scattered and mostly random, and there is no straightforward way to convert it to a parallel algorithm.
</p><p>This makes it popular in embedded systems, real-time computing, and systems concerned with maliciously chosen inputs,<sup class="reference" id="cite_ref-26">[26]</sup> such as the Linux kernel.<sup class="reference" id="cite_ref-27">[27]</sup> It is also a good choice for any application which does not expect to be bottlenecked on sorting.
</p><p>A well-implemented quicksort is usually 2–3 times faster than heapsort.<sup class="reference" id="cite_ref-LaMarca99_6-2">[6]</sup><sup class="reference" id="cite_ref-28">[28]</sup> Although quicksort requires fewer comparisons, this is a minor factor. (Results claiming twice as many comparisons are measuring the top-down version; see § Bottom-up heapsort.) The main advantage of quicksort is its much better locality of reference: partitioning is a linear scan with good spatial locality, and the recursive subdivision has good temporal locality. With additional effort, quicksort can also be implemented in mostly branch-free code, and multiple CPUs can be used to sort subpartitions in parallel. Thus, quicksort is preferred when the additional performance justifies the implementation effort.
</p><p>The other major <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span> sorting algorithm is merge sort, but that rarely competes directly with heapsort because it is not in-place. Merge sort's requirement for <span class="texhtml">Ω(<i>n</i>)</span> extra space (roughly half the size of the input) is usually prohibitive except in the situations where merge sort has a clear advantage:
</p>
<ul><li>When a stable sort is required</li>
<li>When taking advantage of (partially) pre-sorted input</li>
<li>Sorting linked lists (in which case merge sort requires minimal extra space)</li>
<li>Parallel sorting; merge sort parallelizes even better than quicksort and can easily achieve close to linear speedup</li>
<li>External sorting; merge sort has excellent locality of reference</li></ul>
<h2><span class="mw-headline" id="Example">Example</span><span class="mw-editsection"></span></h2>
<p>Let { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)
</p>

<h3><span class="mw-headline" id="Build_the_heap">Build the heap</span><span class="mw-editsection"></span></h3>
<table class="wikitable">
<tbody><tr>
<th>Heap</th>
<th>Newly added element</th>
<th>Swap elements
</th></tr>
<tr>
<td><code style="color: white; background-color: gray; padding: 2px 4px; font-size: smaller;">NULL</code></td>
<td>6</td>
<td>
</td></tr>
<tr>
<td>6</td>
<td>5</td>
<td>
</td></tr>
<tr>
<td>6, 5</td>
<td>3</td>
<td>
</td></tr>
<tr>
<td>6, 5, 3</td>
<td>1</td>
<td>
</td></tr>
<tr>
<td>6, 5, 3, 1</td>
<td>8</td>
<td>
</td></tr>
<tr>
<td>6, <b>5</b>, 3, 1, <b>8 </b></td>
<td></td>
<td>5, 8
</td></tr>
<tr>
<td><b>6</b>, <b>8</b>, 3, 1, 5</td>
<td></td>
<td>6, 8
</td></tr>
<tr>
<td>8, 6, 3, 1, 5</td>
<td>7</td>
<td>
</td></tr>
<tr>
<td>8, 6, <b>3</b>, 1, 5, <b>7</b></td>
<td></td>
<td>3, 7
</td></tr>
<tr>
<td>8, 6, 7, 1, 5, 3</td>
<td>2</td>
<td>
</td></tr>
<tr>
<td>8, 6, 7, 1, 5, 3, 2</td>
<td>4</td>
<td>
</td></tr>
<tr>
<td>8, 6, 7, <b>1</b>, 5, 3, 2, <b>4</b></td>
<td></td>
<td>1, 4
</td></tr>
<tr>
<td>8, 6, 7, 4, 5, 3, 2, 1</td>
<td></td>
<td>
</td></tr></tbody></table>
<h3><span class="mw-headline" id="Sorting">Sorting</span><span class="mw-editsection"></span></h3>
<table class="wikitable">
<tbody><tr>
<th>Heap</th>
<th>Swap elements</th>
<th>Delete element</th>
<th>Sorted array</th>
<th>Details
</th></tr>
<tr>
<td><b>8</b>, 6, 7, 4, 5, 3, 2, <b>1</b></td>
<td>8, 1</td>
<td></td>
<td></td>
<td>Swap 8 and 1 in order to delete 8 from heap
</td></tr>
<tr>
<td>1, 6, 7, 4, 5, 3, 2, <b>8</b></td>
<td></td>
<td>8</td>
<td></td>
<td>Delete 8 from heap and add to sorted array
</td></tr>
<tr>
<td><b>1</b>, 6, <b>7</b>, 4, 5, 3, 2</td>
<td>1, 7</td>
<td></td>
<td style="text-align: right;">8</td>
<td>Swap 1 and 7 as they are not in order in the heap
</td></tr>
<tr>
<td>7, 6, <b>1</b>, 4, 5, <b>3</b>, 2</td>
<td>1, 3</td>
<td></td>
<td style="text-align: right;">8</td>
<td>Swap 1 and 3 as they are not in order in the heap
</td></tr>
<tr>
<td><b>7</b>, 6, 3, 4, 5, 1, <b>2</b></td>
<td>7, 2</td>
<td></td>
<td style="text-align: right;">8</td>
<td>Swap 7 and 2 in order to delete 7 from heap
</td></tr>
<tr>
<td>2, 6, 3, 4, 5, 1, <b>7</b></td>
<td></td>
<td>7</td>
<td style="text-align: right;">8</td>
<td>Delete 7 from heap and add to sorted array
</td></tr>
<tr>
<td><b>2</b>, <b>6</b>, 3, 4, 5, 1</td>
<td>2, 6</td>
<td></td>
<td style="text-align: right;">7, 8</td>
<td>Swap 2 and 6 as they are not in order in the heap
</td></tr>
<tr>
<td>6, <b>2</b>, 3, 4, <b>5</b>, 1</td>
<td>2, 5</td>
<td></td>
<td style="text-align: right;">7, 8</td>
<td>Swap 2 and 5 as they are not in order in the heap
</td></tr>
<tr>
<td><b>6</b>, 5, 3, 4, 2, <b>1</b></td>
<td>6, 1</td>
<td></td>
<td style="text-align: right;">7, 8</td>
<td>Swap 6 and 1 in order to delete 6 from heap
</td></tr>
<tr>
<td>1, 5, 3, 4, 2, <b>6</b></td>
<td></td>
<td>6</td>
<td style="text-align: right;">7, 8</td>
<td>Delete 6 from heap and add to sorted array
</td></tr>
<tr>
<td><b>1</b>, <b>5</b>, 3, 4, 2</td>
<td>1, 5</td>
<td></td>
<td style="text-align: right;">6, 7, 8</td>
<td>Swap 1 and 5 as they are not in order in the heap
</td></tr>
<tr>
<td>5, <b>1</b>, 3, <b>4</b>, 2</td>
<td>1, 4</td>
<td></td>
<td style="text-align: right;">6, 7, 8</td>
<td>Swap 1 and 4 as they are not in order in the heap
</td></tr>
<tr>
<td><b>5</b>, 4, 3, 1, <b>2</b></td>
<td>5, 2</td>
<td></td>
<td style="text-align: right;">6, 7, 8</td>
<td>Swap 5 and 2 in order to delete 5 from heap
</td></tr>
<tr>
<td>2, 4, 3, 1, <b>5</b></td>
<td></td>
<td>5</td>
<td style="text-align: right;">6, 7, 8</td>
<td>Delete 5 from heap and add to sorted array
</td></tr>
<tr>
<td><b>2</b>, <b>4</b>, 3, 1</td>
<td>2, 4</td>
<td></td>
<td style="text-align: right;">5, 6, 7, 8</td>
<td>Swap 2 and 4 as they are not in order in the heap
</td></tr>
<tr>
<td><b>4</b>, 2, 3, <b>1</b></td>
<td>4, 1</td>
<td></td>
<td style="text-align: right;">5, 6, 7, 8</td>
<td>Swap 4 and 1 in order to delete 4 from heap
</td></tr>
<tr>
<td>1, 2, 3, <b>4</b></td>
<td></td>
<td>4</td>
<td style="text-align: right;">5, 6, 7, 8</td>
<td>Delete 4 from heap and add to sorted array
</td></tr>
<tr>
<td><b>1</b>, 2, <b>3</b></td>
<td>1, 3</td>
<td></td>
<td style="text-align: right;">4, 5, 6, 7, 8</td>
<td>Swap 1 and 3 as they are not in order in the heap
</td></tr>
<tr>
<td><b>3</b>, 2, <b>1</b></td>
<td>3, 1</td>
<td></td>
<td style="text-align: right;">4, 5, 6, 7, 8</td>
<td>Swap 3 and 1 in order to delete 3 from heap
</td></tr>
<tr>
<td>1, 2, <b>3</b></td>
<td></td>
<td>3</td>
<td style="text-align: right;">4, 5, 6, 7, 8</td>
<td>Delete 3 from heap and add to sorted array
</td></tr>
<tr>
<td><b>1</b>, <b>2</b></td>
<td>1, 2</td>
<td></td>
<td style="text-align: right;">3, 4, 5, 6, 7, 8</td>
<td>Swap 1 and 2 as they are not in order in the heap
</td></tr>
<tr>
<td><b>2</b>, <b>1</b></td>
<td>2, 1</td>
<td></td>
<td style="text-align: right;">3, 4, 5, 6, 7, 8</td>
<td>Swap 2 and 1 in order to delete 2 from heap
</td></tr>
<tr>
<td>1, <b>2</b></td>
<td></td>
<td>2</td>
<td style="text-align: right;">3, 4, 5, 6, 7, 8</td>
<td>Delete 2 from heap and add to sorted array
</td></tr>
<tr>
<td><b>1</b></td>
<td></td>
<td>1</td>
<td style="text-align: right;">2, 3, 4, 5, 6, 7, 8</td>
<td>Delete 1 from heap and add to sorted array
</td></tr>
<tr>
<td></td>
<td></td>
<td></td>
<td style="text-align: right;">1, 2, 3, 4, 5, 6, 7, 8</td>
<td class="table-success" style="background: #9EFF9E; vertical-align: middle; text-align: center;">Completed
</td></tr></tbody></table>
<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"></span></h2>
<ul><li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation journal cs1" id="CITEREFWilliams1964">Williams, J. W. J. (1964). "Algorithm 232 – Heapsort". <i>Communications of the ACM</i>. <b>7</b> (6): 347–348. doi:10.1145/512274.512284.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Communications+of+the+ACM&amp;rft.atitle=Algorithm+232+%E2%80%93+Heapsort&amp;rft.volume=7&amp;rft.issue=6&amp;rft.pages=347-348&amp;rft.date=1964&amp;rft_id=info%3Adoi%2F10.1145%2F512274.512284&amp;rft.aulast=Williams&amp;rft.aufirst=J.+W.+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHeapsort"></span></li>
<li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation journal cs1" id="CITEREFFloyd1964">Floyd, Robert W. (1964). "Algorithm 245 – Treesort 3". <i>Communications of the ACM</i>. <b>7</b> (12): 701. doi:10.1145/355588.365103. S2CID 52864987.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Communications+of+the+ACM&amp;rft.atitle=Algorithm+245+%E2%80%93+Treesort+3&amp;rft.volume=7&amp;rft.issue=12&amp;rft.pages=701&amp;rft.date=1964&amp;rft_id=info%3Adoi%2F10.1145%2F355588.365103&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A52864987%23id-name%3DS2CID&amp;rft.aulast=Floyd&amp;rft.aufirst=Robert+W.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHeapsort"></span></li>
<li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation journal cs1" id="CITEREFCarlsson1987">Carlsson, Svante (1987). "Average-case results on heapsort". <i>BIT Numerical Mathematics</i>. <b>27</b> (1): 2–17. doi:10.1007/bf01937350. S2CID 31450060.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BIT+Numerical+Mathematics&amp;rft.atitle=Average-case+results+on+heapsort&amp;rft.volume=27&amp;rft.issue=1&amp;rft.pages=2-17&amp;rft.date=1987&amp;rft_id=info%3Adoi%2F10.1007%2Fbf01937350&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A31450060%23id-name%3DS2CID&amp;rft.aulast=Carlsson&amp;rft.aufirst=Svante&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHeapsort"></span></li>
<li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation book cs1 cs1-prop-long-vol" id="CITEREFKnuth1997">Knuth, Donald (1997). "§5.2.3, Sorting by Selection". <i>The Art of Computer Programming</i>. Vol. 3: Sorting and Searching (3rd ed.). Addison-Wesley. pp. 144–155. ISBN <bdi>978-0-201-89685-5</bdi>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=%C2%A75.2.3%2C+Sorting+by+Selection&amp;rft.btitle=The+Art+of+Computer+Programming&amp;rft.pages=144-155&amp;rft.edition=3rd&amp;rft.pub=Addison-Wesley&amp;rft.date=1997&amp;rft.isbn=978-0-201-89685-5&amp;rft.aulast=Knuth&amp;rft.aufirst=Donald&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHeapsort"></span></li>
<li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation book cs1" id="CITEREFCormenLeisersonRivestStein2001">Cormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). <i>Introduction to Algorithms</i> (2nd ed.). MIT Press and McGraw-Hill. ISBN <bdi>0-262-03293-7</bdi>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Introduction+to+Algorithms&amp;rft.edition=2nd&amp;rft.pub=MIT+Press+and+McGraw-Hill&amp;rft.date=2001&amp;rft.isbn=0-262-03293-7&amp;rft.aulast=Cormen&amp;rft.aufirst=Thomas+H.&amp;rft.au=Leiserson%2C+Charles+E.&amp;rft.au=Rivest%2C+Ronald+L.&amp;rft.au=Stein%2C+Clifford&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHeapsort"></span> Chapters 6 and 7 Respectively: Heapsort and Priority Queues</li>
<li>A PDF of Dijkstra's original paper on Smoothsort</li>
<li>Heaps and Heapsort Tutorial by David Carlson, St. Vincent College</li></ul>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1097025294">.mw-parser-output .side-box{margin:4px 0;box-sizing:border-box;border:1px solid #aaa;font-size:88%;line-height:1.25em;background-color:#f9f9f9}.mw-parser-output .side-box-abovebelow,.mw-parser-output .side-box-text{padding:0.25em 0.9em}.mw-parser-output .side-box-image{padding:2px 0 2px 0.9em;text-align:center}.mw-parser-output .side-box-imageright{padding:2px 0.9em 2px 0;text-align:center}@media(min-width:500px){.mw-parser-output .side-box-flex{display:flex;align-items:center}.mw-parser-output .side-box-text{flex:1}}@media(min-width:720px){.mw-parser-output .side-box{width:238px}.mw-parser-output .side-box-right{clear:right;float:right;margin-left:1em}.mw-parser-output .side-box-left{margin-right:1em}}</style>
<ul><li>Animated Sorting Algorithms: Heap Sort at the Wayback Machine (archived 6 March 2015) – graphical demonstration</li>
<li>Courseware on Heapsort from Univ. Oldenburg – With text, animations and interactive exercises</li>
<li>NIST's Dictionary of Algorithms and Data Structures: Heapsort</li>
<li>Heapsort implemented in 12 languages</li>
<li>Sorting revisited by Paul Hsieh</li>
<li>A PowerPoint presentation demonstrating how Heap sort works that is for educators.</li>
<li>Open Data Structures – Section 11.1.3 – Heap-Sort, Pat Morin</li></ul>

<!-- 
NewPP limit report
Parsed by mw2339
Cached time: 20221224011502
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.610 seconds
Real time usage: 0.788 seconds
Preprocessor visited node count: 7181/1000000
Post‐expand include size: 106760/2097152 bytes
Template argument size: 9092/2097152 bytes
Highest expansion depth: 15/100
Expensive parser function count: 10/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 106609/5000000 bytes
Lua time usage: 0.336/10.000 seconds
Lua memory usage: 9956492/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  649.680      1 -total
 32.67%  212.275      1 Template:Reflist
 18.49%  120.103     13 Template:Cite_journal
 10.24%   66.518      1 Template:Refn
  9.81%   63.763      1 Template:Short_description
  8.52%   55.345      9 Template:R
  7.52%   48.870      9 Template:R/ref
  7.28%   47.266      9 Template:Cite_book
  6.42%   41.692      1 Template:Harvnb
  6.15%   39.944      1 Template:Sorting
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:13995-0!canonical and timestamp 20221224011501 and revision id 1123622826.
 -->
</div></body>
</html>