secant_searchIn numerical analysis, the secant method is a root-finding algorithm that uses a succession of roots of secant lines to better approximate a root of a function f.  The secant method can be thought of as a finite-difference approximation of Newton's method. However, the secant method predates Newton's method by over 3000 years.[1]For finding a zero of a function , the secant method is defined by the recurrence relation.As can be seen from this formula, two initial values  and  are required. Ideally, they should be chosen close to the desired zero.Starting with initial values  and , we construct a line through the points  and , as shown in the picture above. In slope–intercept form, the equation of this line isThe root of this linear function, that is the value of  such that  isWe then use this new value of  as  and repeat the process, using  and  instead of  and . We continue this process, solving for , , etc., until we reach a sufficiently high level of precision (a sufficiently small difference between  and ):The iterates  of the secant method converge to a root of  is,if the initial values  and  are sufficiently close to the root. The order of convergence is "φ", whereis the golden ratio. In particular, the convergence is super linear, but not quite quadratic.This result only holds under some technical conditions, namely that  be twice continuously differentiable and the root in question be simple (i.e., with multiplicity 1).If the initial values are not close enough to the root, then there is no guarantee that the secant method converges. There is no general definition of "close enough", but the criterion has to do with how "wiggly" the function is on the interval . For example, if  is differentiable on that interval and there is a point where  on the interval, then the algorithm may not converge.The secant method does not require that the root remain bracketed, like the bisection method does, and hence it does not always converge. The false position method (or ) uses the same formula as the secant method. However, it does not apply the formula on  and , like the secant method, but on  and on the last iterate  such that  and  have a different sign. This means that the false position method always converges; however, only with a linear order of convergence. Bracketing with a super-linear order of convergence as the secant method can be attained with improvements to the false position method (see Regula falsi § Improvements in regula falsi) such as the ITP method or Illinois method.The recurrence formula of the secant method can be derived from the formula for Newton's methodby using the finite-difference approximation, for a small :The secant method can be interpreted as a method in which the derivative is replaced by an approximation and is thus a quasi-Newton method.If we compare Newton's method with the secant method, we see that Newton's method converges faster (order 2 against φ ≈ 1.6). However, Newton's method requires the evaluation of both  and its derivative  at every step, while the secant method only requires the evaluation of . Therefore, the secant method may occasionally be faster in practice. For instance, if we assume that evaluating  takes as much time as evaluating its derivative and we neglect all other costs, we can do two steps of the secant method (decreasing the logarithm of the error by a factor φ2 ≈ 2.6) for the same cost as one step of Newton's method (decreasing the logarithm of the error by a factor 2), so the secant method is faster. If, however, we consider parallel processing for the evaluation of the derivative, Newton's method proves its worth, being faster in time, though still spending more steps.Broyden's method is a generalization of the secant method to more than one dimension.The following graph shows the function f in red and the last secant line in bold blue.  In the graph, the x intercept of the secant line seems to be a good approximation of the root of f.Below, the secant method is implemented in the Python programming language. It is then applied to find a root of the function  with initial points  and It is very important to have a good stopping criterion above, otherwise, due to limited numerical precision of floating point numbers, the algorithm can return inaccurate results if running for too many iterations. For example, the loop above can stop when one of these is reached first: abs(x0 - x1) &lt; tol, or abs(x0-x1)/abs(x1) &lt; tol, or abs(f(x1)) &lt; tol. [2]False position methodAvriel, Mordecai (1976). Nonlinear Programming: Analysis and Methods. Prentice Hall. pp. 220–221. ISBN 0-13-623603-0.Allen, Myron B.; Isaacson, Eli L. (1998). Numerical analysis for applied science. John Wiley &amp; Sons. pp. 188–195. ISBN 978-0-471-55266-6.Secant Method Notes, PPT, Mathcad, Maple, Mathematica, Matlab at Holistic Numerical Methods Institute