<!DOCTYPE html>
<html>
<head>
<title>hidden_Markov_model</title>
</head>
<body>
<div class="mw-parser-output"><img alt="This is a good article. Click here for more information." data-file-height="185" data-file-width="180" decoding="async" height="20" src="//upload.wikimedia.org/wikipedia/en/thumb/9/94/Symbol_support_vote.svg/19px-Symbol_support_vote.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/94/Symbol_support_vote.svg/29px-Symbol_support_vote.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/94/Symbol_support_vote.svg/39px-Symbol_support_vote.svg.png 2x" width="19"/></div><div class="mw-parser-output">
<p>
A <b>hidden Markov model</b> (<b>HMM</b>) is a statistical Markov model in which the system being modeled is assumed to be a Markov process — call it <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X}</annotation>
</semantics>
</math></span><img alt="X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;"/></span> — with unobservable ("<i>hidden</i>") states. As part of the definition, HMM requires that there be an observable process <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Y</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y}</annotation>
</semantics>
</math></span><img alt="Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" style="vertical-align: -0.171ex; width:1.773ex; height:2.009ex;"/></span> whose outcomes are "influenced" by the outcomes of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X}</annotation>
</semantics>
</math></span><img alt="X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;"/></span> in a known way. Since <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X}</annotation>
</semantics>
</math></span><img alt="X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;"/></span> cannot be observed directly, the goal is to learn about <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X}</annotation>
</semantics>
</math></span><img alt="X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;"/></span> by observing <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Y</mi>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle Y.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0c668649af47a30006f93c9847d61fee8d9ffb61" style="vertical-align: -0.338ex; width:2.42ex; height:2.176ex;"/></span> HMM has an additional requirement that the outcome of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Y</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y}</annotation>
</semantics>
</math></span><img alt="Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" style="vertical-align: -0.171ex; width:1.773ex; height:2.009ex;"/></span> at time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t=t_{0}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
<mo>=</mo>
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t=t_{0}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle t=t_{0}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/be6d7492e2d48bf34fdd5dffa189b188c140820c" style="vertical-align: -0.671ex; width:5.832ex; height:2.343ex;"/></span> must be "influenced" exclusively by the outcome of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X}</annotation>
</semantics>
</math></span><img alt="X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;"/></span> at <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t=t_{0}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
<mo>=</mo>
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t=t_{0}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle t=t_{0}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/be6d7492e2d48bf34fdd5dffa189b188c140820c" style="vertical-align: -0.671ex; width:5.832ex; height:2.343ex;"/></span> and that the outcomes of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>X</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X}</annotation>
</semantics>
</math></span><img alt="X" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Y</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y}</annotation>
</semantics>
</math></span><img alt="Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" style="vertical-align: -0.171ex; width:1.773ex; height:2.009ex;"/></span> at <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t&lt;t_{0}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
<mo>&lt;</mo>
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t&lt;t_{0}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle t&lt;t_{0}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/112ff019aeabac599ad8c97e3cbe65e491c5c7e7" style="vertical-align: -0.671ex; width:5.832ex; height:2.343ex;"/></span> must not affect the outcome of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Y</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y}</annotation>
</semantics>
</math></span><img alt="Y" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" style="vertical-align: -0.171ex; width:1.773ex; height:2.009ex;"/></span> at <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t=t_{0}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
<mo>=</mo>
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t=t_{0}.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle t=t_{0}.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/97de8a1e6c41d2310938a0dcdc51f8f7a4c5fb2f" style="vertical-align: -0.671ex; width:6.479ex; height:2.343ex;"/></span>
</p><p>Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech [3], handwriting, gesture recognition,<sup class="reference" id="cite_ref-1">[1]</sup> part-of-speech tagging, musical score following,<sup class="reference" id="cite_ref-2">[2]</sup> partial discharges<sup class="reference" id="cite_ref-3">[3]</sup> and bioinformatics.<sup class="reference" id="cite_ref-4">[4]</sup><sup class="reference" id="cite_ref-5">[5]</sup>
</p>

<h2><span class="mw-headline" id="Definition">Definition</span><span class="mw-editsection"></span></h2>
<p>Let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X_{n}}</annotation>
</semantics>
</math></span><img alt="X_{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/72a8564cedc659cf2f95ae68bc5de2f5207a3285" style="vertical-align: -0.671ex; width:3.143ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y_{n}}</annotation>
</semantics>
</math></span><img alt="Y_{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f19a1b3bf39298aacb7e2daeab9320130a986fb0" style="vertical-align: -0.671ex; width:2.569ex; height:2.509ex;"/></span> be discrete-time stochastic processes and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle n\geq 1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>n</mi>
<mo>≥<!-- ≥ --></mo>
<mn>1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle n\geq 1}</annotation>
</semantics>
</math></span><img alt="n\geq 1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d8ce9ce38d06f6bf5a3fe063118c09c2b6202bfe" style="vertical-align: -0.505ex; width:5.656ex; height:2.343ex;"/></span>. The pair <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (X_{n},Y_{n})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (X_{n},Y_{n})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle (X_{n},Y_{n})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d52ed98685e48f1bab92be87c5cdd638e058116b" style="vertical-align: -0.838ex; width:8.555ex; height:2.843ex;"/></span> is a <i>hidden Markov model</i> if
</p>
<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X_{n}}</annotation>
</semantics>
</math></span><img alt="X_{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/72a8564cedc659cf2f95ae68bc5de2f5207a3285" style="vertical-align: -0.671ex; width:3.143ex; height:2.509ex;"/></span> is a Markov process whose behavior is not directly observable ("hidden");</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{1}=x_{1},\ldots ,X_{n}=x_{n}{\bigr )}=\operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{n}=x_{n}{\bigr )},}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">P</mi>
</mrow>
</mrow>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-OPEN">
<mo maxsize="1.2em" minsize="1.2em">(</mo>
</mrow>
</mrow>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>A</mi>
<mtext> </mtext>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-OPEN">
<mo maxsize="1.2em" minsize="1.2em">|</mo>
</mrow>
</mrow>
<mtext> </mtext>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>=</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>=</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-CLOSE">
<mo maxsize="1.2em" minsize="1.2em">)</mo>
</mrow>
</mrow>
<mo>=</mo>
<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">P</mi>
</mrow>
</mrow>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-OPEN">
<mo maxsize="1.2em" minsize="1.2em">(</mo>
</mrow>
</mrow>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>A</mi>
<mtext> </mtext>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-OPEN">
<mo maxsize="1.2em" minsize="1.2em">|</mo>
</mrow>
</mrow>
<mtext> </mtext>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>=</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-CLOSE">
<mo maxsize="1.2em" minsize="1.2em">)</mo>
</mrow>
</mrow>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{1}=x_{1},\ldots ,X_{n}=x_{n}{\bigr )}=\operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{n}=x_{n}{\bigr )},}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{1}=x_{1},\ldots ,X_{n}=x_{n}{\bigr )}=\operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\ {\bigl |}\ X_{n}=x_{n}{\bigr )},}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e059b412dd575786c2565eb0b89ea602e37298d" style="vertical-align: -1.005ex; width:61.572ex; height:3.176ex;"/></span></li></ul>
<dl><dd>for every <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle n\geq 1,}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>n</mi>
<mo>≥<!-- ≥ --></mo>
<mn>1</mn>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle n\geq 1,}</annotation>
</semantics>
</math></span><img alt="{\displaystyle n\geq 1,}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cc38ec6af7dd11fdc9baa67365f23906d76da4bb" style="vertical-align: -0.671ex; width:6.302ex; height:2.509ex;"/></span> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{1},\ldots ,x_{n},}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle x_{1},\ldots ,x_{n},}</annotation>
</semantics>
</math></span><img alt="{\displaystyle x_{1},\ldots ,x_{n},}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8fb4ea72660b223c376e371c2301215a39e53a55" style="vertical-align: -0.671ex; width:10.757ex; height:2.009ex;"/></span> and every Borel set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle A}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>A</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle A}</annotation>
</semantics>
</math></span><img alt="A" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7daff47fa58cdfd29dc333def748ff5fa4c923e3" style="vertical-align: -0.338ex; width:1.743ex; height:2.176ex;"/></span>.</dd></dl>
<p>Let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X_{t}}</annotation>
</semantics>
</math></span><img alt="X_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82120d04dfb3cbadc4912951dd12b5568c9cd8f3" style="vertical-align: -0.671ex; width:2.75ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y_{t}}</annotation>
</semantics>
</math></span><img alt="Y_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/95734a78eb8407939c3496cbfd92763ced1e41e1" style="vertical-align: -0.671ex; width:2.177ex; height:2.509ex;"/></span> be continuous-time stochastic processes. The pair <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (X_{t},Y_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (X_{t},Y_{t})}</annotation>
</semantics>
</math></span><img alt="(X_{t},Y_{t})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/43551e8342244761a48fcb11ae300d3a1f299722" style="vertical-align: -0.838ex; width:7.77ex; height:2.843ex;"/></span> is a <i>hidden Markov model</i> if
</p>
<ul><li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X_{t}}</annotation>
</semantics>
</math></span><img alt="X_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82120d04dfb3cbadc4912951dd12b5568c9cd8f3" style="vertical-align: -0.671ex; width:2.75ex; height:2.509ex;"/></span> is a Markov process whose behavior is not directly observable ("hidden");</li>
<li><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {\mathbf {P} } (Y_{t_{0}}\in A\mid \{X_{t}\in B_{t}\}_{t\leq t_{0}})=\operatorname {\mathbf {P} } (Y_{t_{0}}\in A\mid X_{t_{0}}\in B_{t_{0}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">P</mi>
</mrow>
</mrow>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">(</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>A</mi>
<mo>∣<!-- ∣ --></mo>
<mo fence="false" stretchy="false">{</mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<msub>
<mo fence="false" stretchy="false">}</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>≤<!-- ≤ --></mo>
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">P</mi>
</mrow>
</mrow>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">(</mo>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>A</mi>
<mo>∣<!-- ∣ --></mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {\mathbf {P} } (Y_{t_{0}}\in A\mid \{X_{t}\in B_{t}\}_{t\leq t_{0}})=\operatorname {\mathbf {P} } (Y_{t_{0}}\in A\mid X_{t_{0}}\in B_{t_{0}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {\mathbf {P} } (Y_{t_{0}}\in A\mid \{X_{t}\in B_{t}\}_{t\leq t_{0}})=\operatorname {\mathbf {P} } (Y_{t_{0}}\in A\mid X_{t_{0}}\in B_{t_{0}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8f5faf9bf8284f377417ed5da47b13c9e84c388b" style="vertical-align: -1.005ex; width:53.308ex; height:3.009ex;"/></span>,</li></ul>
<dl><dd>for every <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t_{0},}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t_{0},}</annotation>
</semantics>
</math></span><img alt="{\displaystyle t_{0},}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/950eb6be65c21a0a4e3b432a2469dcd42fc8a908" style="vertical-align: -0.671ex; width:2.541ex; height:2.343ex;"/></span> every Borel set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle A,}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>A</mi>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle A,}</annotation>
</semantics>
</math></span><img alt="A," aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2746026864cc5896e3e52443a1c917be2df9d8ea" style="vertical-align: -0.671ex; width:2.39ex; height:2.509ex;"/></span> and every family of Borel sets <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \{B_{t}\}_{t\leq t_{0}}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo fence="false" stretchy="false">{</mo>
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<msub>
<mo fence="false" stretchy="false">}</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>≤<!-- ≤ --></mo>
<msub>
<mi>t</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>0</mn>
</mrow>
</msub>
</mrow>
</msub>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \{B_{t}\}_{t\leq t_{0}}.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \{B_{t}\}_{t\leq t_{0}}.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0e299f1f9dff053498c3a598ac5c59aae3e02d9c" style="vertical-align: -1.005ex; width:9.091ex; height:3.009ex;"/></span></dd></dl>
<h3><span class="mw-headline" id="Terminology">Terminology</span><span class="mw-editsection"></span></h3>
<p>The states of the process <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X_{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X_{n}}</annotation>
</semantics>
</math></span><img alt="X_{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/72a8564cedc659cf2f95ae68bc5de2f5207a3285" style="vertical-align: -0.671ex; width:3.143ex; height:2.509ex;"/></span> (resp. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X_{t})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X_{t})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle X_{t})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9ca8defb232945c28630136118eb55cf0892f3dc" style="vertical-align: -0.838ex; width:3.655ex; height:2.843ex;"/></span> are called <i>hidden states</i>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\mid X_{n}=x_{n}{\bigr )}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">P</mi>
</mrow>
</mrow>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-OPEN">
<mo maxsize="1.2em" minsize="1.2em">(</mo>
</mrow>
</mrow>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>A</mi>
<mo>∣<!-- ∣ --></mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mo>=</mo>
<msub>
<mi>x</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-CLOSE">
<mo maxsize="1.2em" minsize="1.2em">)</mo>
</mrow>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\mid X_{n}=x_{n}{\bigr )}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{n}\in A\mid X_{n}=x_{n}{\bigr )}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e99a011d0f67e99b6bd3f45638dcc4dc48a65a0b" style="vertical-align: -1.005ex; width:22.223ex; height:3.176ex;"/></span> (resp. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{t}\in A\mid X_{t}\in B_{t}{\bigr )})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="bold">P</mi>
</mrow>
</mrow>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-OPEN">
<mo maxsize="1.2em" minsize="1.2em">(</mo>
</mrow>
</mrow>
<msub>
<mi>Y</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<mi>A</mi>
<mo>∣<!-- ∣ --></mo>
<msub>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mo>∈<!-- ∈ --></mo>
<msub>
<mi>B</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-CLOSE">
<mo maxsize="1.2em" minsize="1.2em">)</mo>
</mrow>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{t}\in A\mid X_{t}\in B_{t}{\bigr )})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \operatorname {\mathbf {P} } {\bigl (}Y_{t}\in A\mid X_{t}\in B_{t}{\bigr )})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5a9f1d7ee383ebd16313eaf22e52bfce46c67304" style="vertical-align: -1.005ex; width:22.127ex; height:3.176ex;"/></span> is called <i>emission probability</i> or <i>output probability</i>.
</p>
<h2><span class="mw-headline" id="Examples">Examples</span><span class="mw-editsection"></span></h2>
<h3><span class="mw-headline" id="Drawing_balls_from_hidden_urns">Drawing balls from hidden urns</span><span class="mw-editsection"></span></h3>

<p>In its discrete form, a hidden Markov process can be visualized as a generalization of the urn problem with replacement (where each item from the urn is returned to the original urn before the next step).<sup class="reference" id="cite_ref-6">[6]</sup> Consider this example: in a room that is not visible to an observer there is a genie. The room contains urns X1, X2, X3, ... each of which contains a known mix of balls, each ball labeled y1, y2, y3, ... .  The genie chooses an urn in that room and randomly draws a ball from that urn.  It then puts the ball onto a conveyor belt, where the observer can observe the sequence of the balls but not the sequence of urns from which they were drawn. The genie has some procedure to choose urns; the choice of the urn for the <i>n</i>-th ball depends only upon a random number and the choice of the urn for the (<i>n</i> − 1)-th ball.  The choice of urn does not directly depend on the urns chosen before this single previous urn; therefore, this is called a Markov process. It can be described by the upper part of Figure 1.
</p><p>The Markov process itself cannot be observed, only the sequence of labeled balls, thus this arrangement is called a "hidden Markov process". This is illustrated by the lower part of the diagram shown in Figure 1, where one can see that balls y1, y2, y3, y4 can be drawn at each state. Even if the observer knows the composition of the urns and has just observed a sequence of three balls, <i>e.g.</i> y1, y2 and y3 on the conveyor belt, the observer still cannot be <i>sure</i> which urn (<i>i.e.</i>, at which state) the genie has drawn the third ball from. However, the observer can work out other information, such as the likelihood that the third ball came from each of the urns.
</p>
<h3><span class="mw-headline" id="Weather_guessing_game">Weather guessing game</span><span class="mw-editsection"></span></h3>
<p>Consider two friends, Alice and Bob, who live far apart from each other and who talk together daily over the telephone about what they did that day. Bob is only interested in three activities: walking in the park, shopping, and cleaning his apartment. The choice of what to do is determined exclusively by the weather on a given day. Alice has no definite information about the weather, but she knows general trends. Based on what Bob tells her he did each day, Alice tries to guess what the weather must have been like.
</p><p>Alice believes that the weather operates as a discrete Markov chain. There are two states, "Rainy" and "Sunny", but she cannot observe them directly, that is, they are <i>hidden</i> from her. On each day, there is a certain chance that Bob will perform one of the following activities, depending on the weather: "walk", "shop", or "clean". Since Bob tells Alice about his activities, those are the <i>observations</i>. The entire system is that of a hidden Markov model (HMM).
</p><p>Alice knows the general weather trends in the area, and what Bob likes to do on average. In other words, the parameters of the HMM are known. They can be represented as follows in Python:
</p>

<p>In this piece of code, <code>start_probability</code> represents Alice's belief about which state the HMM is in when Bob first calls her (all she knows is that it tends to be rainy on average). The particular probability distribution used here is not the equilibrium one, which is (given the transition probabilities) approximately <code>{'Rainy': 0.57, 'Sunny': 0.43}</code>. The <code>transition_probability</code> represents the change of the weather in the underlying Markov chain. In this example, there is only a 30% chance that tomorrow will be sunny if today is rainy. The <code>emission_probability</code> represents how likely Bob is to perform a certain activity on each day. If it is rainy, there is a 50% chance that he is cleaning his apartment; if it is sunny, there is a 60% chance that he is outside for a walk.
</p>

<p><i>A similar example is further elaborated in the Viterbi algorithm page.</i>
</p>
<h2><span class="mw-headline" id="Structural_architecture">Structural architecture</span><span class="mw-editsection"></span></h2>
<p>The diagram below shows the general architecture of an instantiated HMM. Each oval shape represents a random variable that can adopt any of a number of values. The random variable <i>x</i>(<i>t</i>) is the hidden state at time <span class="texhtml mvar" style="font-style:italic;">t</span> (with the model from the above diagram, <i>x</i>(<i>t</i>) ∈ { <i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, <i>x</i><sub>3</sub> }). The random variable <i>y</i>(<i>t</i>) is the observation at time <span class="texhtml mvar" style="font-style:italic;">t</span> (with <i>y</i>(<i>t</i>) ∈ { <i>y</i><sub>1</sub>, <i>y</i><sub>2</sub>, <i>y</i><sub>3</sub>, <i>y</i><sub>4</sub> }). The arrows in the diagram (often called a trellis diagram) denote conditional dependencies.
</p><p>From the diagram, it is clear that the conditional probability distribution of the hidden variable <i>x</i>(<i>t</i>) at time <span class="texhtml mvar" style="font-style:italic;">t</span>, given the values of the hidden variable <span class="texhtml mvar" style="font-style:italic;">x</span> at all times, depends <i>only</i> on the value of the hidden variable <i>x</i>(<i>t</i> − 1); the values at time <i>t</i> − 2 and before have no influence. This is called the Markov property. Similarly, the value of the observed variable <i>y</i>(<i>t</i>) only depends on the value of the hidden variable <i>x</i>(<i>t</i>) (both at time <span class="texhtml mvar" style="font-style:italic;">t</span>).
</p><p>In the standard type of hidden Markov model considered here, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution).  The parameters of a hidden Markov model are of two types, <i>transition probabilities</i> and <i>emission probabilities</i> (also known as <i>output probabilities</i>).  The transition probabilities control the way the hidden state at time <span class="texhtml mvar" style="font-style:italic;">t</span> is chosen given the hidden state at time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t-1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t-1}</annotation>
</semantics>
</math></span><img alt="t-1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a215d9553945bb84b3b5a79cc796fb7d6e0629f0" style="vertical-align: -0.505ex; width:4.842ex; height:2.343ex;"/></span>.
</p><p>The hidden state space is assumed to consist of one of <span class="texhtml mvar" style="font-style:italic;">N</span> possible values, modelled as a categorical distribution. (See the section below on extensions for other possibilities.) This means that for each of the <span class="texhtml mvar" style="font-style:italic;">N</span> possible states that a hidden variable at time <span class="texhtml mvar" style="font-style:italic;">t</span> can be in, there is a transition probability from this state to each of the <span class="texhtml mvar" style="font-style:italic;">N</span> possible states of the hidden variable at time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle t+1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>t</mi>
<mo>+</mo>
<mn>1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle t+1}</annotation>
</semantics>
</math></span><img alt="t+1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ab2785d8415d6902b0c93efe1419c4bc3ce4643d" style="vertical-align: -0.505ex; width:4.842ex; height:2.343ex;"/></span>, for a total of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle N^{2}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle N^{2}}</annotation>
</semantics>
</math></span><img alt="N^{2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fe131b76af8a2bc86e01b14a7ba843db69c1a164" style="vertical-align: -0.338ex; width:3.177ex; height:2.676ex;"/></span> transition probabilities. Note that the set of transition probabilities for transitions from any given state must sum to 1. Thus, the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle N\times N}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>N</mi>
<mo>×<!-- × --></mo>
<mi>N</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle N\times N}</annotation>
</semantics>
</math></span><img alt="N\times N" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/99a86c5231bb3cbb863d9d428ebe9ac8db8d4ffb" style="vertical-align: -0.338ex; width:6.968ex; height:2.176ex;"/></span> matrix of transition probabilities is a Markov matrix. Because any transition probability can be determined once the others are known, there are a total of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle N(N-1)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>N</mi>
<mo stretchy="false">(</mo>
<mi>N</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle N(N-1)}</annotation>
</semantics>
</math></span><img alt="N(N-1)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f1fee54b95983b9c3f7403047c1cfc4af3d43c5" style="vertical-align: -0.838ex; width:9.939ex; height:2.843ex;"/></span> transition parameters.
</p><p>In addition, for each of the <span class="texhtml mvar" style="font-style:italic;">N</span> possible states, there is a set of emission probabilities governing the distribution of the observed variable at a particular time given the state of the hidden variable at that time.  The size of this set depends on the nature of the observed variable.  For example, if the observed variable is discrete with <span class="texhtml mvar" style="font-style:italic;">M</span> possible values, governed by a categorical distribution, there will be <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M-1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>M</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M-1}</annotation>
</semantics>
</math></span><img alt="M-1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a0ff0c82e48914e34b3c3bd227cf4d09a2fb5eb7" style="vertical-align: -0.505ex; width:6.445ex; height:2.343ex;"/></span> separate parameters, for a total of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle N(M-1)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>N</mi>
<mo stretchy="false">(</mo>
<mi>M</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle N(M-1)}</annotation>
</semantics>
</math></span><img alt="N(M-1)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/53e1437b1c4092d3415c638d73fbcecc74a4df3d" style="vertical-align: -0.838ex; width:10.318ex; height:2.843ex;"/></span> emission parameters over all hidden states.  On the other hand, if the observed variable is an <span class="texhtml mvar" style="font-style:italic;">M</span>-dimensional vector distributed according to an arbitrary multivariate Gaussian distribution, there will be <span class="texhtml mvar" style="font-style:italic;">M</span> parameters controlling the means and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\frac {M(M+1)}{2}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mi>M</mi>
<mo stretchy="false">(</mo>
<mi>M</mi>
<mo>+</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
<mn>2</mn>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\frac {M(M+1)}{2}}}</annotation>
</semantics>
</math></span><img alt="{\frac {M(M+1)}{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/18dcc1b051fcb4bfe62daa4696df064d873905dd" style="vertical-align: -1.838ex; width:11.533ex; height:5.676ex;"/></span> parameters controlling the covariance matrix, for a total of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle N\left(M+{\frac {M(M+1)}{2}}\right)={\frac {NM(M+3)}{2}}=O(NM^{2})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>N</mi>
<mrow>
<mo>(</mo>
<mrow>
<mi>M</mi>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mi>M</mi>
<mo stretchy="false">(</mo>
<mi>M</mi>
<mo>+</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
<mn>2</mn>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mi>N</mi>
<mi>M</mi>
<mo stretchy="false">(</mo>
<mi>M</mi>
<mo>+</mo>
<mn>3</mn>
<mo stretchy="false">)</mo>
</mrow>
<mn>2</mn>
</mfrac>
</mrow>
<mo>=</mo>
<mi>O</mi>
<mo stretchy="false">(</mo>
<mi>N</mi>
<msup>
<mi>M</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle N\left(M+{\frac {M(M+1)}{2}}\right)={\frac {NM(M+3)}{2}}=O(NM^{2})}</annotation>
</semantics>
</math></span><img alt="N\left(M+{\frac {M(M+1)}{2}}\right)={\frac {NM(M+3)}{2}}=O(NM^{2})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fea9b108de1b00ed6a13b5dee407a894798ea7c0" style="vertical-align: -2.505ex; width:51.68ex; height:6.343ex;"/></span> emission parameters. (In such a case, unless the value of <span class="texhtml mvar" style="font-style:italic;">M</span> is small, it may be more practical to restrict the nature of the covariances between individual elements of the observation vector, e.g. by assuming that the elements are independent of each other, or less restrictively, are independent of all but a fixed number of adjacent elements.)
</p>

<h2><span class="mw-headline" id="Inference">Inference</span><span class="mw-editsection"></span></h2>

<p>Several inference problems are associated with hidden Markov models, as outlined below.
</p>
<h3><span class="mw-headline" id="Probability_of_an_observed_sequence">Probability of an observed sequence</span><span class="mw-editsection"></span></h3>
<p>The task is to compute in a best way, given the parameters of the model, the probability of a particular output sequence.  This requires summation over all possible state sequences:
</p><p>The probability of observing a sequence
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Y=y(0),y(1),\dots ,y(L-1)\,}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Y</mi>
<mo>=</mo>
<mi>y</mi>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mi>y</mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<mi>y</mi>
<mo stretchy="false">(</mo>
<mi>L</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mspace width="thinmathspace"></mspace>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Y=y(0),y(1),\dots ,y(L-1)\,}</annotation>
</semantics>
</math></span><img alt="Y=y(0),y(1),\dots ,y(L-1)\," aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a4b513af58f26432a7d7a0356e4ec571270c2873" style="vertical-align: -0.838ex; width:28.276ex; height:2.843ex;"/></span></dd></dl>
<p>of length <i>L</i> is given by
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P(Y)=\sum _{X}P(Y\mid X)P(X),\,}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>Y</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<munder>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>X</mi>
</mrow>
</munder>
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>Y</mi>
<mo>∣<!-- ∣ --></mo>
<mi>X</mi>
<mo stretchy="false">)</mo>
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>X</mi>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mspace width="thinmathspace"></mspace>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P(Y)=\sum _{X}P(Y\mid X)P(X),\,}</annotation>
</semantics>
</math></span><img alt="P(Y)=\sum _{X}P(Y\mid X)P(X),\," aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/62433038e2fa335317993d8deb20c0be53b416c4" style="vertical-align: -3.005ex; width:27.982ex; height:5.509ex;"/></span></dd></dl>
<p>where the sum runs over all possible hidden-node sequences
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle X=x(0),x(1),\dots ,x(L-1).\,}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>X</mi>
<mo>=</mo>
<mi>x</mi>
<mo stretchy="false">(</mo>
<mn>0</mn>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mi>x</mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<mi>x</mi>
<mo stretchy="false">(</mo>
<mi>L</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo>.</mo>
<mspace width="thinmathspace"></mspace>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle X=x(0),x(1),\dots ,x(L-1).\,}</annotation>
</semantics>
</math></span><img alt="X=x(0),x(1),\dots ,x(L-1).\," aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04ddfb5ccab85138570048a2b8ad576cfbd2d4d8" style="vertical-align: -0.838ex; width:29.652ex; height:2.843ex;"/></span></dd></dl>
<p>Applying the principle of dynamic programming, this problem, too, can be handled efficiently using the forward algorithm.
</p>
<h3><span class="mw-headline" id="Probability_of_the_latent_variables">Probability of the latent variables</span><span class="mw-editsection"></span></h3>
<p>A number of related tasks ask about the probability of one or more of the latent variables, given the model's parameters and a sequence of observations <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle y(1),\dots ,y(t).}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>y</mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<mi>y</mi>
<mo stretchy="false">(</mo>
<mi>t</mi>
<mo stretchy="false">)</mo>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle y(1),\dots ,y(t).}</annotation>
</semantics>
</math></span><img alt="y(1),\dots ,y(t)." aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a535986e8c88ca6726fd000c129a7b5b83f946bb" style="vertical-align: -0.838ex; width:13.757ex; height:2.843ex;"/></span>
</p>
<h4><span class="mw-headline" id="Filtering">Filtering</span><span class="mw-editsection"></span></h4>
<p>The task is to compute, given the model's parameters and a sequence of observations, the distribution over hidden states of the last latent variable at the end of the sequence, i.e. to compute <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P(x(t)\ |\ y(1),\dots ,y(t))}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">(</mo>
<mi>t</mi>
<mo stretchy="false">)</mo>
<mtext> </mtext>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mtext> </mtext>
<mi>y</mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<mi>y</mi>
<mo stretchy="false">(</mo>
<mi>t</mi>
<mo stretchy="false">)</mo>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P(x(t)\ |\ y(1),\dots ,y(t))}</annotation>
</semantics>
</math></span><img alt="P(x(t)\ |\ y(1),\dots ,y(t))" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/84c378ce7388d8acfb81dedc1d76763fd8a8a06a" style="vertical-align: -0.838ex; width:22.451ex; height:2.843ex;"/></span>.  This task is normally used when the sequence of latent variables is thought of as the underlying states that a process moves through at a sequence of points of time, with corresponding observations at each point in time.  Then, it is natural to ask about the state of the process at the end.
</p><p>This problem can be handled efficiently using the forward algorithm.
</p>
<h4><span class="mw-headline" id="Smoothing">Smoothing</span><span class="mw-editsection"></span></h4>
<p>This is similar to filtering but asks about the distribution of a latent variable somewhere in the middle of a sequence, i.e. to compute <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P(x(k)\ |\ y(1),\dots ,y(t))}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>x</mi>
<mo stretchy="false">(</mo>
<mi>k</mi>
<mo stretchy="false">)</mo>
<mtext> </mtext>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mtext> </mtext>
<mi>y</mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<mi>y</mi>
<mo stretchy="false">(</mo>
<mi>t</mi>
<mo stretchy="false">)</mo>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P(x(k)\ |\ y(1),\dots ,y(t))}</annotation>
</semantics>
</math></span><img alt="P(x(k)\ |\ y(1),\dots ,y(t))" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4861e645dc5e7cbca8e87e703ab7ff35e93a37aa" style="vertical-align: -0.838ex; width:22.823ex; height:2.843ex;"/></span> for some <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle k&lt;t}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>k</mi>
<mo>&lt;</mo>
<mi>t</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle k&lt;t}</annotation>
</semantics>
</math></span><img alt="k&lt;t" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7ca1a3ea2d76668acdda69e481b958420402d408" style="vertical-align: -0.338ex; width:5.149ex; height:2.176ex;"/></span>.  From the perspective described above, this can be thought of as the probability distribution over hidden states for a point in time <i>k</i> in the past, relative to time <i>t</i>.
</p><p>The forward-backward algorithm is a good method for computing the smoothed values for all hidden state variables.
</p>
<h4><span class="mw-headline" id="Most_likely_explanation">Most likely explanation</span><span class="mw-editsection"></span></h4>
<p>The task, unlike the previous two, asks about the joint probability of the <i>entire</i> sequence of hidden states that generated a particular sequence of observations (see illustration on the right).  This task is generally applicable when HMM's are applied to different sorts of problems from those for which the tasks of filtering and smoothing are applicable.  An example is part-of-speech tagging, where the hidden states represent the underlying parts of speech corresponding to an observed sequence of words.  In this case, what is of interest is the entire sequence of parts of speech, rather than simply the part of speech for a single word, as filtering or smoothing would compute.
</p><p>This task requires finding a maximum over all possible state sequences, and can be solved efficiently by the Viterbi algorithm.
</p>
<h3><span class="mw-headline" id="Statistical_significance">Statistical significance</span><span class="mw-editsection"></span></h3>
<p>For some of the above problems, it may also be interesting to ask about statistical significance.  What is the probability that a sequence drawn from some null distribution will have an HMM probability (in the case of the forward algorithm) or a maximum state sequence probability (in the case of the Viterbi algorithm) at least as large as that of a particular output sequence?<sup class="reference" id="cite_ref-7">[7]</sup>  When an HMM is used to evaluate the relevance of a hypothesis for a particular output sequence, the statistical significance indicates the false positive rate associated with failing to reject the hypothesis for the output sequence.
</p>
<h2><span class="mw-headline" id="Learning">Learning</span><span class="mw-editsection"></span></h2>
<p>The parameter learning task in HMMs is to find, given an output sequence or a set of such sequences, the best set of state transition and emission probabilities. The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the set of output sequences. No tractable algorithm is known for solving this problem exactly, but a local maximum likelihood can be derived efficiently using the Baum–Welch algorithm or the Baldi–Chauvin algorithm.  The Baum–Welch algorithm is a special case of the expectation-maximization algorithm. 
</p><p>If the HMMs are used for time series prediction, more sophisticated Bayesian inference methods, like Markov chain Monte Carlo (MCMC) sampling are proven to be favorable over finding a single maximum likelihood model both in terms of accuracy and stability.<sup class="reference" id="cite_ref-8">[8]</sup> Since MCMC imposes significant computational burden, in cases where computational scalability is also of interest, one may alternatively resort to variational approximations to Bayesian inference, e.g.<sup class="reference" id="cite_ref-9">[9]</sup> Indeed, approximate variational inference offers computational efficiency comparable to expectation-maximization, while yielding an accuracy profile only slightly inferior to exact MCMC-type Bayesian inference.
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"></span></h2>

<p>HMMs can be applied in many fields where the goal is to recover a data sequence that is not immediately observable (but other data that depend on the sequence are). Applications include:
</p>
<ul><li>Computational finance<sup class="reference" id="cite_ref-10">[10]</sup><sup class="reference" id="cite_ref-11">[11]</sup></li>
<li>Single-molecule kinetic analysis<sup class="reference" id="cite_ref-12">[12]</sup></li>
<li>Neuroscience<sup class="reference" id="cite_ref-13">[13]</sup></li>
<li>Cryptanalysis</li>
<li>Speech recognition, including Siri<sup class="reference" id="cite_ref-14">[14]</sup></li>
<li>Speech synthesis</li>
<li>Part-of-speech tagging</li>
<li>Document separation in scanning solutions</li>
<li>Machine translation</li>
<li>Partial discharge</li>
<li>Gene prediction</li>
<li>Handwriting recognition<sup class="reference" id="cite_ref-15">[15]</sup></li>
<li>Alignment of bio-sequences</li>
<li>Time series analysis</li>
<li>Activity recognition</li>
<li>Protein folding<sup class="reference" id="cite_ref-16">[16]</sup></li>
<li>Sequence classification<sup class="reference" id="cite_ref-17">[17]</sup></li>
<li>Metamorphic virus detection<sup class="reference" id="cite_ref-18">[18]</sup></li>
<li>DNA motif discovery<sup class="reference" id="cite_ref-19">[19]</sup></li>
<li>DNA hybridization kinetics<sup class="reference" id="cite_ref-20">[20]</sup><sup class="reference" id="cite_ref-21">[21]</sup></li>
<li>Chromatin state discovery<sup class="reference" id="cite_ref-22">[22]</sup></li>
<li>Transportation forecasting<sup class="reference" id="cite_ref-23">[23]</sup></li>
<li>Solar irradiance variability <sup class="reference" id="cite_ref-24">[24]</sup><sup class="reference" id="cite_ref-25">[25]</sup><sup class="reference" id="cite_ref-26">[26]</sup></li></ul>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"></span></h2>
<p>Hidden Markov models were described in a series of statistical papers by Leonard E. Baum and other authors in the second half of the 1960s.<sup class="reference" id="cite_ref-27">[27]</sup><sup class="reference" id="cite_ref-28">[28]</sup><sup class="reference" id="cite_ref-29">[29]</sup><sup class="reference" id="cite_ref-30">[30]</sup><sup class="reference" id="cite_ref-31">[31]</sup> One of the first applications of HMMs was speech recognition, starting in the mid-1970s.<sup class="reference" id="cite_ref-32">[32]</sup><sup class="reference" id="cite_ref-33">[33]</sup><sup class="reference" id="cite_ref-34">[34]</sup><sup class="reference" id="cite_ref-35">[35]</sup>
</p><p>In the second half of the 1980s, HMMs began to be applied to the analysis of biological sequences,<sup class="reference" id="cite_ref-36">[36]</sup> in particular DNA. Since then, they have become ubiquitous in the field of bioinformatics.<sup class="reference" id="cite_ref-durbin_37-0">[37]</sup>
</p>
<h2><span class="mw-headline" id="Extensions">Extensions</span><span class="mw-editsection"></span></h2>
<p>In the hidden Markov models considered above, the state space of the hidden variables is discrete, while the observations themselves can either be discrete (typically generated from a categorical distribution) or continuous (typically from a Gaussian distribution). Hidden Markov models can also be generalized to allow continuous state spaces. Examples of such models are those where the Markov process over hidden variables is a linear dynamical system, with a linear relationship among related variables and where all hidden and observed variables follow a Gaussian distribution. In simple cases, such as the linear dynamical system just mentioned, exact inference is tractable (in this case, using the Kalman filter); however, in general, exact inference in HMMs with continuous latent variables is infeasible, and approximate methods must be used, such as the extended Kalman filter or the particle filter.
</p><p>Hidden Markov models are generative models, in which the joint distribution of observations and hidden states, or equivalently both the prior distribution of hidden states (the <i>transition probabilities</i>) and conditional distribution of observations given states (the <i>emission probabilities</i>), is modeled.  The above algorithms implicitly assume a uniform prior distribution over the transition probabilities. However, it is also possible to create hidden Markov models with other types of prior distributions. An obvious candidate, given the categorical distribution of the transition probabilities, is the Dirichlet distribution, which is the conjugate prior distribution of the categorical distribution.  Typically, a symmetric Dirichlet distribution is chosen, reflecting ignorance about which states are inherently more likely than others.  The single parameter of this distribution (termed the <i>concentration parameter</i>) controls the relative density or sparseness of the resulting transition matrix.  A choice of 1 yields a uniform distribution.  Values greater than 1 produce a dense matrix, in which the transition probabilities between pairs of states are likely to be nearly equal.  Values less than 1 result in a sparse matrix in which, for each given source state, only a small number of destination states have non-negligible transition probabilities.  It is also possible to use a two-level prior Dirichlet distribution, in which one Dirichlet distribution (the upper distribution) governs the parameters of another Dirichlet distribution (the lower distribution), which in turn governs the transition probabilities.  The upper distribution governs the overall distribution of states, determining how likely each state is to occur; its concentration parameter determines the density or sparseness of states.  Such a two-level prior distribution, where both concentration parameters are set to produce sparse distributions, might be useful for example in unsupervised part-of-speech tagging, where some parts of speech occur much more commonly than others; learning algorithms that assume a uniform prior distribution generally perform poorly on this task.  The parameters of models of this sort, with non-uniform prior distributions, can be learned using Gibbs sampling or extended versions of the expectation-maximization algorithm.
</p><p>An extension of the previously described hidden Markov models with Dirichlet priors uses a Dirichlet process in place of a Dirichlet distribution.  This type of model allows for an unknown and potentially infinite number of states.  It is common to use a two-level Dirichlet process, similar to the previously described model with two levels of Dirichlet distributions.  Such a model is called a <i>hierarchical Dirichlet process hidden Markov model</i>, or <i>HDP-HMM</i> for short. It was originally described under the name "Infinite Hidden Markov Model"<sup class="reference plainlinks nourlexpansion" id="ref_Beal,_Matthew_J.,_Zoubin_Ghahramani,_and_Carl_Edward_Rasmussen.">[4]</sup> and was further formalized in<sup class="reference plainlinks nourlexpansion" id="ref_Teh,_Yee_Whye,_et_al.">[5]</sup>.
</p><p>A different type of extension uses a discriminative model in place of the generative model of standard HMMs.  This type of model directly models the conditional distribution of the hidden states given the observations, rather than modeling the joint distribution.  An example of this model is the so-called <i>maximum entropy Markov model</i> (MEMM), which models the conditional distribution of the states using logistic regression (also known as a "maximum entropy model").  The advantage of this type of model is that arbitrary features (i.e. functions) of the observations can be modeled, allowing domain-specific knowledge of the problem at hand to be injected into the model.  Models of this sort are not limited to modeling direct dependencies between a hidden state and its associated observation; rather, features of nearby observations, of combinations of the associated observation and nearby observations, or in fact of arbitrary observations at any distance from a given hidden state can be included in the process used to determine the value of a hidden state.  Furthermore, there is no need for these features to be statistically independent of each other, as would be the case if such features were used in a generative model.  Finally, arbitrary features over pairs of adjacent hidden states can be used rather than simple transition probabilities.  The disadvantages of such models are: (1) The types of prior distributions that can be placed on hidden states are severely limited; (2) It is not possible to predict the probability of seeing an arbitrary observation.  This second limitation is often not an issue in practice, since many common usages of HMM's do not require such predictive probabilities.
</p><p>A variant of the previously described discriminative model is the linear-chain conditional random field.  This uses an undirected graphical model (aka Markov random field) rather than the directed graphical models of MEMM's and similar models.  The advantage of this type of model is that it does not suffer from the so-called <i>label bias</i> problem of MEMM's, and thus may make more accurate predictions.  The disadvantage is that training can be slower than for MEMM's.
</p><p>Yet another variant is the <i>factorial hidden Markov model</i>, which allows for a single observation to be conditioned on the corresponding hidden variables of a set of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle K}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>K</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle K}</annotation>
</semantics>
</math></span><img alt="K" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;"/></span> independent Markov chains, rather than a single Markov chain. It is equivalent to a single HMM, with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle N^{K}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>K</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle N^{K}}</annotation>
</semantics>
</math></span><img alt="N^{K}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4cda0a35f9eb8276d799071a218a515391019a42" style="vertical-align: -0.338ex; width:3.816ex; height:2.676ex;"/></span> states (assuming there are <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle N}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>N</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle N}</annotation>
</semantics>
</math></span><img alt="N" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3" style="vertical-align: -0.338ex; width:2.064ex; height:2.176ex;"/></span> states for each chain), and therefore, learning in such a model is difficult: for a sequence of length <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle T}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>T</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle T}</annotation>
</semantics>
</math></span><img alt="T" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ec7200acd984a1d3a3d7dc455e262fbe54f7f6e0" style="vertical-align: -0.338ex; width:1.636ex; height:2.176ex;"/></span>, a straightforward Viterbi algorithm has complexity <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(N^{2K}\,T)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<msup>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
<mi>K</mi>
</mrow>
</msup>
<mspace width="thinmathspace"></mspace>
<mi>T</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(N^{2K}\,T)}</annotation>
</semantics>
</math></span><img alt="O(N^{2K}\,T)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b874841ca8bf26f386805f273a4d87d43c1cc867" style="vertical-align: -0.838ex; width:10.244ex; height:3.176ex;"/></span>. To find an exact solution, a junction tree algorithm could be used, but it results in an <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(N^{K+1}\,K\,T)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<msup>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>K</mi>
<mo>+</mo>
<mn>1</mn>
</mrow>
</msup>
<mspace width="thinmathspace"></mspace>
<mi>K</mi>
<mspace width="thinmathspace"></mspace>
<mi>T</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(N^{K+1}\,K\,T)}</annotation>
</semantics>
</math></span><img alt="O(N^{K+1}\,K\,T)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/830574b6e885441d00387d772cbaea341d85c3cd" style="vertical-align: -0.838ex; width:13.976ex; height:3.176ex;"/></span> complexity. In practice, approximate techniques, such as variational approaches, could be used.<sup class="reference" id="cite_ref-38">[38]</sup>
</p><p>All of the above models can be extended to allow for more distant dependencies among hidden states, e.g. allowing for a given state to be dependent on the previous two or three states rather than a single previous state; i.e. the transition probabilities are extended to encompass sets of three or four adjacent states (or in general <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle K}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>K</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle K}</annotation>
</semantics>
</math></span><img alt="K" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;"/></span> adjacent states).  The disadvantage of such models is that dynamic-programming algorithms for training them have an <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(N^{K}\,T)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<msup>
<mi>N</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>K</mi>
</mrow>
</msup>
<mspace width="thinmathspace"></mspace>
<mi>T</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(N^{K}\,T)}</annotation>
</semantics>
</math></span><img alt="O(N^{K}\,T)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60f4bf98121cb8c500aad219064d7e98930c8282" style="vertical-align: -0.838ex; width:9.422ex; height:3.176ex;"/></span> running time, for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle K}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>K</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle K}</annotation>
</semantics>
</math></span><img alt="K" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;"/></span> adjacent states and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle T}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>T</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle T}</annotation>
</semantics>
</math></span><img alt="T" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ec7200acd984a1d3a3d7dc455e262fbe54f7f6e0" style="vertical-align: -0.338ex; width:1.636ex; height:2.176ex;"/></span> total observations (i.e. a length-<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle T}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>T</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle T}</annotation>
</semantics>
</math></span><img alt="T" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ec7200acd984a1d3a3d7dc455e262fbe54f7f6e0" style="vertical-align: -0.338ex; width:1.636ex; height:2.176ex;"/></span> Markov chain).
</p><p>Another recent extension is the <i>triplet Markov model</i>,<sup class="reference" id="cite_ref-TMM_39-0">[39]</sup> in which an auxiliary underlying process is added to model some data specificities. Many variants of this model have been proposed. One should also mention the interesting link that has been established between the <i>theory of evidence</i> and the <i>triplet Markov models</i><sup class="reference" id="cite_ref-TMMEV_40-0">[40]</sup> and which allows to fuse data in Markovian context<sup class="reference" id="cite_ref-JASP_41-0">[41]</sup> and to model nonstationary data.<sup class="reference" id="cite_ref-TSP_42-0">[42]</sup><sup class="reference" id="cite_ref-SPL_43-0">[43]</sup> Note that alternative multi-stream data fusion strategies have also been proposed in the recent literature, e.g.<sup class="reference" id="cite_ref-44">[44]</sup>
</p><p>Finally, a different rationale towards addressing the problem of modeling nonstationary data by means of hidden Markov models was suggested in 2012.<sup class="reference" id="cite_ref-Reservoir-HMM_45-0">[45]</sup> It consists in employing a small recurrent neural network (RNN), specifically a reservoir network,<sup class="reference" id="cite_ref-46">[46]</sup> to capture the evolution of the temporal dynamics in the observed data. This information, encoded in the form of a high-dimensional vector, is used as a conditioning variable of the HMM state transition probabilities. Under such a setup, we eventually obtain a nonstationary HMM the transition probabilities of which evolve over time in a manner that is inferred from the data itself, as opposed to some unrealistic ad-hoc model of temporal evolution.
</p><p>The model suitable in the context of longitudinal data is named latent Markov model.<sup class="reference" id="cite_ref-47">[47]</sup> The basic version of this model has been extended to include individual covariates, random effects and to model more complex data structures such as multilevel data. A complete overview of the latent Markov models, with special attention to the model assumptions and  to their practical use is provided in<sup class="reference" id="cite_ref-48">[48]</sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r998391716">.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}</style>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1097025294">.mw-parser-output .side-box{margin:4px 0;box-sizing:border-box;border:1px solid #aaa;font-size:88%;line-height:1.25em;background-color:#f9f9f9}.mw-parser-output .side-box-abovebelow,.mw-parser-output .side-box-text{padding:0.25em 0.9em}.mw-parser-output .side-box-image{padding:2px 0 2px 0.9em;text-align:center}.mw-parser-output .side-box-imageright{padding:2px 0.9em 2px 0;text-align:center}@media(min-width:500px){.mw-parser-output .side-box-flex{display:flex;align-items:center}.mw-parser-output .side-box-text{flex:1}}@media(min-width:720px){.mw-parser-output .side-box{width:238px}.mw-parser-output .side-box-right{clear:right;float:right;margin-left:1em}.mw-parser-output .side-box-left{margin-right:1em}}</style>
<h3><span class="mw-headline" id="Concepts">Concepts</span><span class="mw-editsection"></span></h3>
<ul><li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation journal cs1" id="CITEREFTeifRippe2010">Teif, V. B.; Rippe, K. (2010). "Statistical–mechanical lattice models for protein–DNA binding in chromatin". <i>J. Phys.: Condens. Matter</i>. <b>22</b> (41): 414105. arXiv:<span class="cs1-lock-free" title="Freely accessible">1004.5514</span>. Bibcode:2010JPCM...22O4105T. doi:10.1088/0953-8984/22/41/414105. PMID 21386588. S2CID 103345.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=J.+Phys.%3A+Condens.+Matter&amp;rft.atitle=Statistical%E2%80%93mechanical+lattice+models+for+protein%E2%80%93DNA+binding+in+chromatin&amp;rft.volume=22&amp;rft.issue=41&amp;rft.pages=414105&amp;rft.date=2010&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A103345%23id-name%3DS2CID&amp;rft_id=info%3Abibcode%2F2010JPCM...22O4105T&amp;rft_id=info%3Aarxiv%2F1004.5514&amp;rft_id=info%3Apmid%2F21386588&amp;rft_id=info%3Adoi%2F10.1088%2F0953-8984%2F22%2F41%2F414105&amp;rft.aulast=Teif&amp;rft.aufirst=V.+B.&amp;rft.au=Rippe%2C+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AHidden+Markov+model"></span></li>
<li>A Revealing Introduction to Hidden Markov Models by Mark Stamp, San Jose State University.</li>
<li>Fitting HMM's with expectation-maximization – complete derivation</li>
<li>A step-by-step tutorial on HMMs Archived 2017-08-13 at the Wayback Machine <i>(University of Leeds)</i></li>
<li>Hidden Markov Models <i>(an exposition using basic mathematics)</i></li>
<li>Hidden Markov Models <i>(by Narada Warakagoda)</i></li>
<li>Hidden Markov Models: Fundamentals and Applications Part 1, Part 2 <i>(by V. Petrushin)</i></li>
<li>Lecture on a Spreadsheet by Jason Eisner, Video and interactive spreadsheet</li></ul>


<!-- 
NewPP limit report
Parsed by mw1433
Cached time: 20221223231015
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.753 seconds
Real time usage: 0.972 seconds
Preprocessor visited node count: 3472/1000000
Post‐expand include size: 134575/2097152 bytes
Template argument size: 2491/2097152 bytes
Highest expansion depth: 17/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 144925/5000000 bytes
Lua time usage: 0.411/10.000 seconds
Lua memory usage: 6531074/52428800 bytes
Number of Wikibase entities loaded: 1/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  678.009      1 -total
 64.62%  438.133      1 Template:Reflist
 45.40%  307.831     31 Template:Cite_journal
  7.99%   54.145      1 Template:Short_description
  6.48%   43.902      1 Template:Stochastic_processes
  6.14%   41.622      1 Template:Navbox
  5.37%   36.443      7 Template:Main_other
  5.10%   34.561      1 Template:Commons_category
  4.82%   32.707      1 Template:Sister_project
  4.69%   31.808      1 Template:Authority_control
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:98770-0!canonical and timestamp 20221223231014 and revision id 1127218464.
 -->
</div></body>
</html>