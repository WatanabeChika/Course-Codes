<!DOCTYPE html>
<html>
<head>
<title>prisoner's_dilemma</title>
</head>
<body>
<div class="mw-parser-output">
<style data-mw-deduplicate="TemplateStyles:r1033289096">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style>
<link href="mw-data:TemplateStyles:r1033289096" rel="mw-deduplicated-inline-style"/>
<style data-mw-deduplicate="TemplateStyles:r1097763485">.mw-parser-output .ambox{border:1px solid #a2a9b1;border-left:10px solid #36c;background-color:#fbfbfb;box-sizing:border-box}.mw-parser-output .ambox+link+.ambox,.mw-parser-output .ambox+link+style+.ambox,.mw-parser-output .ambox+link+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+style+.ambox,.mw-parser-output .ambox+.mw-empty-elt+link+link+.ambox{margin-top:-1px}html body.mediawiki .mw-parser-output .ambox.mbox-small-left{margin:4px 1em 4px 0;overflow:hidden;width:238px;border-collapse:collapse;font-size:88%;line-height:1.25em}.mw-parser-output .ambox-speedy{border-left:10px solid #b32424;background-color:#fee7e6}.mw-parser-output .ambox-delete{border-left:10px solid #b32424}.mw-parser-output .ambox-content{border-left:10px solid #f28500}.mw-parser-output .ambox-style{border-left:10px solid #fc3}.mw-parser-output .ambox-move{border-left:10px solid #9932cc}.mw-parser-output .ambox-protection{border-left:10px solid #a2a9b1}.mw-parser-output .ambox .mbox-text{border:none;padding:0.25em 0.5em;width:100%}.mw-parser-output .ambox .mbox-image{border:none;padding:2px 0 2px 0.5em;text-align:center}.mw-parser-output .ambox .mbox-imageright{border:none;padding:2px 0.5em 2px 0;text-align:center}.mw-parser-output .ambox .mbox-empty-cell{border:none;padding:0;width:1px}.mw-parser-output .ambox .mbox-image-div{width:52px}html.client-js body.skin-minerva .mw-parser-output .mbox-text-span{margin-left:23px!important}@media(min-width:720px){.mw-parser-output .ambox{margin:0 10%}}</style><table class="box-Copy_edit plainlinks metadata ambox ambox-style ambox-Copy_edit" role="presentation"><tbody><tr><td class="mbox-image"></td><td class="mbox-text"></td></tr></tbody></table>
<table class="wikitable floatright">
<caption>Standard prisoner's dilemma payoff matrix
</caption>
<tbody><tr>
<th style="background:#EAECF0;background:linear-gradient(to top right,#EAECF0 49%,#AAA 49.5%,#AAA 50.5%,#EAECF0 51%);line-height:1.2;padding:0.1em 0.4em;">
</th>
<th>B stays<br/>silent
</th>
<th>B<br/>betrays
</th></tr>
<tr>
<th>A stays<br/>silent
</th>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td></tr>
<tr>
<th>A<br/>betrays
</th>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td></tr></tbody></table>
<p>The <b>Prisoner's Dilemma</b> is an example of a game analyzed in game theory<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><span title="claims to be an example of a game that is analysed via game theory, I removed the word stand from proceeding example, as it said a standard example of ... (October 2022)">citation needed</span></i>]</sup>. It is also a thought experiment that challenges two completely rational agents to a dilemma: cooperate with their partner for mutual reward, or betray their partner ("defect") for individual reward.
</p><p>
This dilemma was originally framed by Merrill Flood and Melvin Dresher while working at RAND in 1950<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><span title="it's a claim that's not cited, except with links to badly written democratic wiki pages (October 2022)">citation needed</span></i>]</sup>. Albert W. Tucker appropriated the game and formalized it by structuring the rewards in terms of prison sentences and named it "prisoner's dilemma".<sup class="reference" id="cite_ref-FOOTNOTEPoundstone19938,_117_1-0">[1]</sup> William Poundstone in his 1993 book <i>Prisoner's Dilemma</i> writes the following version:</p><blockquote><p>Two members of a criminal gang are arrested and imprisoned. Each prisoner is in solitary confinement with no means of speaking to or exchanging messages with the other. The police admit they don't have enough evidence to convict the pair on the principal charge. They plan to sentence both to two years in prison on a lesser charge. Simultaneously, the police offer each prisoner a Faustian bargain.</p></blockquote><p>The possible outcomes are:
</p><ul><li>A: If A and B each betray the other, each of them serves 5 years in prison</li>
<li>B: If A betrays B but B remains silent, A will be set free and B will serve 10 years in prison</li>
<li>C: If A remains silent but B betrays A, A will serve 10 years in prison and B will be set free</li>
<li>D: If A and B both remain silent, both of them will serve 2 years in prison (on the lesser charge).</li></ul>
<p>As a projection of rational behaviour in terms of loyalty to one's partner in crime, the Prisoner's Dilemma suggests that criminals who are offered a greater reward will betray their partner for the reward. 
</p><p>Accepting offers such as B shows that loyalty to one's partner is, in this game, irrational. Rationality in a system that profits from others, governs this behaviour.  Alternative ideas governing behaviour have been proposed, see for example, Elinor Ostrom a Nobel Laureate in Economics. 
</p><p>An assumption of the Prisoner's Dilemma is that all purely rational behaviour is self-interested, and as such people will betray each other for self-interest. This implies the only possible outcome for two purely rational prisoners is for them to betray each other, even though mutual cooperation would yield a greater reward.<sup class="reference" id="cite_ref-2">[2]</sup>
</p><p>In this case, "to betray" is the dominant strategy for both players, meaning it is the player's best response in all circumstances, and it is aligned with the sure-thing principle.<sup class="reference" id="cite_ref-:0_3-0">[3]</sup> The prisoner's dilemma also illustrates that the decisions made under collective rationality may not necessarily be the same as those made under individual rationality, and this conflict can also be witnessed in a situation called the "Tragedy of the Commons". This case indicates the fact that public goods are always prone to over-use.<sup class="reference" id="cite_ref-:0_3-1">[3]</sup>
</p><p>In reality, such systemic bias towards cooperative behavior happens despite what is predicted by simple models of "rational" self-interested action.<sup class="reference" id="cite_ref-Fehr_4-0">[4]</sup><sup class="reference" id="cite_ref-Amos_5-0">[5]</sup><sup class="reference" id="cite_ref-Ahn_6-0">[6]</sup><sup class="reference" id="cite_ref-Hessel_7-0">[7]</sup> This bias towards cooperation has been known since the test was first conducted at RAND; the secretaries involved trusted each other and worked together for the best common outcome.<sup class="reference" id="cite_ref-8">[8]</sup> The prisoner's dilemma became the focus of extensive experimental research.<sup class="reference" id="cite_ref-9">[9]</sup><sup class="reference" id="cite_ref-10">[10]</sup> This experimental research usually takes one of three forms: single play, iterated play and iterated play against a programmed player, each with different purposes.<sup class="reference" id="cite_ref-:0_3-2">[3]</sup> And as a summary of these experiments, their results justify the categorical imperative raised by Immanuel Kant, which states that a rational agent is expected to "act in the way you wish others to act." This theory is vital for a situation when there are different players each acting for their best interest, and has to take others' actions into consideration to form their own choice. It underlines the interconnectedness of players in such a game, thus stressing the fact that a strategy has to consider others' reactions to be successful, including their responsiveness, their tendency to imitate, etc.<sup class="reference" id="cite_ref-:0_3-3">[3]</sup>
</p><p>An extended "iterated" version of the game also exists. In this version, the classic game is played repeatedly between the same prisoners, who continuously have the opportunity to penalize the other for previous decisions. If the number of times the game will be played is known to the players, then by backward induction two classically rational players will betray each other repeatedly, for the same reasons as the single-shot variant. In an infinite or unknown length game there is no fixed optimum strategy, and prisoner's dilemma tournaments have been held to compete and test algorithms for such cases.<sup class="reference" id="cite_ref-11">[11]</sup>
</p><p>The iterated version of the prisoner's dilemma is of particular interest to researchers. Due to its iterative nature, previous researchers observed that the frequency for players to cooperate could change, based on the outcomes of each iteration. Specifically, a player may be less willing to cooperate if their counterpart did not cooperate many times, which renders disappointment. Conversely, as time goes by, cooperation could increase, mainly attributable to the fact that a "tacit agreement" between players has been set up. Another interesting aspect concerning the iterated version of experiment, however, is that this tacit agreement between players has always been established successfully even though the number of iterations is made public to both sides.<sup class="reference" id="cite_ref-:0_3-4">[3]</sup>
</p><p>The prisoner's dilemma game can be used as a model for many real world situations involving cooperative behavior. In casual usage, the label "prisoner's dilemma" may be applied to situations not strictly matching the formal criteria of the classic or iterative games: for instance, those in which two entities could gain important benefits from cooperating or suffer from the failure to do so, but find it difficult or expensive—not necessarily impossible—to coordinate their activities.
</p>

<h2><span id="Strategy_for_the_prisoner.27s_dilemma"></span><span class="mw-headline" id="Strategy_for_the_prisoner's_dilemma">Strategy for the prisoner's dilemma</span><span class="mw-editsection"></span></h2>
<p>Two prisoners are separated into individual rooms and cannot communicate with each other.
The normal game is shown below:
</p>
<table class="wikitable">
<tbody><tr>
<th style="background:#EAECF0;background:linear-gradient(to top right,#EAECF0 49%,#AAA 49.5%,#AAA 50.5%,#EAECF0 51%);line-height:1.2;padding:0.1em 0.4em;"></th>
<th>Prisoner B stays silent<br/>(<i>cooperates</i>)</th>
<th>Prisoner B betrays<br/>(<i>defects</i>)
</th></tr>
<tr>
<th>Prisoner A stays silent<br/>(<i>cooperates</i>)
</th>
<td>Each serves 2 years</td>
<td>Prisoner A: 10 years<br/>Prisoner B: goes free
</td></tr>
<tr>
<th>Prisoner A betrays<br/>(<i>defects</i>)
</th>
<td>Prisoner A: goes free<br/>Prisoner B: 10 years</td>
<td>Each serves 5 years
</td></tr></tbody></table>
<p>It is assumed that both prisoners understand the nature of the game, have no loyalty to each other, and will have no opportunity for retribution or reward outside the game. Regardless of what the other decides, each prisoner gets a higher reward by betraying the other ("defecting"). The reasoning involves analyzing both players' best responses: B will either cooperate or defect. If B cooperates, A should defect, because going free is better than serving 2 years. If B defects, A should also defect, because serving 5 years is better than serving 10. So either way, A should defect since defecting is A's best response regardless of B's strategy. Parallel reasoning will show that B should defect.
</p><p>Because defection always results in a better payoff than cooperation regardless of the other player's choice, it is a strictly dominant strategy for both A and B. Mutual defection is the only strong Nash equilibrium in the game (i.e. the only outcome from which each player could only do worse by unilaterally changing strategy). The dilemma, then, is that mutual cooperation yields a better outcome than mutual defection but is not the rational outcome because the choice to cooperate, from a self-interested perspective, is irrational. Thus, Prisoner's dilemma is a game where the Nash equilibrium is not Pareto efficient.
</p>
<h2><span class="mw-headline" id="Generalized_form">Generalized form</span><span class="mw-editsection"></span></h2>
<p>The structure of the traditional prisoner's dilemma can be generalized from its original prisoner setting. Suppose that the two players are represented by the colors red and blue and that each player chooses to either "cooperate" (stay silent) or "defect" (betray).
</p><p>If both players cooperate, they both receive the reward <i>R</i> for cooperating. If both players defect, they both receive the punishment payoff <i>P</i>. If Blue defects while Red cooperates, then Blue receives the temptation payoff <i>T</i>, while Red receives the "sucker's" payoff, <i>S</i>. Similarly, if Blue cooperates while Red defects, then Blue receives the sucker's payoff <i>S</i>, while Red receives the temptation payoff <i>T</i>.
</p><p>This can be expressed in normal form:
</p>
<table class="wikitable" style="text-align:center">
<caption>Canonical PD payoff matrix
</caption>
<tbody><tr>
<th style="background:#EAECF0;background:linear-gradient(to top right,#EAECF0 49%,#AAA 49.5%,#AAA 50.5%,#EAECF0 51%);line-height:1.2;padding:0.1em 0.4em;">
</th>
<th scope="col" style="width:60px;"><span style="color:#900">Cooperate</span>
</th>
<th scope="col" style="width:60px;"><span style="color:#900">Defect</span>
</th></tr>
<tr>
<th scope="row" style="width:60px;"><span style="color:#009">Cooperate</span>
</th>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td></tr>
<tr>
<th scope="row"><span style="color:#009">Defect</span>
</th>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td></tr></tbody></table>
<p>and to be a prisoner's dilemma game in the strong sense, the following condition must hold for the payoffs:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle T&gt;R&gt;P&gt;S}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>T</mi>
<mo>&gt;</mo>
<mi>R</mi>
<mo>&gt;</mo>
<mi>P</mi>
<mo>&gt;</mo>
<mi>S</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle T&gt;R&gt;P&gt;S}</annotation>
</semantics>
</math></span><img alt="{\displaystyle T&gt;R&gt;P&gt;S}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fe6fac90a83474d2b977ae4ff2075c146534cb60" style="vertical-align: -0.338ex; width:15.94ex; height:2.176ex;"/></span></dd></dl>
<p>The payoff relationship <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle R&gt;P}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>R</mi>
<mo>&gt;</mo>
<mi>P</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle R&gt;P}</annotation>
</semantics>
</math></span><img alt="{\displaystyle R&gt;P}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/37afbcb755b88dab03bced457fbdbcd90d55a904" style="vertical-align: -0.338ex; width:6.608ex; height:2.176ex;"/></span> implies that mutual cooperation is superior to mutual defection, while the payoff relationships <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle T&gt;R}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>T</mi>
<mo>&gt;</mo>
<mi>R</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle T&gt;R}</annotation>
</semantics>
</math></span><img alt="{\displaystyle T&gt;R}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d1fa60c9da906b73422a9f79364c30dda045601" style="vertical-align: -0.338ex; width:6.499ex; height:2.176ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P&gt;S}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo>&gt;</mo>
<mi>S</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P&gt;S}</annotation>
</semantics>
</math></span><img alt="{\displaystyle P&gt;S}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7edfe25ef9f2cf28786fe0526e7104453dbda43" style="vertical-align: -0.338ex; width:6.343ex; height:2.176ex;"/></span> imply that defection is the dominant strategy for both agents.
</p>
<h3><span class="mw-headline" id="Special_case:_donation_game">Special case: donation game</span><span class="mw-editsection"></span></h3>
<p>The "donation game"<sup class="reference" id="cite_ref-Hilbe2013_12-0">[12]</sup> is a form of prisoner's dilemma in which cooperation corresponds to offering the other player a benefit <i>b</i> at a personal cost <i>c</i> with <i>b</i> &gt; <i>c</i>. Defection means offering nothing. The payoff matrix is thus
</p>
<table class="wikitable" style="text-align:center">
<tbody><tr>
<th style="background:#EAECF0;background:linear-gradient(to top right,#EAECF0 49%,#AAA 49.5%,#AAA 50.5%,#EAECF0 51%);line-height:1.2;padding:0.1em 0.4em;">
</th>
<th scope="col" style="width:60px;"><span style="color:#900">Cooperate</span>
</th>
<th scope="col" style="width:60px;"><span style="color:#900">Defect</span>
</th></tr>
<tr>
<th scope="row" style="width:60px;"><span style="color:#009">Cooperate</span>
</th>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td></tr>
<tr>
<th scope="row"><span style="color:#009">Defect</span>
</th>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td></tr></tbody></table>
<p>Note that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle 2R&gt;T+S}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mn>2</mn>
<mi>R</mi>
<mo>&gt;</mo>
<mi>T</mi>
<mo>+</mo>
<mi>S</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle 2R&gt;T+S}</annotation>
</semantics>
</math></span><img alt="{\displaystyle 2R&gt;T+S}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b75e44a0fe5e91b7367b6fe318f0aa3e84066e10" style="vertical-align: -0.505ex; width:12.001ex; height:2.343ex;"/></span> (i.e. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle 2(b-c)&gt;b-c}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mn>2</mn>
<mo stretchy="false">(</mo>
<mi>b</mi>
<mo>−<!-- − --></mo>
<mi>c</mi>
<mo stretchy="false">)</mo>
<mo>&gt;</mo>
<mi>b</mi>
<mo>−<!-- − --></mo>
<mi>c</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle 2(b-c)&gt;b-c}</annotation>
</semantics>
</math></span><img alt="{\displaystyle 2(b-c)&gt;b-c}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86f7c78701c47d21d1eaefe880c4e61e88d7322d" style="vertical-align: -0.838ex; width:15.76ex; height:2.843ex;"/></span>) which qualifies the donation game to be an iterated game (see next section).
</p><p>The donation game may be applied to markets. Suppose X grows oranges, Y grows apples. The marginal utility of an apple to the orange-grower X is <i>b</i>, which is higher than the marginal utility (<i>c</i>) of an orange, since X has a surplus of oranges and no apples. Similarly, for apple-grower Y, the marginal utility of an orange is <i>b</i> while the marginal utility of an apple is <i>c</i>. If X and Y contract to exchange an apple and an orange, and each fulfills their end of the deal, then each receive a payoff of <i>b</i>-<i>c</i>. If one "defects" and does not deliver as promised, the defector will receive a payoff of <i>b</i>, while the cooperator will lose <i>c</i>. If both defect, then neither one gains or loses anything.
</p>
<h2><span id="The_iterated_prisoner.27s_dilemma"></span><span class="mw-headline" id="The_iterated_prisoner's_dilemma">The iterated prisoner's dilemma</span><span class="mw-editsection"></span></h2>
<link href="mw-data:TemplateStyles:r1097763485" rel="mw-deduplicated-inline-style"/><table class="box-More_citations_needed_section plainlinks metadata ambox ambox-content ambox-Refimprove" role="presentation"><tbody><tr><td class="mbox-image"></td><td class="mbox-text"></td></tr></tbody></table>
<p>If two players play prisoner's dilemma more than once in succession and they remember previous actions of their opponent and change their strategy accordingly, the game is called iterated prisoner's dilemma.
</p><p>In addition to the general form above, the iterative version also requires that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle 2R&gt;T+S}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mn>2</mn>
<mi>R</mi>
<mo>&gt;</mo>
<mi>T</mi>
<mo>+</mo>
<mi>S</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle 2R&gt;T+S}</annotation>
</semantics>
</math></span><img alt="{\displaystyle 2R&gt;T+S}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b75e44a0fe5e91b7367b6fe318f0aa3e84066e10" style="vertical-align: -0.505ex; width:12.001ex; height:2.343ex;"/></span>, to prevent alternating cooperation and defection giving a greater reward than mutual cooperation.
</p><p>The iterated prisoner's dilemma game is fundamental to some theories of human cooperation and trust. On the assumption that the game can model transactions between two people requiring trust, cooperative behavior in populations may be modeled by a multi-player, iterated, version of the game. It has, consequently, fascinated many scholars over the years. In 1975, Grofman and Pool estimated the count of scholarly articles devoted to it at over 2,000. The iterated prisoner's dilemma has also been referred to as the "peace-war game".<sup class="reference" id="cite_ref-Shy_13-0">[13]</sup>
</p><p>If the game is played exactly <i>N</i> times and both players know this, then the dominant strategy is to defect in all rounds. The only possible Nash equilibrium is to always defect. The proof is inductive: one might as well defect on the last turn, since the opponent will not have a chance to later retaliate. Therefore, both will defect on the last turn. Thus, the player might as well defect on the second-to-last turn, since the opponent will defect on the last no matter what is done, and so on. The same applies if the game length is unknown but has a known upper limit.
</p><p>Unlike the standard prisoner's dilemma, in the iterated prisoner's dilemma the defection strategy is counter-intuitive and fails badly to predict the behavior of human players. Within standard economic theory, though, this is the only correct answer. The superrational strategy in the iterated prisoner's dilemma with fixed <i>N</i> is to cooperate against a superrational opponent, and in the limit of large <i>N</i>, experimental results on strategies agree with the superrational version, not the game-theoretic rational one.
</p><p>For cooperation to emerge between game theoretic rational players, the total number of rounds <i>N</i> must be unknown to the players. In this case "always defect" may no longer be a strictly dominant strategy, only a Nash equilibrium. Amongst results shown by Robert Aumann in a 1959 paper, rational players repeatedly interacting for indefinitely long games can sustain the cooperative outcome.
</p><p>According to a 2019 experimental study in the <i>American Economic Review</i> which tested what strategies real-life subjects used in iterated prisoners' dilemma situations with perfect monitoring, the majority of chosen strategies were always to defect, tit-for-tat, and grim trigger. Which strategy the subjects chose depended on the parameters of the game.<sup class="reference" id="cite_ref-14">[14]</sup>
</p>
<h3><span id="Strategy_for_the_iterated_prisoner.27s_dilemma"></span><span class="mw-headline" id="Strategy_for_the_iterated_prisoner's_dilemma">Strategy for the iterated prisoner's dilemma</span><span class="mw-editsection"></span></h3>
<p>Interest in the iterated prisoner's dilemma (IPD) was kindled by Robert Axelrod in his book <i>The Evolution of Cooperation</i> (1984). In it he reports on a tournament he organized of the <i>N</i> step prisoner's dilemma (with <i>N</i> fixed) in which participants have to choose their mutual strategy again and again, and have memory of their previous encounters. Axelrod invited academic colleagues all over the world to devise computer strategies to compete in an IPD tournament. The programs that were entered varied widely in algorithmic complexity, initial hostility, capacity for forgiveness, and so forth.
</p><p>Axelrod discovered that when these encounters were repeated over a long period of time with many players, each with different strategies, greedy strategies tended to do very poorly in the long run while more altruistic strategies did better, as judged purely by self-interest. He used this to show a possible mechanism for the evolution of altruistic behavior from mechanisms that are initially purely selfish, by natural selection.
</p><p>The winning deterministic strategy was tit for tat, which Anatol Rapoport developed and entered into the tournament. It was the simplest of any program entered, containing only four lines of BASIC, and won the contest. The strategy is simply to cooperate on the first iteration of the game; after that, the player does what his or her opponent did on the previous move. Depending on the situation, a slightly better strategy can be "tit for tat with forgiveness". When the opponent defects, on the next move, the player sometimes cooperates anyway, with a small probability (around 1–5%). This allows for occasional recovery from getting trapped in a cycle of defections. The exact probability depends on the line-up of opponents.
</p><p>By analysing the top-scoring strategies, Axelrod stated several conditions necessary for a strategy to be successful.
</p>
<dl><dt>Nice</dt>
<dd>The most important condition is that the strategy must be "nice", that is, it will not defect before its opponent does (this is sometimes referred to as an "optimistic" algorithm). Almost all of the top-scoring strategies were nice. A purely selfish strategy will not "cheat" on its opponent, for purely self-interested reasons first.</dd>
<dt>Retaliating</dt>
<dd>However, Axelrod contended, the successful strategy must not be a blind optimist. It must sometimes retaliate. An example of a non-retaliating strategy is Always Cooperate. This is a very bad choice, as "nasty" strategies will ruthlessly exploit such players.</dd>
<dt>Forgiving</dt>
<dd>Successful strategies must also be forgiving. Though players will retaliate, they will once again fall back to cooperating if the opponent does not continue to defect. This stops long runs of revenge and counter-revenge, maximizing points.</dd>
<dt>Non-envious</dt>
<dd>The last quality is being non-envious, that is not striving to score more than the opponent.</dd></dl>
<p>The optimal (points-maximizing) strategy for the one-time PD game is simply defection; as explained above, this is true whatever the composition of opponents may be. However, in the iterated-PD game the optimal strategy depends upon the strategies of likely opponents, and how they will react to defections and cooperations. For example, consider a population where everyone defects every time, except for a single individual following the tit for tat strategy. That individual is at a slight disadvantage because of the loss on the first turn. In such a population, the optimal strategy for that individual is to defect every time. In a population with a certain percentage of always-defectors and the rest being tit for tat players, the optimal strategy for an individual depends on the percentage, and on the length of the game.
</p><p>In the strategy called Pavlov, win-stay, lose-switch, faced with a failure to cooperate, the player switches strategy the next turn.<sup class="reference" id="cite_ref-15">[15]</sup> In certain circumstances,<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><span title="Statement needs to be more specific about the content to which it refers. (November 2012)">specify</span></i>]</sup> Pavlov beats all other strategies by giving preferential treatment to co-players using a similar strategy.
</p><p>Deriving the optimal strategy is generally done in two ways:
</p>
<ul><li>Bayesian Nash equilibrium: If the statistical distribution of opposing strategies can be determined (e.g. 50% tit for tat, 50% always cooperate) an optimal counter-strategy can be derived analytically.<sup class="reference" id="cite_ref-17">[a]</sup></li>
<li>Monte Carlo simulations of populations have been made, where individuals with low scores die off, and those with high scores reproduce (a genetic algorithm for finding an optimal strategy). The mix of algorithms in the final population generally depends on the mix in the initial population. The introduction of mutation (random variation during reproduction) lessens the dependency on the initial population; empirical experiments with such systems tend to produce tit for tat players (see for instance Chess 1988),<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><span title="The text near this tag may need clarification or removal of jargon. (August 2016)">clarification needed</span></i>]</sup> but no analytic proof exists that this will always occur.<sup class="reference" id="cite_ref-18">[17]</sup></li></ul>
<p>Although tit for tat is considered to be the most robust basic strategy, a team from Southampton University in England introduced a new strategy at the 20th-anniversary iterated prisoner's dilemma competition, which proved to be more successful than tit for tat. This strategy relied on collusion between programs to achieve the highest number of points for a single program. The university submitted 60 programs to the competition, which were designed to recognize each other through a series of five to ten moves at the start.<sup class="reference" id="cite_ref-19">[18]</sup> Once this recognition was made, one program would always cooperate and the other would always defect, assuring the maximum number of points for the defector. If the program realized that it was playing a non-Southampton player, it would continuously defect in an attempt to minimize the score of the competing program. As a result, the 2004 Prisoners' Dilemma Tournament results show University of Southampton's strategies in the first three places, despite having fewer wins and many more losses than the GRIM strategy. (In a PD tournament, the aim of the game is not to "win" matches – that can easily be achieved by frequent defection). This strategy ended up taking the top three positions in the competition, as well as a number of positions towards the bottom.
</p><p>The Southampton strategy takes advantage of the fact that multiple entries were allowed in this particular competition and that the performance of a team was measured by that of the highest-scoring player (meaning that the use of self-sacrificing players was a form of minmaxing). In a competition where one has control of only a single player, tit for tat is certainly a better strategy. Because of this new rule, this competition also has little theoretical significance when analyzing single agent strategies as compared to Axelrod's seminal tournament. However, it provided a basis for analysing how to achieve cooperative strategies in multi-agent frameworks, especially in the presence of noise. In fact, long before this new-rules tournament was played, Dawkins, in his book <i>The Selfish Gene</i>, pointed out the possibility of such strategies winning if multiple entries were allowed, but he remarked that most probably Axelrod would not have allowed them if they had been submitted. It also relies on circumventing rules about the prisoner's dilemma in that there is no communication allowed between the two players, which the Southampton programs arguably did with their preprogrammed "ten move dance" to recognize one another; this only reinforces just how valuable communication can be in shifting the balance of the game.
</p><p>Even without implicit collusion between software strategies (exploited by the Southampton team) tit for tat is not always the absolute winner of any given tournament; it would be more precise to say that its long run results over a series of tournaments outperform its rivals. (In any one event a given strategy can be slightly better adjusted to the competition than tit for tat, but tit for tat is more robust). The same applies for the tit for tat with forgiveness variant, and other optimal strategies: on any given day they might not "win" against a specific mix of counter-strategies. An alternative way of putting it is using the Darwinian ESS simulation. In such a simulation, tit for tat will almost always come to dominate, though nasty strategies will drift in and out of the population because a tit for tat population is penetrable by non-retaliating nice strategies, which in turn are easy prey for the nasty strategies. Richard Dawkins showed that here, no static mix of strategies form a stable equilibrium and the system will always oscillate between bounds.
</p>
<h3><span id="Stochastic_iterated_prisoner.27s_dilemma"></span><span class="mw-headline" id="Stochastic_iterated_prisoner's_dilemma">Stochastic iterated prisoner's dilemma</span><span class="mw-editsection"></span></h3>
<p>In a stochastic iterated prisoner's dilemma game, strategies are specified by in terms of "cooperation probabilities".<sup class="reference" id="cite_ref-Press2012_20-0">[19]</sup> In an encounter between player <i>X</i> and player <i>Y</i>, <i>X</i><span class="nowrap" style="padding-left:0.1em;">'</span>s strategy is specified by a set of probabilities <i>P</i> of cooperating with <i>Y</i>. <i>P</i> is a function of the outcomes of their previous encounters or some subset thereof. If <i>P</i> is a function of only their most recent <i>n</i> encounters, it is called a "memory-n" strategy. A memory-1 strategy is then specified by four cooperation probabilities: <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P=\{P_{cc},P_{cd},P_{dc},P_{dd}\}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo>=</mo>
<mo fence="false" stretchy="false">{</mo>
<msub>
<mi>P</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
<mi>c</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>P</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
<mi>d</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>P</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
<mi>c</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>P</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
<mi>d</mi>
</mrow>
</msub>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P=\{P_{cc},P_{cd},P_{dc},P_{dd}\}}</annotation>
</semantics>
</math></span><img alt="P=\{P_{cc},P_{cd},P_{dc},P_{dd}\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a6049cd9b759ca645ad577ed9677915c00c4fa29" style="vertical-align: -0.838ex; width:23.456ex; height:2.843ex;"/></span>, where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P_{ab}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>P</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>a</mi>
<mi>b</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P_{ab}}</annotation>
</semantics>
</math></span><img alt="P_{ab}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04933f46bcf319ef1e1a78dda1a9f08fd7394eb8" style="vertical-align: -0.671ex; width:3.3ex; height:2.509ex;"/></span> is the probability that <i>X</i> will cooperate in the present encounter given that the previous encounter was characterized by (ab). For example, if the previous encounter was one in which <i>X</i> cooperated and <i>Y</i> defected, then <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P_{cd}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>P</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
<mi>d</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P_{cd}}</annotation>
</semantics>
</math></span><img alt="P_{cd}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3a1359d79b5df5be1a716b8d3ac172ab612ca298" style="vertical-align: -0.671ex; width:3.296ex; height:2.509ex;"/></span> is the probability that <i>X</i> will cooperate in the present encounter. If each of the probabilities are either 1 or 0, the strategy is called deterministic. An example of a deterministic strategy is the tit-for-tat strategy written as <i>P</i>={1,0,1,0}, in which <i>X</i> responds as <i>Y</i> did in the previous encounter. Another is the win–stay, lose–switch strategy written as <i>P</i>={1,0,0,1}, in which <i>X</i> responds as in the previous encounter, if it was a "win" (i.e., cc or dc) but changes strategy if it was a loss (i.e., cd or dd). It has been shown that for any memory-n strategy there is a corresponding memory-1 strategy that gives the same statistical results, so that only memory-1 strategies need be considered.<sup class="reference" id="cite_ref-Press2012_20-1">[19]</sup>
</p><p>If we define <i>P</i> as the above 4-element strategy vector of <i>X</i> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle Q=\{Q_{cc},Q_{cd},Q_{dc},Q_{dd}\}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>Q</mi>
<mo>=</mo>
<mo fence="false" stretchy="false">{</mo>
<msub>
<mi>Q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
<mi>c</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
<mi>d</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
<mi>c</mi>
</mrow>
</msub>
<mo>,</mo>
<msub>
<mi>Q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
<mi>d</mi>
</mrow>
</msub>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle Q=\{Q_{cc},Q_{cd},Q_{dc},Q_{dd}\}}</annotation>
</semantics>
</math></span><img alt="Q=\{Q_{cc},Q_{cd},Q_{dc},Q_{dd}\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1e102e855f62f2cc7e5f197f19a6275a31f1b434" style="vertical-align: -0.838ex; width:24.933ex; height:2.843ex;"/></span> as the 4-element strategy vector of <i>Y</i>, a transition matrix <i>M</i> may be defined for <i>X</i> whose <i>ij</i> th entry is the probability that the outcome of a particular encounter between <i>X</i> and <i>Y</i> will be <i>j</i> given that the previous encounter was <i>i</i>, where <i>i</i> and <i>j</i> are one of the four outcome indices: <i>cc</i>, <i>cd</i>, <i>dc</i>, or <i>dd</i>. For example, from <i>X</i><span class="nowrap" style="padding-left:0.1em;">'</span>s point of view, the probability that the outcome of the present encounter is <i>cd</i> given that the previous encounter was <i>cd</i> is equal to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M_{cd,cd}=P_{cd}(1-Q_{dc})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>M</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
<mi>d</mi>
<mo>,</mo>
<mi>c</mi>
<mi>d</mi>
</mrow>
</msub>
<mo>=</mo>
<msub>
<mi>P</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>c</mi>
<mi>d</mi>
</mrow>
</msub>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>−<!-- − --></mo>
<msub>
<mi>Q</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>d</mi>
<mi>c</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M_{cd,cd}=P_{cd}(1-Q_{dc})}</annotation>
</semantics>
</math></span><img alt="M_{cd,cd}=P_{cd}(1-Q_{dc})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fc930c8ddbf26a0ed1e868c21982154bbf63fc6f" style="vertical-align: -1.005ex; width:21.936ex; height:3.009ex;"/></span>. (The indices for <i>Q</i> are from <i>Y</i><span class="nowrap" style="padding-left:0.1em;">'</span>s point of view: a <i>cd</i> outcome for <i>X</i> is a <i>dc</i> outcome for <i>Y</i>.) Under these definitions, the iterated prisoner's dilemma qualifies as a stochastic process and <i>M</i> is a stochastic matrix, allowing all of the theory of stochastic processes to be applied.<sup class="reference" id="cite_ref-Press2012_20-2">[19]</sup>
</p><p>One result of stochastic theory is that there exists a stationary vector <i>v</i> for the matrix <i>M</i> such that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle v\cdot M=v}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>v</mi>
<mo>⋅<!-- ⋅ --></mo>
<mi>M</mi>
<mo>=</mo>
<mi>v</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle v\cdot M=v}</annotation>
</semantics>
</math></span><img alt="v\cdot M=v" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f4391ee63cdd73b80bea8b4cd7807305ffa9b28c" style="vertical-align: -0.338ex; width:9.475ex; height:2.176ex;"/></span>. Without loss of generality, it may be specified that <i>v</i> is normalized so that the sum of its four components is unity. The <i>ij</i> th entry in <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M^{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>M</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M^{n}}</annotation>
</semantics>
</math></span><img alt="M^{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/655dc6ee87dfc8866fb4adaa13ed2eb37f519a1d" style="vertical-align: -0.338ex; width:3.717ex; height:2.343ex;"/></span> will give the probability that the outcome of an encounter between <i>X</i> and <i>Y</i> will be <i>j</i> given that the encounter <i>n</i> steps previous is <i>i</i>. In the limit as <i>n</i> approaches infinity, <i>M</i> will converge to a matrix with fixed values, giving the long-term probabilities of an encounter producing <i>j</i> which will be independent of <i>i</i>. In other words, the rows of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M^{\infty }}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>M</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M^{\infty }}</annotation>
</semantics>
</math></span><img alt="M^{\infty }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/28929b01548f9a3f737aed54493dd719dee1d49a" style="vertical-align: -0.338ex; width:4.374ex; height:2.343ex;"/></span> will be identical, giving the long-term equilibrium result probabilities of the iterated prisoners dilemma without the need to explicitly evaluate a large number of interactions. It can be seen that <i>v</i> is a stationary vector for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M^{n}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>M</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M^{n}}</annotation>
</semantics>
</math></span><img alt="M^{n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/655dc6ee87dfc8866fb4adaa13ed2eb37f519a1d" style="vertical-align: -0.338ex; width:3.717ex; height:2.343ex;"/></span> and particularly <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M^{\infty }}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>M</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M^{\infty }}</annotation>
</semantics>
</math></span><img alt="M^{\infty }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/28929b01548f9a3f737aed54493dd719dee1d49a" style="vertical-align: -0.338ex; width:4.374ex; height:2.343ex;"/></span>, so that each row of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle M^{\infty }}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>M</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle M^{\infty }}</annotation>
</semantics>
</math></span><img alt="M^{\infty }" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/28929b01548f9a3f737aed54493dd719dee1d49a" style="vertical-align: -0.338ex; width:4.374ex; height:2.343ex;"/></span> will be equal to <i>v</i>. Thus the stationary vector specifies the equilibrium outcome probabilities for <i>X</i>. Defining <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle S_{x}=\{R,S,T,P\}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
<mo>=</mo>
<mo fence="false" stretchy="false">{</mo>
<mi>R</mi>
<mo>,</mo>
<mi>S</mi>
<mo>,</mo>
<mi>T</mi>
<mo>,</mo>
<mi>P</mi>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle S_{x}=\{R,S,T,P\}}</annotation>
</semantics>
</math></span><img alt="S_{x}=\{R,S,T,P\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0b088554d9d2b52d1f6aa9de2a7673585c3fcdd0" style="vertical-align: -0.838ex; width:17.768ex; height:2.843ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle S_{y}=\{R,T,S,P\}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
<mo>=</mo>
<mo fence="false" stretchy="false">{</mo>
<mi>R</mi>
<mo>,</mo>
<mi>T</mi>
<mo>,</mo>
<mi>S</mi>
<mo>,</mo>
<mi>P</mi>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle S_{y}=\{R,T,S,P\}}</annotation>
</semantics>
</math></span><img alt="S_{y}=\{R,T,S,P\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/446cf67988646d5db122d4efec2f8657c8691c13" style="vertical-align: -1.005ex; width:17.644ex; height:3.009ex;"/></span> as the short-term payoff vectors for the {cc,cd,dc,dd} outcomes (From <i>X</i><span class="nowrap" style="padding-left:0.1em;">'</span>s point of view), the equilibrium payoffs for <i>X</i> and <i>Y</i> can now be specified as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle s_{x}=v\cdot S_{x}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
<mo>=</mo>
<mi>v</mi>
<mo>⋅<!-- ⋅ --></mo>
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle s_{x}=v\cdot S_{x}}</annotation>
</semantics>
</math></span><img alt="s_{x}=v\cdot S_{x}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d1bf287447ee2a31d42bc00cf771dc6f5de7ea26" style="vertical-align: -0.671ex; width:10.766ex; height:2.509ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle s_{y}=v\cdot S_{y}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
<mo>=</mo>
<mi>v</mi>
<mo>⋅<!-- ⋅ --></mo>
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle s_{y}=v\cdot S_{y}}</annotation>
</semantics>
</math></span><img alt="s_{y}=v\cdot S_{y}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/34e26c0518d10aaeee1ec53f2bc598a47240195c" style="vertical-align: -1.005ex; width:10.519ex; height:2.843ex;"/></span>, allowing the two strategies <i>P</i> and <i>Q</i> to be compared for their long term payoffs.
</p>
<h4><span class="mw-headline" id="Zero-determinant_strategies">Zero-determinant strategies</span><span class="mw-editsection"></span></h4>

<p>In 2012, William H. Press and Freeman Dyson published a new class of strategies for the stochastic iterated prisoner's dilemma called "zero-determinant" (ZD) strategies.<sup class="reference" id="cite_ref-Press2012_20-3">[19]</sup> The long term payoffs for encounters between <i>X</i> and <i>Y</i> can be expressed as the determinant of a matrix which is a function of the two strategies and the short term payoff vectors: <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle s_{x}=D(P,Q,S_{x})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
<mo>=</mo>
<mi>D</mi>
<mo stretchy="false">(</mo>
<mi>P</mi>
<mo>,</mo>
<mi>Q</mi>
<mo>,</mo>
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle s_{x}=D(P,Q,S_{x})}</annotation>
</semantics>
</math></span><img alt="s_{x}=D(P,Q,S_{x})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/dfdaf95696e005f8fd55ed4c1a56f998f1c81481" style="vertical-align: -0.838ex; width:17.344ex; height:2.843ex;"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle s_{y}=D(P,Q,S_{y})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
<mo>=</mo>
<mi>D</mi>
<mo stretchy="false">(</mo>
<mi>P</mi>
<mo>,</mo>
<mi>Q</mi>
<mo>,</mo>
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle s_{y}=D(P,Q,S_{y})}</annotation>
</semantics>
</math></span><img alt="s_{y}=D(P,Q,S_{y})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5acac39c6528344d71063e838e90d50c270fe3b1" style="vertical-align: -1.005ex; width:17.098ex; height:3.009ex;"/></span>, which do not involve the stationary vector <i>v</i>. Since the determinant function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle s_{y}=D(P,Q,f)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
<mo>=</mo>
<mi>D</mi>
<mo stretchy="false">(</mo>
<mi>P</mi>
<mo>,</mo>
<mi>Q</mi>
<mo>,</mo>
<mi>f</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle s_{y}=D(P,Q,f)}</annotation>
</semantics>
</math></span><img alt="s_{y}=D(P,Q,f)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82b9d3488898743cf7e8fef292ecb2876eb2cdb1" style="vertical-align: -1.005ex; width:15.902ex; height:3.009ex;"/></span> is linear in <i>f</i>, it follows that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \alpha s_{x}+\beta s_{y}+\gamma =D(P,Q,\alpha S_{x}+\beta S_{y}+\gamma U)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>α<!-- α --></mi>
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
<mo>+</mo>
<mi>β<!-- β --></mi>
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
<mo>+</mo>
<mi>γ<!-- γ --></mi>
<mo>=</mo>
<mi>D</mi>
<mo stretchy="false">(</mo>
<mi>P</mi>
<mo>,</mo>
<mi>Q</mi>
<mo>,</mo>
<mi>α<!-- α --></mi>
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
<mo>+</mo>
<mi>β<!-- β --></mi>
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
<mo>+</mo>
<mi>γ<!-- γ --></mi>
<mi>U</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \alpha s_{x}+\beta s_{y}+\gamma =D(P,Q,\alpha S_{x}+\beta S_{y}+\gamma U)}</annotation>
</semantics>
</math></span><img alt="\alpha s_{x}+\beta s_{y}+\gamma =D(P,Q,\alpha S_{x}+\beta S_{y}+\gamma U)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8930e2a276f19aca0ed87691a91f1f8e15e40357" style="vertical-align: -1.005ex; width:43.266ex; height:3.009ex;"/></span> (where <i>U</i>={1,1,1,1}). Any strategies for which <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle D(P,Q,\alpha S_{x}+\beta S_{y}+\gamma U)=0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>D</mi>
<mo stretchy="false">(</mo>
<mi>P</mi>
<mo>,</mo>
<mi>Q</mi>
<mo>,</mo>
<mi>α<!-- α --></mi>
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
<mo>+</mo>
<mi>β<!-- β --></mi>
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
<mo>+</mo>
<mi>γ<!-- γ --></mi>
<mi>U</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle D(P,Q,\alpha S_{x}+\beta S_{y}+\gamma U)=0}</annotation>
</semantics>
</math></span><img alt="D(P,Q,\alpha S_{x}+\beta S_{y}+\gamma U)=0" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f46745dc035ae94029dbc29a4b48bd746921822d" style="vertical-align: -1.005ex; width:30.263ex; height:3.009ex;"/></span> is by definition a ZD strategy, and the long term payoffs obey the relation <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \alpha s_{x}+\beta s_{y}+\gamma =0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>α<!-- α --></mi>
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
<mo>+</mo>
<mi>β<!-- β --></mi>
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
<mo>+</mo>
<mi>γ<!-- γ --></mi>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \alpha s_{x}+\beta s_{y}+\gamma =0}</annotation>
</semantics>
</math></span><img alt="\alpha s_{x}+\beta s_{y}+\gamma =0" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/240d75a0c5a515d7254713636ea476fd43d4b81d" style="vertical-align: -1.005ex; width:18.426ex; height:2.843ex;"/></span>.
</p><p>Tit-for-tat is a ZD strategy which is "fair" in the sense of not gaining advantage over the other player. However, the ZD space also contains strategies that, in the case of two players, can allow one player to unilaterally set the other player's score or alternatively, force an evolutionary player to achieve a payoff some percentage lower than his own. The extorted player could defect but would thereby hurt himself by getting a lower payoff. Thus, extortion solutions turn the iterated prisoner's dilemma into a sort of ultimatum game. Specifically, <i>X</i> is able to choose a strategy for which <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle D(P,Q,\beta S_{y}+\gamma U)=0}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>D</mi>
<mo stretchy="false">(</mo>
<mi>P</mi>
<mo>,</mo>
<mi>Q</mi>
<mo>,</mo>
<mi>β<!-- β --></mi>
<msub>
<mi>S</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
<mo>+</mo>
<mi>γ<!-- γ --></mi>
<mi>U</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mn>0</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle D(P,Q,\beta S_{y}+\gamma U)=0}</annotation>
</semantics>
</math></span><img alt="D(P,Q,\beta S_{y}+\gamma U)=0" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/03e4a74271ab0e66ad82ba2c69a55a4a6495aa55" style="vertical-align: -1.005ex; width:23.338ex; height:3.009ex;"/></span>, unilaterally setting <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle s_{y}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>y</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle s_{y}}</annotation>
</semantics>
</math></span><img alt="s_{y}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bec93eb58677e5642c86ecdb2430703a8ee57da2" style="vertical-align: -1.005ex; width:2.14ex; height:2.343ex;"/></span> to a specific value within a particular range of values, independent of <i>Y</i><span class="nowrap" style="padding-left:0.1em;">'</span>s strategy, offering an opportunity for <i>X</i> to "extort" player <i>Y</i> (and vice versa). (It turns out that if <i>X</i> tries to set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle s_{x}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>s</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>x</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle s_{x}}</annotation>
</semantics>
</math></span><img alt="s_{x}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f738a5f93f694ded413a8f8da71081d0fdff4501" style="vertical-align: -0.671ex; width:2.263ex; height:2.009ex;"/></span> to a particular value, the range of possibilities is much smaller, only consisting of complete cooperation or complete defection.<sup class="reference" id="cite_ref-Press2012_20-4">[19]</sup>)
</p><p>An extension of the IPD is an evolutionary stochastic IPD, in which the relative abundance of particular strategies is allowed to change, with more successful strategies relatively increasing. This process may be accomplished by having less successful players imitate the more successful strategies, or by eliminating less successful players from the game, while multiplying the more successful ones. It has been shown that unfair ZD strategies are not evolutionarily stable. The key intuition is that an evolutionarily stable strategy must not only be able to invade another population (which extortionary ZD strategies can do) but must also perform well against other players of the same type (which extortionary ZD players do poorly because they reduce each other's surplus).<sup class="reference" id="cite_ref-21">[20]</sup>
</p><p>Theory and simulations confirm that beyond a critical population size, ZD extortion loses out in evolutionary competition against more cooperative strategies, and as a result, the average payoff in the population increases when the population is larger. In addition, there are some cases in which extortioners may even catalyze cooperation by helping to break out of a face-off between uniform defectors and win–stay, lose–switch agents.<sup class="reference" id="cite_ref-Hilbe2013_12-1">[12]</sup>
</p><p>While extortionary ZD strategies are not stable in large populations, another ZD class called "generous" strategies <i>is</i> both stable and robust. In fact, when the population is not too small, these strategies can supplant any other ZD strategy and even perform well against a broad array of generic strategies for iterated prisoner's dilemma, including win–stay, lose–switch. This was proven specifically for the donation game by Alexander Stewart and Joshua Plotkin in 2013.<sup class="reference" id="cite_ref-Stewart2013_22-0">[21]</sup> Generous strategies will cooperate with other cooperative players, and in the face of defection, the generous player loses more utility than its rival. Generous strategies are the intersection of ZD strategies and so-called "good" strategies, which were defined by Akin (2013)<sup class="reference" id="cite_ref-Akin2013_23-0">[22]</sup> to be those for which the player responds to past mutual cooperation with future cooperation and splits expected payoffs equally if he receives at least the cooperative expected payoff. Among good strategies, the generous (ZD) subset performs well when the population is not too small. If the population is very small, defection strategies tend to dominate.<sup class="reference" id="cite_ref-Stewart2013_22-1">[21]</sup>
</p>
<h3><span id="Continuous_iterated_prisoner.27s_dilemma"></span><span class="mw-headline" id="Continuous_iterated_prisoner's_dilemma">Continuous iterated prisoner's dilemma</span><span class="mw-editsection"></span></h3>
<p>Most work on the iterated prisoner's dilemma has focused on the discrete case, in which players either cooperate or defect, because this model is relatively simple to analyze. However, some researchers have looked at models of the continuous iterated prisoner's dilemma, in which players are able to make a variable contribution to the other player. Le and Boyd<sup class="reference" id="cite_ref-24">[23]</sup> found that in such situations, cooperation is much harder to evolve than in the discrete iterated prisoner's dilemma. The basic intuition for this result is straightforward: in a continuous prisoner's dilemma, if a population starts off in a non-cooperative equilibrium, players who are only marginally more cooperative than non-cooperators get little benefit from assorting with one another. By contrast, in a discrete prisoner's dilemma, tit-for-tat cooperators get a big payoff boost from assorting with one another in a non-cooperative equilibrium, relative to non-cooperators. Since nature arguably offers more opportunities for variable cooperation rather than a strict dichotomy of cooperation or defection, the continuous prisoner's dilemma may help explain why real-life examples of tit-for-tat-like cooperation are extremely rare in nature (ex. Hammerstein<sup class="reference" id="cite_ref-25">[24]</sup>) even though tit for tat seems robust in theoretical models.
</p>
<h3><span class="mw-headline" id="Emergence_of_stable_strategies">Emergence of stable strategies</span><span class="mw-editsection"></span></h3>
<p>Players cannot seem to coordinate mutual cooperation, thus often get locked into the inferior yet stable strategy of defection. In this way, iterated rounds facilitate the evolution of stable strategies.<sup class="reference" id="cite_ref-26">[25]</sup> Iterated rounds often produce novel strategies, which have implications to complex social interaction. One such strategy is win-stay lose-shift. This strategy outperforms a simple Tit-For-Tat strategy – that is, if you can get away with cheating, repeat that behavior. However if you get caught, switch.<sup class="reference" id="cite_ref-27">[26]</sup>
</p><p>The only problem of this tit-for-tat strategy is that they are vulnerable to signal error. The problem arises when one individual cheats in retaliation but the other interprets it as cheating. As a result of this, the second individual now cheats, and then it starts a see-saw pattern of cheating in a chain reaction.
</p><p>Even without repeated games, strong enlightened self-interest can result in a stable and efficient outcome.<sup class="reference" id="cite_ref-28">[27]</sup>
</p>
<h2><span class="mw-headline" id="Real-life_examples">Real-life examples</span><span class="mw-editsection"></span></h2>
<p>The prisoner setting may seem contrived, but there are in fact many examples in human interaction as well as interactions in nature that have the same payoff matrix. The prisoner's dilemma is therefore of interest to the social sciences such as economics, politics, and sociology, as well as to the biological sciences such as ethology and evolutionary biology. Many natural processes have been abstracted into models in which living beings are engaged in endless games of prisoner's dilemma. This wide applicability of the PD gives the game its substantial importance.
</p>
<h3><span class="mw-headline" id="Environmental_studies">Environmental studies</span><span class="mw-editsection"></span></h3>
<p>In environmental studies, the PD is evident in crises such as global climate-change. It is argued all countries will benefit from a stable climate, but any single country is often hesitant to curb CO<sub style="font-size: 80%;vertical-align: -0.35em">2</sub> emissions. The immediate benefit to any one country from maintaining current behavior is perceived to be greater than the purported eventual benefit to that country if all countries' behavior was changed, therefore explaining the impasse concerning climate-change in 2007.<sup class="reference" id="cite_ref-29">[28]</sup>
</p><p>An important difference between climate-change politics and the prisoner's dilemma is uncertainty; the extent and pace at which pollution can change climate is not known. The dilemma faced by governments is therefore different from the prisoner's dilemma in that the payoffs of cooperation are unknown. This difference suggests that states will cooperate much less than in a real iterated prisoner's dilemma, so that the probability of avoiding a possible climate catastrophe is much smaller than that suggested by a game-theoretical analysis of the situation using a real iterated prisoner's dilemma.<sup class="reference" id="cite_ref-30">[29]</sup>
</p><p>Osang and Nandy (2003) provide a theoretical explanation with proofs for a regulation-driven win-win situation along the lines of Michael Porter's hypothesis, in which government regulation of competing firms is substantial.<sup class="reference" id="cite_ref-31">[30]</sup>
</p>
<h3><span class="mw-headline" id="Animals">Animals</span><span class="mw-editsection"></span></h3>
<p>Cooperative behavior of many animals can be understood as an example of the prisoner's dilemma. Often animals engage in long-term partnerships, which can be more specifically modeled as iterated prisoner's dilemma. For example, guppies inspect predators cooperatively in groups, and they are thought to punish non-cooperative inspectors.<sup class="reference" id="cite_ref-32">[31]</sup>
</p><p>Vampire bats are social animals that engage in reciprocal food exchange. Applying the payoffs from the prisoner's dilemma can help explain this behavior:<sup class="reference" id="cite_ref-33">[32]</sup>
</p>
<ul><li>Cooperate/Cooperate: "Reward: I get blood on my unlucky nights, which saves me from starving. I have to give blood on my lucky nights, which doesn't cost me too much."</li>
<li>Defect/Cooperate: "Temptation: You save my life on my poor night. But then I get the added benefit of not having to pay the slight cost of feeding you on my good night."</li>
<li>Cooperate/Defect: "Sucker's Payoff: I pay the cost of saving your life on my good night. But on my bad night you don't feed me and I run a real risk of starving to death."</li>
<li>Defect/Defect: "Punishment: I don't have to pay the slight costs of feeding you on my good nights. But I run a real risk of starving on my poor nights."</li></ul>
<h3><span class="mw-headline" id="Psychology">Psychology</span><span class="mw-editsection"></span></h3>
<p>In addiction research / behavioral economics, George Ainslie points out<sup class="reference" id="cite_ref-34">[33]</sup> that addiction can be cast as an intertemporal PD problem between the present and future selves of the addict. In this case, <i>defecting</i> means <i>relapsing</i>, and it is easy to see that not defecting both today and in the future is by far the best outcome. The case where one abstains today but relapses in the future is the worst outcome – in some sense the discipline and self-sacrifice involved in abstaining today have been "wasted" because the future relapse means that the addict is right back where they started and will have to start over (which is quite demoralizing, and makes starting over more difficult). Relapsing today and tomorrow is a slightly "better" outcome, because while the addict is still addicted, they haven't put the effort in to trying to stop. The final case, where one engages in the addictive behavior today while abstaining "tomorrow" will be familiar to anyone who has struggled with an addiction. The problem here is that (as in other PDs) there is an obvious benefit to defecting "today", but tomorrow one will face the same PD, and the same obvious benefit will be present then, ultimately leading to an endless string of defections.
</p><p>John Gottman in his research described in "The Science of Trust" defines good relationships as those where partners know not to enter the (D,D) cell or at least not to get dynamically stuck there in a loop. In cognitive neuroscience, fast brain signaling associated with processing different rounds may indicate choices at the next round. Mutual cooperation outcomes entail brain activity changes predictive of how quickly a person will cooperate in kind at the next opportunity;<sup class="reference" id="cite_ref-35">[34]</sup> this activity may be linked to basic homeostatic and motivational processes, possibly increasing the likelihood to short-cut into the (C,C) cell of the game.
</p>
<h3><span class="mw-headline" id="Economics">Economics</span><span class="mw-editsection"></span></h3>
<p>The prisoner's dilemma has been called the <i>E. coli</i> of social psychology, and it has been used widely to research various topics such as oligopolistic competition and collective action to produce a collective good.<sup class="reference" id="cite_ref-36">[35]</sup>
</p><p>Advertising is sometimes cited as a real-example of the prisoner's dilemma. When cigarette advertising was legal in the United States, competing cigarette manufacturers had to decide how much money to spend on advertising. The effectiveness of Firm A's advertising was partially determined by the advertising conducted by Firm B. Likewise, the profit derived from advertising for Firm B is affected by the advertising conducted by Firm A. If both Firm A and Firm B chose to advertise during a given period, then the advertisement from each firm negates the other's, receipts remain constant, and expenses increase due to the cost of advertising. Both firms would benefit from a reduction in advertising. However, should Firm B choose not to advertise, Firm A could benefit greatly by advertising. Nevertheless, the optimal amount of advertising by one firm depends on how much advertising the other undertakes. As the best strategy is dependent on what the other firm chooses there is no dominant strategy, which makes it slightly different from a prisoner's dilemma. The outcome is similar, though, in that both firms would be better off were they to advertise less than in the equilibrium. Sometimes cooperative behaviors do emerge in business situations. For instance, cigarette manufacturers endorsed the making of laws banning cigarette advertising, understanding that this would reduce costs and increase profits across the industry.<sup class="reference" id="cite_ref-37">[36]</sup><sup class="reference" id="cite_ref-38">[b]</sup> This analysis is likely to be pertinent in many other business situations involving advertising.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><span title="This doesn't sound like cooperation (November 2012)">citation needed</span></i>]</sup>
</p><p>Without enforceable agreements, members of a cartel are also involved in a (multi-player) prisoner's dilemma.<sup class="reference" id="cite_ref-39">[37]</sup> 'Cooperating' typically means keeping prices at a pre-agreed minimum level. 'Defecting' means selling under this minimum level, instantly taking business (and profits) from other cartel members. Anti-trust authorities want potential cartel members to mutually defect, ensuring the lowest possible prices for consumers.
</p>
<h3><span class="mw-headline" id="Sport">Sport</span><span class="mw-editsection"></span></h3>
<p>Doping in sport has been cited as an example of a prisoner's dilemma.<sup class="reference" id="cite_ref-wired_40-0">[38]</sup>
</p><p>Two competing athletes have the option to use an illegal and/or dangerous drug to boost their performance. If neither athlete takes the drug, then neither gains an advantage. If only one does, then that athlete gains a significant advantage over their competitor, reduced by the legal and/or medical dangers of having taken the drug. If both athletes take the drug, however, the benefits cancel out and only the dangers remain, putting them both in a worse position than if neither had used doping.<sup class="reference" id="cite_ref-wired_40-1">[38]</sup>
</p><p>In a conversation with Ken Griffey Jr. after the 1998 MLB season, Barry Bonds expressed his frustration with other players' use of steroids. Bonds stated "I had a helluva season last year, and nobody gave a crap. Nobody. As much as I've complained about McGwire and Canseco and all of the bull with steroids, I'm tired of fighting it. I turn 35 this year. I've got three or four good seasons left, and I wanna get paid. I'm just gonna start using some hard-core stuff, and hopefully it won't hurt my body. Then I'll get out of the game and be done with it."<sup class="reference" id="cite_ref-41">[39]</sup> Bonds found himself in the prisoner's dilemma that is doping in baseball, the feeling that he has to use steroids so that his competitors don't have such a significant advantage over him, putting him on an even playing field, though everyone is worse off than if no one had used steroids at all.
</p>
<h3><span class="mw-headline" id="International_politics">International politics</span><span class="mw-editsection"></span></h3>
<p>In international political theory, the Prisoner's Dilemma is often used to demonstrate the coherence of strategic realism, which holds that in international relations, all states (regardless of their internal policies or professed ideology), will act in their rational self-interest given international anarchy. A classic example is an arms race like the Cold War and similar conflicts.<sup class="reference" id="cite_ref-42">[40]</sup> During the Cold War the opposing alliances of NATO and the Warsaw Pact both had the choice to arm or disarm. From each side's point of view, disarming whilst their opponent continued to arm would have led to military inferiority and possible annihilation. Conversely, arming whilst their opponent disarmed would have led to superiority. If both sides chose to arm, neither could afford to attack the other, but both incurred the high cost of developing and maintaining a nuclear arsenal. If both sides chose to disarm, war would be avoided and there would be no costs.
</p><p>Although the 'best' overall outcome is for both sides to disarm, the rational course for both sides is to arm, and this is indeed what happened. Both sides poured enormous resources into military research and armament in a war of attrition for the next thirty years until the Soviet Union could not withstand the economic cost.<sup class="reference" id="cite_ref-43">[41]</sup> The same logic could be applied in any similar scenario, be it economic or technological competition between sovereign states.
</p>
<h3><span class="mw-headline" id="Multiplayer_dilemmas">Multiplayer dilemmas</span><span class="mw-editsection"></span></h3>
<p>Many real-life dilemmas involve multiple players.<sup class="reference" id="cite_ref-44">[42]</sup> Although metaphorical, Hardin's tragedy of the commons may be viewed as an example of a multi-player generalization of the PD: Each villager makes a choice for personal gain or restraint. The collective reward for unanimous (or even frequent) defection is very low payoffs (representing the destruction of the "commons"). A commons dilemma most people can relate to is washing the dishes in a shared house. By not washing dishes an individual can gain by saving his time, but if that behavior is adopted by every resident, the collective cost is no clean plates for anyone.
</p><p>The commons are not always exploited: William Poundstone, in a book about the prisoner's dilemma, describes a situation in New Zealand where newspaper boxes are left unlocked. It is possible for people to take a paper without paying (<i>defecting</i>), but very few do, feeling that if they do not pay then neither will others, destroying the system.<sup class="reference" id="cite_ref-FOOTNOTEPoundstone1993126–127_45-0">[43]</sup> Subsequent research by Elinor Ostrom, winner of the 2009 Nobel Memorial Prize in Economic Sciences, hypothesized that the tragedy of the commons is oversimplified, with the negative outcome influenced by outside influences. Without complicating pressures, groups communicate and manage the commons among themselves for their mutual benefit, enforcing social norms to preserve the resource and achieve the maximum good for the group, an example of effecting the best case outcome for PD.<sup class="reference" id="cite_ref-46">[44]</sup><sup class="reference" id="cite_ref-47">[45]</sup>
</p>
<h2><span class="mw-headline" id="Related_games">Related games</span><span class="mw-editsection"></span></h2>
<h3><span class="mw-headline" id="Closed-bag_exchange">Closed-bag exchange</span><span class="mw-editsection"></span></h3>

<p>Douglas Hofstadter<sup class="reference" id="cite_ref-dh_48-0">[46]</sup> once suggested that people often find problems such as the PD problem easier to understand when it is illustrated in the form of a simple game, or trade-off. One of several examples he used was "closed bag exchange":
</p>
<style data-mw-deduplicate="TemplateStyles:r996844942">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}</style><blockquote class="templatequote"><p>Two people meet and exchange closed bags, with the understanding that one of them contains money, and the other contains a purchase. Either player can choose to honor the deal by putting into his or her bag what he or she agreed, or he or she can defect by handing over an empty bag.</p></blockquote>
<h3><span id="Friend_or_Foe.3F"></span><span class="mw-headline" id="Friend_or_Foe?"><i>Friend or Foe?</i></span><span class="mw-editsection"></span></h3>
<p><i>Friend or Foe?</i> is a game show that aired from 2002 to 2003 on the Game Show Network in the US. It is an example of the prisoner's dilemma game tested on real people, but in an artificial setting. On the game show, three pairs of people compete. When a pair is eliminated, they play a game similar to the prisoner's dilemma to determine how the winnings are split. If they both cooperate (Friend), they share the winnings and they both get 50% of the winnings. If one cooperates and the other defects (Foe), the defector gets 80% of the winnings and the cooperator gets 20% of the winnings. If both defect, both leave with nothing. Notice that the reward matrix is slightly different from the standard one given above, as the rewards for the "both defect" and the "cooperate while the opponent defects" cases are identical. This makes the "both defect" case a weak equilibrium, compared with being a strict equilibrium in the standard prisoner's dilemma. If a contestant knows that their opponent is going to vote "Foe", then their own choice does not affect their own winnings. In a specific sense, <i>Friend or Foe</i> has a rewards model the same as the game of Chicken.
</p><p>The rewards matrix is
</p>
<table class="wikitable">
<tbody><tr>
<th style="background:#EAECF0;background:linear-gradient(to top right,#EAECF0 49%,#AAA 49.5%,#AAA 50.5%,#EAECF0 51%);line-height:1.2;padding:0.1em 0.4em;">
</th>
<th scope="col" style="width:6em;"><span style="color:#900">"Friend"<br/>(cooperate)</span>
</th>
<th scope="col" style="width:6em;"><span style="color:#900">"Foe"<br/>(defect)</span>
</th></tr>
<tr>
<th scope="row" style="width:6em;"><span style="color:#009">"Friend"<br/>(cooperate)</span>
</th>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td></tr>
<tr>
<th scope="row"><span style="color:#009">"Foe"<br/>(defect)</span>
</th>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td>
<td style="background:transparent;background:linear-gradient(to top right,transparent 49%,#AAA 49.5%,#AAA 50.5%,transparent 51%);line-height:1.2;padding:0.1em 0.4em;">
</td></tr></tbody></table>
<p>This payoff matrix has also been used on the British television programmes <i>Trust Me</i>, <i>Shafted</i>, <i>The Bank Job</i> and <i>Golden Balls</i>, and on the American game shows <i>Take It All</i>, as well as for the winning couple on the Reality Show shows <i>Bachelor Pad</i> and <i>Love Island</i>. Game data from the <i>Golden Balls</i> series has been analyzed by a team of economists, who found that cooperation was "surprisingly high" for amounts of money that would seem consequential in the real world but were comparatively low in the context of the game.<sup class="reference" id="cite_ref-49">[47]</sup>
</p>
<h3><span class="mw-headline" id="Iterated_snowdrift">Iterated snowdrift</span><span class="mw-editsection"></span></h3>
<p>Researchers from the University of Lausanne and the University of Edinburgh have suggested that the "Iterated Snowdrift Game" may more closely reflect real-world social situations. Although this model is actually a chicken game, it will be described here. In this model, the risk of being exploited through defection is lower, and individuals always gain from taking the cooperative choice. The snowdrift game imagines two drivers who are stuck on opposite sides of a snowdrift, each of whom is given the option of shoveling snow to clear a path, or remaining in their car. A player's highest payoff comes from leaving the opponent to clear all the snow by themselves, but the opponent is still nominally rewarded for their work.
</p><p>This may better reflect real-world scenarios, the researchers giving the example of two scientists collaborating on a report, both of whom would benefit if the other worked harder. "But when your collaborator doesn't do any work, it's probably better for you to do all the work yourself. You'll still end up with a completed project."<sup class="reference" id="cite_ref-50">[48]</sup>
</p>
<table>
<tbody><tr>
<td>
<table class="wikitable" style="text-align: center;">
<caption>Example snowdrift payouts (A, B)
</caption>
<tbody><tr>
<th style="background:#EAECF0;background:linear-gradient(to top right,#EAECF0 49%,#AAA 49.5%,#AAA 50.5%,#EAECF0 51%);line-height:1.2;padding:0.1em 0.4em;"></th>
<th>Cooperates</th>
<th>Defects
</th></tr>
<tr>
<th>Cooperates
</th>
<td>500, 500</td>
<td>200, 800
</td></tr>
<tr>
<th>Defects
</th>
<td>800, 200</td>
<td>0, 0
</td></tr></tbody></table>
</td>
<td>
<table class="wikitable" style="text-align: center;margin-left:2em;">
<caption>Example PD payouts (A, B)
</caption>
<tbody><tr>
<th style="background:#EAECF0;background:linear-gradient(to top right,#EAECF0 49%,#AAA 49.5%,#AAA 50.5%,#EAECF0 51%);line-height:1.2;padding:0.1em 0.4em;"></th>
<th>Cooperates</th>
<th>Defects
</th></tr>
<tr>
<th>Cooperates
</th>
<td>500, 500</td>
<td>-200, 1200
</td></tr>
<tr>
<th>Defects
</th>
<td>1200, -200</td>
<td>0, 0
</td></tr></tbody></table>
</td></tr></tbody></table>
<h3><span class="mw-headline" id="Coordination_games">Coordination games</span><span class="mw-editsection"></span></h3>
<link href="mw-data:TemplateStyles:r1033289096" rel="mw-deduplicated-inline-style"/>
<p>In coordination games, players must coordinate their strategies for a good outcome. An example is two cars that abruptly meet in a blizzard; each must choose whether to swerve left or right. If both swerve left, or both right, the cars do not collide. The local left- and right-hand traffic convention helps to co-ordinate their actions.
</p><p>Symmetrical co-ordination games include Stag hunt and Bach or Stravinsky.
</p>
<h3><span id="Asymmetric_prisoner.27s_dilemmas"></span><span class="mw-headline" id="Asymmetric_prisoner's_dilemmas">Asymmetric prisoner's dilemmas</span><span class="mw-editsection"></span></h3>
<p>A more general set of games are asymmetric. As in the prisoner's dilemma, the best outcome is cooperation, and there are motives for defection. Unlike the symmetric prisoner's dilemma, though, one player has more to lose and/or more to gain than the other. Some such games have been described as a prisoner's dilemma in which one prisoner has an alibi, whence the term "alibi game".<sup class="reference" id="cite_ref-51">[49]</sup>
</p><p>In experiments, players getting unequal payoffs in repeated games may seek to maximize profits, but only under the condition that both players receive equal payoffs; this may lead to a stable equilibrium strategy in which the disadvantaged player defects every X games, while the other always co-operates. Such behaviour may depend on the experiment's social norms around fairness.<sup class="reference" id="cite_ref-52">[50]</sup>
</p>
<h3><span id="Guardian.27s_Dilemma"></span><span class="mw-headline" id="Guardian's_Dilemma">Guardian's Dilemma</span><span class="mw-editsection"></span></h3>
<p>It is not only prisoners who face dilemmas. Guardians also confront situations in which there are only unattractive choices from which to choose. Examples can easily be found in cases where one agent must smooth tensions between its own partners: one can think of two colleagues jockeying for career advancement and the troubles this causes their company's managing director; two officials competing for promotion and the tension this causes for the head of their bureau; or in parenting when two siblings vie for attention and the anxiety this causes their parents. If the behaviour of the guardian satisfies one side, the other side feels exposed and alienated.
</p><p>From an international relations perspective, Dr Spyros Katsoulas introduces the concept of the guardian's dilemma.<sup class="reference" id="cite_ref-sk_53-0">[51]</sup> The guardian's dilemma is defined as the condition in which two states maintain their enmity towards one another despite sharing a stronger common ally. By default, a dilemma is a situation with unsatisfactory choices. The guardian's dilemma lies in the fact that the stronger state can neither stay out of a crisis between its allies nor get actively involved without affecting the fragile equilibrium. If the guardian abstains, the situation may spin out of control; if the guardian gets involved, any tilt against one side may be seen as a victory or a window of opportunity for the other. Expanding on Glenn Snyder's concept of the alliance security dilemma,<sup class="reference" id="cite_ref-54">[52]</sup> the outcomes of the interaction between the guardian and the two smaller partners are described as abandonment, entrapment, and emboldening.
</p>
<h2><span class="mw-headline" id="Software">Software</span><span class="mw-editsection"></span></h2>
<p>Several software packages have been created to run prisoner's dilemma simulations and tournaments, some of which have available source code.
</p>
<ul><li>The source code for the second tournament run by Robert Axelrod (written by Axelrod and many contributors in Fortran) is available online</li>
<li>Prison, a library written in Java, last updated in 1998</li>
<li>Axelrod-Python, written in Python</li>
<li>Evoplex, a fast agent-based modeling program released in 2018 by Marcos Cardinot</li></ul>
<h2><span class="mw-headline" id="In_fiction">In fiction</span><span class="mw-editsection"></span></h2>
<p>Hannu Rajaniemi set the opening scene of his <i>The Quantum Thief</i> trilogy in a "dilemma prison". The main theme of the series has been described as the "inadequacy of a binary universe" and the ultimate antagonist is a character called the All-Defector. Rajaniemi is particularly interesting as an artist treating this subject in that he is a Cambridge-trained mathematician and holds a Ph.D. in mathematical physics – the interchangeability of matter and information is a major feature of the books, which take place in a "post-singularity" future. The first book in the series was published in 2010, with the two sequels, <i>The Fractal Prince</i> and <i>The Causal Angel</i>, published in 2012 and 2014, respectively.
</p><p>A game modeled after the (iterated) prisoner's dilemma is a central focus of the 2012 video game <i>Zero Escape: Virtue's Last Reward</i> and a minor part in its 2016 sequel <i>Zero Escape: Zero Time Dilemma</i>.
</p><p>In <i>The Mysterious Benedict Society and the Prisoner's Dilemma</i> by Trenton Lee Stewart, the main characters start by playing a version of the game and escaping from the "prison" altogether. Later they become actual prisoners and escape once again.
</p><p>In <i>The Adventure Zone: Balance</i> during <i>The Suffering Game</i> subarc, the player characters are twice presented with the prisoner's dilemma during their time in two liches' domain, once cooperating and once defecting.
</p><p>In the 8th novel from the author James S. A. Corey <i>Tiamat's Wrath,</i> Winston Duarte explains the prisoners dilemma to his 14-year-old daughter, Teresa, to train her in strategic thinking.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><span title="This claim needs references to reliable sources. (April 2020)">citation needed</span></i>]</sup>
</p><p>An extreme version of the prisoner's dilemma is featured in the 2008 film <i>The Dark Knight</i> in which the Joker rigs two ferries, one containing prisoners and the other containing civilians, arming both groups with the means to detonate the bomb on each other's ferries. Ultimately, the two sides decide not to act.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r998391716">.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}</style>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style>
<link href="mw-data:TemplateStyles:r1011085734" rel="mw-deduplicated-inline-style"/>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1054258005">.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li{margin-left:0;padding-left:3.2em;text-indent:-3.2em}.mw-parser-output .refbegin-hanging-indents ul,.mw-parser-output .refbegin-hanging-indents ul li{list-style:none}@media(max-width:720px){.mw-parser-output .refbegin-hanging-indents>ul>li{padding-left:1.6em;text-indent:-1.6em}}.mw-parser-output .refbegin-columns{margin-top:0.3em}.mw-parser-output .refbegin-columns ul{margin-top:0}.mw-parser-output .refbegin-columns li{page-break-inside:avoid;break-inside:avoid-column}</style>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"></span></h2>
<ul><li><img alt="" class="noviewer" data-file-height="1376" data-file-width="1024" decoding="async" height="16" src="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/12px-Commons-logo.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/18px-Commons-logo.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/24px-Commons-logo.svg.png 2x" width="12"/> Media related to Prisoner's dilemma at Wikimedia Commons</li>
<li>Prisoner's Dilemma (<i>Stanford Encyclopedia of Philosophy</i>)</li>
<li>The Bowerbird's Dilemma The Prisoner's Dilemma in ornithology – mathematical cartoon by Larry Gonick.</li>
<li>The Prisoner's Dilemma The Prisoner's Dilemma with Lego minifigures.</li>
<li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation encyclopaedia cs1" id="CITEREFDixitNalebuff2008">Dixit, Avinash; Nalebuff, Barry (2008). "Prisoner's Dilemma".  In David R. Henderson (ed.). <i>Concise Encyclopedia of Economics</i> (2nd ed.). Indianapolis: Library of Economics and Liberty. ISBN <bdi>978-0865976658</bdi>. OCLC 237794267.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Prisoner%27s+Dilemma&amp;rft.btitle=Concise+Encyclopedia+of+Economics&amp;rft.place=Indianapolis&amp;rft.edition=2nd&amp;rft.pub=Library+of+Economics+and+Liberty&amp;rft.date=2008&amp;rft_id=info%3Aoclcnum%2F237794267&amp;rft.isbn=978-0865976658&amp;rft.aulast=Dixit&amp;rft.aufirst=Avinash&amp;rft.au=Nalebuff%2C+Barry&amp;rft_id=http%3A%2F%2Fwww.econlib.org%2Flibrary%2FEnc%2FPrisonersDilemma.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3APrisoner%27s+dilemma"></span></li>
<li>Game Theory 101: Prisoner's Dilemma</li>
<li>Dawkins: Nice Guys Finish First</li>
<li>Axelrod Iterated Prisoner's Dilemma Python library</li>
<li>Play Prisoner's Dilemma on <i>oTree</i> (N/A 11-5-17)</li>
<li>Nicky Case's Evolution of Trust, an example of the donation game</li>
<li>Iterated Prisoner's Dilemma online game by Wayne Davis</li></ul>



<!-- 
NewPP limit report
Parsed by mw2277
Cached time: 20221223232952
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.961 seconds
Real time usage: 1.218 seconds
Preprocessor visited node count: 5836/1000000
Post‐expand include size: 195160/2097152 bytes
Template argument size: 8433/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 17/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 180156/5000000 bytes
Lua time usage: 0.508/10.000 seconds
Lua memory usage: 9406257/52428800 bytes
Number of Wikibase entities loaded: 1/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  920.992      1 -total
 39.16%  360.638      2 Template:Reflist
 12.05%  111.005     21 Template:Cite_journal
 10.17%   93.681      6 Template:Cite_web
  7.36%   67.761      2 Template:Sfn
  6.55%   60.328     13 Template:Cite_book
  6.28%   57.826      2 Template:Navbox
  6.20%   57.142      1 Template:Decision_theory_paradoxes
  6.07%   55.885      1 Template:Short_description
  5.29%   48.712      4 Template:Citation_needed
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:43717-0!canonical and timestamp 20221223232951 and revision id 1126110182.
 -->
</div></body>
</html>