<!DOCTYPE html>
<html>
<head>
<title>Benford's_law</title>
</head>
<body>
<div class="mw-parser-output"><img alt="Page protected with pending changes" data-file-height="512" data-file-width="512" decoding="async" height="20" src="//upload.wikimedia.org/wikipedia/en/thumb/b/b7/Pending-protection-shackle.svg/20px-Pending-protection-shackle.svg.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/b/b7/Pending-protection-shackle.svg/30px-Pending-protection-shackle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/b/b7/Pending-protection-shackle.svg/40px-Pending-protection-shackle.svg.png 2x" width="20"/></div><div class="mw-parser-output"><p class="mw-empty-elt">
</p>

<style data-mw-deduplicate="TemplateStyles:r1033289096">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style>


<p class="mw-empty-elt">
</p><p><b>Benford's law</b>, also known as the <b>Newcomb–Benford law</b>, the <b>law of anomalous numbers</b>, or the <b>first-digit law</b>, is an observation that in many real-life sets of numerical data, the leading digit is likely to be small.<sup class="reference" id="cite_ref-BergerHill2011_1-0">[1]</sup> In sets that obey the law, the number 1 appears as the leading significant digit about 30% of the time, while 9 appears as the leading significant digit less than 5% of the time. If the digits were distributed uniformly, they would each occur about 11.1% of the time.<sup class="reference" id="cite_ref-2">[2]</sup> Benford's law also makes predictions about the distribution of second digits, third digits, digit combinations, and so on. 
</p><p>The graph to the right shows Benford's law for base 10, one of infinitely many cases of a generalized law regarding numbers expressed in arbitrary (integer) bases, which rules out the possibility that the phenomenon might be an artifact of the base-10 number system. Further generalizations published in 1995<sup class="reference" id="cite_ref-3">[3]</sup> included analogous statements for both the <i>n</i>th leading digit and the joint distribution of the leading <i>n</i> digits, the latter of which leads to a corollary wherein the significant digits are shown to be a statistically dependent quantity.
</p><p>It has been shown that this result applies to a wide variety of data sets, including electricity bills, street addresses, stock prices, house prices, population numbers, death rates, lengths of rivers, and physical and mathematical constants.<sup class="reference" id="cite_ref-4">[4]</sup> Like other general principles about natural data—for example, the fact that many data sets are well approximated by a normal distribution—there are illustrative examples and explanations that cover many of the cases where Benford's law applies, though there are many other cases where Benford's law applies that resist simple explanations.<sup class="reference" id="cite_ref-BergerHill2020_5-0">[5]</sup><sup class="reference" id="cite_ref-6">[6]</sup> Benford's Law tends to be most accurate when values are distributed across multiple orders of magnitude, especially if the process generating the numbers is described by a power law (which is common in nature).
</p><p>The law is named after physicist Frank Benford, who stated it in 1938 in an article titled "The Law of Anomalous Numbers",<sup class="reference" id="cite_ref-Benford_7-0">[7]</sup> although it had been previously stated by Simon Newcomb in 1881.<sup class="reference" id="cite_ref-Newcomb_8-0">[8]</sup><sup class="reference" id="cite_ref-Formann2010_9-0">[9]</sup>
</p><p>The law is similar in concept, though not identical in distribution, to Zipf's law.
</p>

<h2><span class="mw-headline" id="Definition">Definition</span><span class="mw-editsection"></span></h2>

<p>A set of numbers is said to satisfy Benford's law if the leading digit <span class="texhtml mvar" style="font-style:italic;">d</span> (<span class="texhtml"><var style="padding-right: 1px;">d</var> ∈ {1, ..., 9}</span>) occurs with probability<sup class="reference" id="cite_ref-Miller2015_10-0">[10]</sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P(d)=\log _{10}(d+1)-\log _{10}(d)=\log _{10}\left({\frac {d+1}{d}}\right)=\log _{10}\left(1+{\frac {1}{d}}\right).}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>d</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">(</mo>
<mi>d</mi>
<mo>+</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">(</mo>
<mi>d</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow>
<mo>(</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mi>d</mi>
<mo>+</mo>
<mn>1</mn>
</mrow>
<mi>d</mi>
</mfrac>
</mrow>
<mo>)</mo>
</mrow>
<mo>=</mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>d</mi>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P(d)=\log _{10}(d+1)-\log _{10}(d)=\log _{10}\left({\frac {d+1}{d}}\right)=\log _{10}\left(1+{\frac {1}{d}}\right).}</annotation>
</semantics>
</math></span><img alt="{\displaystyle P(d)=\log _{10}(d+1)-\log _{10}(d)=\log _{10}\left({\frac {d+1}{d}}\right)=\log _{10}\left(1+{\frac {1}{d}}\right).}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/48649074b19e71dc8dc5e8dd82717f05bc541b67" style="vertical-align: -2.505ex; width:65.95ex; height:6.176ex;"/></span></dd></dl>
<p>The leading digits in such a set thus have the following distribution:
</p>
<table class="wikitable">
<tbody><tr>
<th><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span class="texhtml mvar" style="font-style:italic;">d</span></span></th>
<th><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P(d)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>d</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P(d)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle P(d)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b0fbd4a799466d09813255541d4be785c15d66df" style="vertical-align: -0.838ex; width:4.771ex; height:2.843ex;"/></span></th>
<th>Relative size of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P(d)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>d</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P(d)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle P(d)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b0fbd4a799466d09813255541d4be785c15d66df" style="vertical-align: -0.838ex; width:4.771ex; height:2.843ex;"/></span>
</th></tr>
<tr>
<td>1</td>
<td style="text-align:right"> 30.1%
</td>
<td align="left"><span style="display:none;">30.1</span>
</td></tr>
<tr>
<td>2</td>
<td style="text-align:right"> 17.6%
</td>
<td align="left"><span style="display:none;">17.6</span>
</td></tr>
<tr>
<td>3</td>
<td style="text-align:right"> 12.5%
</td>
<td align="left"><span style="display:none;">12.5</span>
</td></tr>
<tr>
<td>4</td>
<td style="text-align:right"> 9.7%
</td>
<td align="left"><span style="display:none;">9.7</span>
</td></tr>
<tr>
<td>5</td>
<td style="text-align:right"> 7.9%
</td>
<td align="left"><span style="display:none;">7.9</span>
</td></tr>
<tr>
<td>6</td>
<td style="text-align:right"> 6.7%
</td>
<td align="left"><span style="display:none;">6.7</span>
</td></tr>
<tr>
<td>7</td>
<td style="text-align:right"> 5.8%
</td>
<td align="left"><span style="display:none;">5.8</span>
</td></tr>
<tr>
<td>8</td>
<td style="text-align:right"> 5.1%
</td>
<td align="left"><span style="display:none;">5.1</span>
</td></tr>
<tr>
<td>9</td>
<td style="text-align:right"> 4.6%
</td>
<td align="left"><span style="display:none;">4.6</span>
</td></tr></tbody></table>
<p>The quantity <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P(d)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>d</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P(d)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle P(d)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b0fbd4a799466d09813255541d4be785c15d66df" style="vertical-align: -0.838ex; width:4.771ex; height:2.843ex;"/></span> is proportional to the space between <span class="texhtml mvar" style="font-style:italic;">d</span> and <span class="texhtml"><var style="padding-right: 1px;">d</var> + 1</span> on a logarithmic scale. Therefore, this is the distribution expected if the <i>logarithms</i> of the numbers (but not the numbers themselves) are uniformly and randomly distributed.
</p><p>For example, a number <span class="texhtml mvar" style="font-style:italic;">x</span>, constrained to lie between 1 and 10, starts with the digit 1 if <span class="texhtml">1 ≤ <var style="padding-right: 1px;">x</var> &lt; 2</span>, and starts with the digit 9 if <span class="texhtml">9 ≤ <var style="padding-right: 1px;">x</var> &lt; 10</span>. Therefore, <span class="texhtml mvar" style="font-style:italic;">x</span> starts with the digit 1 if <span class="texhtml">log 1 ≤ log  <var style="padding-right: 1px;">x</var> &lt; log 2</span>, or starts with 9 if <span class="texhtml">log 9 ≤ log <i>x</i> &lt; log 10</span>. The interval <span class="texhtml">[log 1, log 2]</span> is much wider than the interval <span class="texhtml">[log 9, log 10]</span> (0.30 and 0.05 respectively); therefore if log <span class="texhtml mvar" style="font-style:italic;">x</span> is uniformly and randomly distributed, it is much more likely to fall into the wider interval than the narrower interval, i.e. more likely to start with 1 than with 9; the probabilities are proportional to the interval widths, giving the equation above (as well as the generalization to other bases besides decimal).
</p><p>Benford's law is sometimes stated in a stronger form, asserting that the fractional part of the logarithm of data is typically close to uniformly distributed between 0 and 1; from this, the main claim about the distribution of first digits can be derived.<sup class="reference" id="cite_ref-BergerHill2020_5-1">[5]</sup>
</p>
<h3><span class="mw-headline" id="In_other_bases">In other bases</span><span class="mw-editsection"></span></h3>

<p>An extension of Benford's law predicts the distribution of first digits in other bases besides decimal; in fact, any base <span class="texhtml"><i>b</i> ≥ 2</span>. The general form is<sup class="reference" id="cite_ref-12">[12]</sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle P(d)=\log _{b}(d+1)-\log _{b}(d)=\log _{b}\left(1+{\frac {1}{d}}\right).}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>P</mi>
<mo stretchy="false">(</mo>
<mi>d</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>b</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">(</mo>
<mi>d</mi>
<mo>+</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>b</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">(</mo>
<mi>d</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>b</mi>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>d</mi>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle P(d)=\log _{b}(d+1)-\log _{b}(d)=\log _{b}\left(1+{\frac {1}{d}}\right).}</annotation>
</semantics>
</math></span><img alt="{\displaystyle P(d)=\log _{b}(d+1)-\log _{b}(d)=\log _{b}\left(1+{\frac {1}{d}}\right).}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/497fed9e00ccc3f57bde46836051139fe18d7489" style="vertical-align: -2.505ex; width:45.712ex; height:6.176ex;"/></span></dd></dl>
<p>For <span class="texhtml"><i>b</i> = 2, 1</span> (the binary and unary) number systems, Benford's law is true but trivial: All binary and unary numbers (except for 0 or the empty set) start with the digit 1. (On the other hand, the generalization of Benford's law to second and later digits is not trivial, even for binary numbers.<sup class="reference" id="cite_ref-13">[13]</sup>)
</p>
<h2><span class="mw-headline" id="Examples">Examples</span><span class="mw-editsection"></span></h2>

<p>Examining a list of the heights of the 58 tallest structures in the world by category shows that 1 is by far the most common leading digit, <i>irrespective of the unit of measurement</i> (see "scale invariance" below):
</p>
<table class="wikitable" style="text-align:right">
<tbody><tr>
<th rowspan="2">Leading<br/> digit
</th>
<th colspan="2">m
</th>
<th colspan="2">ft
</th>
<th rowspan="2">Per<br/> Benford's law
</th></tr>
<tr>
<th>Count
</th>
<th>Share
</th>
<th>Count
</th>
<th>Share
</th></tr>
<tr>
<td>1
</td>
<td>24
</td>
<td>41.4 %
</td>
<td>16
</td>
<td>27.6 %
</td>
<td>30.1 %
</td></tr>
<tr>
<td>2
</td>
<td>9
</td>
<td>15.5 %
</td>
<td>8
</td>
<td>13.8 %
</td>
<td>17.6 %
</td></tr>
<tr>
<td>3
</td>
<td>7
</td>
<td>12.1 %
</td>
<td>5
</td>
<td>8.6 %
</td>
<td>12.5 %
</td></tr>
<tr>
<td>4
</td>
<td>6
</td>
<td>10.3 %
</td>
<td>7
</td>
<td>12.1 %
</td>
<td>9.7 %
</td></tr>
<tr>
<td>5
</td>
<td>1
</td>
<td>1.7 %
</td>
<td>10
</td>
<td>17.2 %
</td>
<td>7.9 %
</td></tr>
<tr>
<td>6
</td>
<td>5
</td>
<td>8.6 %
</td>
<td>4
</td>
<td>6.9 %
</td>
<td>6.7 %
</td></tr>
<tr>
<td>7
</td>
<td>1
</td>
<td>1.7 %
</td>
<td>2
</td>
<td>3.4 %
</td>
<td>5.8 %
</td></tr>
<tr>
<td>8
</td>
<td>4
</td>
<td>6.9 %
</td>
<td>5
</td>
<td>8.6 %
</td>
<td>5.1 %
</td></tr>
<tr>
<td>9
</td>
<td>1
</td>
<td>1.7 %
</td>
<td>1
</td>
<td>1.7 %
</td>
<td>4.6 %
</td></tr></tbody></table>
<p>Another example is the leading digit of <span class="texhtml">2<sup><i>n</i></sup></span>. The sequence of the first 96 leading digits (1, 2, 4, 8, 1, 3, 6, 1, 2, 5, 1, 2, 4, 8, 1, 3, 6, 1, ... (sequence <span class="nowrap external">A008952</span> in the OEIS)) exhibits closer adherence to Benford’s law than is expected for random sequences of the same length, because it is derived from a geometric sequence.<sup class="reference" id="cite_ref-:0_14-0">[14]</sup>
</p>
<table class="wikitable" style="text-align:right">
<tbody><tr>
<th rowspan="2">Leading<br/> digit
</th>
<th colspan="2">Occurrence
</th>
<th rowspan="2">Per<br/> Benford's law
</th></tr>
<tr>
<th>Count
</th>
<th>Share
</th></tr>
<tr>
<td>1
</td>
<td>29
</td>
<td>30.2 %
</td>
<td>30.1 %
</td></tr>
<tr>
<td>2
</td>
<td>17
</td>
<td>17.7 %
</td>
<td>17.6 %
</td></tr>
<tr>
<td>3
</td>
<td>12
</td>
<td>12.5 %
</td>
<td>12.5 %
</td></tr>
<tr>
<td>4
</td>
<td>10
</td>
<td>10.4 %
</td>
<td>9.7 %
</td></tr>
<tr>
<td>5
</td>
<td>7
</td>
<td>7.3 %
</td>
<td>7.9 %
</td></tr>
<tr>
<td>6
</td>
<td>6
</td>
<td>6.3 %
</td>
<td>6.7 %
</td></tr>
<tr>
<td>7
</td>
<td>5
</td>
<td>5.2 %
</td>
<td>5.8 %
</td></tr>
<tr>
<td>8
</td>
<td>5
</td>
<td>5.2 %
</td>
<td>5.1 %
</td></tr>
<tr>
<td>9
</td>
<td>5
</td>
<td>5.2 %
</td>
<td>4.6 %
</td></tr></tbody></table>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"></span></h2>
<p>The discovery of Benford's law goes back to 1881, when the Canadian-American astronomer Simon Newcomb noticed that in logarithm tables the earlier pages (that started with 1) were much more worn than the other pages.<sup class="reference" id="cite_ref-Newcomb_8-1">[8]</sup> Newcomb's published result is the first known instance of this observation and includes a distribution on the second digit as well. Newcomb proposed a law that the probability of a single number <i>N</i> being the first digit of a number was equal to log(<i>N</i> + 1) − log(<i>N</i>).
</p><p>The phenomenon was again noted in 1938 by the physicist Frank Benford,<sup class="reference" id="cite_ref-Benford_7-1">[7]</sup> who tested it on data from 20 different domains and was credited for it. His data set included the surface areas of 335 rivers, the sizes of 3259 US populations, 104 physical constants, 1800 molecular weights, 5000 entries from a mathematical handbook, 308 numbers contained in an issue of <i>Reader's Digest</i>, the street addresses of the first 342 persons listed in <i>American Men of Science</i> and 418 death rates. The total number of observations used in the paper was 20,229. This discovery was later named after Benford (making it an example of Stigler's law).
</p><p>In 1995, Ted Hill proved the result about mixed distributions mentioned below.<sup class="reference" id="cite_ref-Hill1995_15-0">[15]</sup><sup class="reference" id="cite_ref-Hill1995b_16-0">[16]</sup>
</p>
<h2><span class="mw-headline" id="Explanations">Explanations</span><span class="mw-editsection"></span></h2>
<p>Benford's law tends to apply most accurately to data that span several orders of magnitude. As a rule of thumb, the more orders of magnitude that the data evenly covers, the more accurately Benford's law applies. For instance, one can expect that Benford's law would apply to a list of numbers representing the populations of UK settlements. But if a "settlement" is defined as a village with population between 300 and 999, then Benford's law will not apply.<sup class="reference" id="cite_ref-dspguide_17-0">[17]</sup><sup class="reference" id="cite_ref-fewster_18-0">[18]</sup>
</p><p>Consider the probability distributions shown below, referenced to a log scale. In each case, the total area in red is the relative probability that the first digit is 1, and the total area in blue is the relative probability that the first digit is 8. For the first distribution, the size of the areas of red and blue are approximately proportional to the widths of each red and blue bar. Therefore, the numbers drawn from this distribution will approximately follow Benford's law. On the other hand, for the second distribution, the ratio of the areas of red and blue is very different from the ratio of the widths of each red and blue bar. Rather, the relative areas of red and blue are determined more by the heights of the bars than the widths. Accordingly, the first digits in this distribution do not satisfy Benford's law at all.<sup class="reference" id="cite_ref-fewster_18-1">[18]</sup>
</p>
<table>
<tbody><tr>
<td>
</td>
<td>

</td></tr></tbody></table>
<p>Thus, real-world distributions that span several orders of magnitude rather uniformly (e.g., stock-market prices and populations of villages, towns, and cities) are likely to satisfy Benford's law very accurately. On the other hand, a distribution mostly or entirely within one order of magnitude (e.g., IQ scores or heights of human adults) is unlikely to satisfy Benford's law very accurately, if at all.<sup class="reference" id="cite_ref-dspguide_17-1">[17]</sup><sup class="reference" id="cite_ref-fewster_18-2">[18]</sup> However, the difference between applicable and inapplicable regimes is not a sharp cut-off: as the distribution gets narrower, the deviations from Benford's law increase gradually.
</p><p>(This discussion is not a full explanation of Benford's law, because it has not explained why data sets are so often encountered that, when plotted as a probability distribution of the logarithm of the variable, are relatively uniform over several orders of magnitude.<sup class="reference" id="cite_ref-BergerHillExplain_19-0">[19]</sup>)
</p>
<h3><span id="Krieger.E2.80.93Kafri_entropy_explanation"></span><span class="mw-headline" id="Krieger–Kafri_entropy_explanation">Krieger–Kafri entropy explanation</span><span class="mw-editsection"></span></h3>
<p>In 1970 Wolfgang Krieger proved what is now called the Krieger generator theorem.<sup class="reference" id="cite_ref-Krieger1970_20-0">[20]</sup><sup class="reference" id="cite_ref-Downarowicz2011_21-0">[21]</sup> The Krieger generator theorem might be viewed as a justification for the assumption in the Kafri ball-and-box model that, in a given base <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle B}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>B</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle B}</annotation>
</semantics>
</math></span><img alt="B" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/47136aad860d145f75f3eed3022df827cee94d7a" style="vertical-align: -0.338ex; width:1.764ex; height:2.176ex;"/></span> with a fixed number of digits 0, 1, ..., <i>n</i>, ..., <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle B-1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>B</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle B-1}</annotation>
</semantics>
</math></span><img alt="{\displaystyle B-1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e7797a12072de209e92559b7b003916964f7e571" style="vertical-align: -0.505ex; width:5.767ex; height:2.343ex;"/></span>, digit <i>n</i> is equivalent to a Kafri box containing <i>n</i> non-interacting balls. A number of other scientists and statisticians have suggested entropy-related explanations for Benford's law.<sup class="reference" id="cite_ref-22">[22]</sup><sup class="reference" id="cite_ref-Jolion2001_23-0">[23]</sup><sup class="reference" id="cite_ref-Miller2015_10-1">[10]</sup><sup class="reference" id="cite_ref-Lemons2019_24-0">[24]</sup>
</p>
<h3><span class="mw-headline" id="Multiplicative_fluctuations">Multiplicative fluctuations</span><span class="mw-editsection"></span></h3>
<p>Many real-world examples of Benford's law arise from multiplicative fluctuations.<sup class="reference" id="cite_ref-Pietronero_25-0">[25]</sup> For example, if a stock price starts at $100, and then each day it gets multiplied by a randomly chosen factor between 0.99 and 1.01, then over an extended period the probability distribution of its price satisfies Benford's law with higher and higher accuracy.
</p><p>The reason is that the <i>logarithm</i> of the stock price is undergoing a random walk, so over time its probability distribution will get more and more broad and smooth (see above).<sup class="reference" id="cite_ref-Pietronero_25-1">[25]</sup> (More technically, the central limit theorem says that multiplying more and more random variables will create a log-normal distribution with larger and larger variance, so eventually it covers many orders of magnitude almost uniformly.) To be sure of approximate agreement with Benford's law, the distribution has to be approximately invariant when scaled up by any factor up to 10; a log-normally distributed data set with wide dispersion would have this approximate property.
</p><p>Unlike multiplicative fluctuations, <i>additive</i> fluctuations do not lead to Benford's law: They lead instead to normal probability distributions (again by the central limit theorem), which do not satisfy Benford's law. By contrast, that hypothetical stock price described above can be written as the <i>product</i> of many random variables (i.e. the price change factor for each day), so is <i>likely</i> to follow Benford's law quite well.
</p>
<h3><span class="mw-headline" id="Multiple_probability_distributions">Multiple probability distributions</span><span class="mw-editsection"></span></h3>
<p>Anton Formann provided an alternative explanation by directing attention to the interrelation between the distribution of the significant digits and the distribution of the observed variable. He showed in a simulation study that long-right-tailed distributions of a random variable are compatible with the Newcomb–Benford law, and that for distributions of the ratio of two random variables the fit generally improves.<sup class="reference" id="cite_ref-26">[26]</sup> For numbers drawn from certain distributions (IQ scores, human heights) the Benford's law fails to hold because these variates obey a normal distribution, which is known not to satisfy Benford's law,<sup class="reference" id="cite_ref-Formann2010_9-1">[9]</sup> since normal distributions can't span several orders of magnitude and the mantissae of their logarithms will not be (even approximately) uniformly distributed. However, if one "mixes" numbers from those distributions, for example, by taking numbers from newspaper articles, Benford's law reappears. This can also be proven mathematically: if one repeatedly "randomly" chooses a probability distribution (from an uncorrelated set) and then randomly chooses a number according to that distribution, the resulting list of numbers will obey Benford's law.<sup class="reference" id="cite_ref-Hill1995_15-1">[15]</sup><sup class="reference" id="cite_ref-Hill1998_27-0">[27]</sup> A similar probabilistic explanation for the appearance of Benford's law in everyday-life numbers has been advanced by showing that it arises naturally when one considers mixtures of uniform distributions.<sup class="reference" id="cite_ref-28">[28]</sup>
</p>
<h3><span class="mw-headline" id="Invariance">Invariance</span><span class="mw-editsection"></span></h3>
<p>In a list of lengths, the distribution of first digits of numbers in the list may be generally similar regardless of whether all the lengths are expressed in metres, yards, feet, inches, etc. The same applies to monetary units.
</p><p>This is not always the case. For example, the height of adult humans almost always starts with a 1 or 2 when measured in metres and almost always starts with 4, 5, 6, or 7 when measured in feet. But in a list of lengths spread evenly over many orders of magnitude—for example, a list of 1000 lengths mentioned in scientific papers that includes the measurements of molecules, bacteria, plants, and galaxies—it is reasonable to expect the distribution of first digits to be the same no matter whether the lengths are written in metres or in feet.
</p><p>When the distribution of the first digits of a data set is scale-invariant (independent of the units that the data are expressed in), it is always given by Benford's law.<sup class="reference" id="cite_ref-Pinkham_29-0">[29]</sup><sup class="reference" id="cite_ref-wolfram_30-0">[30]</sup>
</p><p>For example, the first (non-zero) digit on the aforementioned list of lengths should have the same distribution whether the unit of measurement is feet or yards. But there are three feet in a yard, so the probability that the first digit of a length in yards is 1 must be the same as the probability that the first digit of a length in feet is 3, 4, or 5; similarly, the probability that the first digit of a length in yards is 2 must be the same as the probability that the first digit of a length in feet is 6, 7, or 8. Applying this to all possible measurement scales gives the logarithmic distribution of Benford's law.
</p><p>Benford's law for first digits is base invariant for number systems. There are conditions and proofs of sum invariance, inverse invariance, and addition and subtraction invariance.<sup class="reference" id="cite_ref-31">[31]</sup><sup class="reference" id="cite_ref-32">[32]</sup>
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"></span></h2>
<h3><span class="mw-headline" id="Accounting_fraud_detection">Accounting fraud detection</span><span class="mw-editsection"></span></h3>
<p>In 1972, Hal Varian suggested that the law could be used to detect possible fraud in lists of socio-economic data submitted in support of public planning decisions. Based on the plausible assumption that people who fabricate figures tend to distribute their digits fairly uniformly, a simple comparison of first-digit frequency distribution from the data with the expected distribution according to Benford's law ought to show up any anomalous results.<sup class="reference" id="cite_ref-33">[33]</sup>
</p>
<h3><span class="mw-headline" id="Use_in_criminal_trials">Use in criminal trials</span><span class="mw-editsection"></span></h3>
<p>In the United States, evidence based on Benford's law has been admitted in criminal cases at the federal, state, and local levels.<sup class="reference" id="cite_ref-34">[34]</sup>
</p>
<h3><span class="mw-headline" id="Election_data">Election data</span><span class="mw-editsection"></span></h3>
<p>Walter Mebane, a political scientist and statistician at the University of Michigan, was the first to apply the second-digit Benford's law-test (2BL-test) in election forensics.<sup class="reference" id="cite_ref-35">[35]</sup> Such analysis is considered a simple, though not foolproof, method of identifying irregularities in election results.<sup class="reference" id="cite_ref-36">[36]</sup> Scientific consensus to support the applicability of Benford's law to elections has not been reached in the literature. A 2011 study by the political scientists Joseph Deckert, Mikhail Myagkov, and Peter C. Ordeshook argued that Benford's law is problematic and misleading as a statistical indicator of election fraud.<sup class="reference" id="cite_ref-37">[37]</sup> Their method was criticized by Mebane in a response, though he agreed that there are many caveats to the application of Benford's law to election data.<sup class="reference" id="cite_ref-38">[38]</sup>
</p><p>Benford's law has been used as evidence of fraud in the 2009 Iranian elections.<sup class="reference" id="cite_ref-39">[39]</sup> An analysis by Mebane found that the second digits in vote counts for President Mahmoud Ahmadinejad, the winner of the election, tended to differ significantly from the expectations of Benford's law, and that the ballot boxes with very few invalid ballots had a greater influence on the results, suggesting widespread ballot stuffing.<sup class="reference" id="cite_ref-40">[40]</sup> Another study used bootstrap simulations to find that the candidate Mehdi Karroubi received almost twice as many vote counts beginning with the digit 7 as would be expected according to Benford's law,<sup class="reference" id="cite_ref-41">[41]</sup> while an analysis from Columbia University concluded that the probability that a fair election would produce both too few non-adjacent digits and the suspicious deviations in last-digit frequencies as found in the 2009 Iranian presidential election is less than 0.5 percent.<sup class="reference" id="cite_ref-42">[42]</sup> Benford's law has also been applied for forensic auditing and fraud detection on data from the 2003 California gubernatorial election,<sup class="reference" id="cite_ref-43">[43]</sup> the 2000 and 2004 United States presidential elections,<sup class="reference" id="cite_ref-election-forensics_44-0">[44]</sup> and the 2009 German federal election;<sup class="reference" id="cite_ref-45">[45]</sup> the Benford's Law Test was found to be "worth taking seriously as a statistical test for fraud," although "is not sensitive to distortions we know significantly affected many votes."<sup class="reference" id="cite_ref-election-forensics_44-1">[44]</sup><sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><span title="What were the results? Was fraud detected? (November 2020)">further explanation needed</span></i>]</sup>
</p><p>Benford's law has also been misapplied to claim election fraud. When applying the law to Joe Biden's election returns for Chicago, Milwaukee, and other localities in the 2020 United States presidential election, the distribution of the first digit did not follow Benford's law. The misapplication was a result of looking at data that was tightly bound in range, which violates the assumption inherent in Benford's law that the range of the data be large. The first digit test was applied to precinct-level data, but because precincts rarely receive more than a few thousand votes or fewer than several dozen, Benford's law cannot be expected to apply. According to Mebane, "It is widely understood that the first digits of precinct vote counts are not useful for trying to diagnose election frauds."<sup class="reference" id="cite_ref-46">[46]</sup><sup class="reference" id="cite_ref-47">[47]</sup>
</p>
<h3><span class="mw-headline" id="Macroeconomic_data">Macroeconomic data</span><span class="mw-editsection"></span></h3>
<p>Similarly, the macroeconomic data the Greek government reported to the European Union before entering the eurozone was shown to be probably fraudulent using Benford's law, albeit years after the country joined.<sup class="reference" id="cite_ref-48">[48]</sup><sup class="reference" id="cite_ref-Goldacre_49-0">[49]</sup>
</p>
<h3><span class="mw-headline" id="Price_digit_analysis">Price digit analysis</span><span class="mw-editsection"></span></h3>
<p>Benford's law as a benchmark for the investigation of price digits has been successfully introduced into the context of pricing research. The importance of this benchmark for detecting irregularities in prices was first demonstrated in a Europe-wide study<sup class="reference" id="cite_ref-50">[50]</sup> which investigated consumer price digits before and after the euro introduction for price adjustments. The introduction of the euro in 2002, with its various exchange rates, distorted existing nominal price patterns while at the same time retaining real prices. While the first digits of nominal prices distributed according to Benford's law, the study showed a clear deviation from this benchmark for the second and third digits in nominal market prices with a clear trend towards psychological pricing after the nominal shock of the euro introduction.
</p>
<h3><span class="mw-headline" id="Genome_data">Genome data</span><span class="mw-editsection"></span></h3>
<p>The number of open reading frames and their relationship to genome size differs between eukaryotes and prokaryotes with the former showing a log-linear relationship and the latter a linear relationship. Benford's law has been used to test this observation with an excellent fit to the data in both cases.<sup class="reference" id="cite_ref-Friar2012_51-0">[51]</sup>
</p>
<h3><span class="mw-headline" id="Scientific_fraud_detection">Scientific fraud detection</span><span class="mw-editsection"></span></h3>
<p>A test of regression coefficients in published papers showed agreement with Benford's law.<sup class="reference" id="cite_ref-Diekmann2007_52-0">[52]</sup> As a comparison group subjects were asked to fabricate statistical estimates. The fabricated results conformed to Benford's law on first digits, but failed to obey Benford's law on second digits.
</p>
<h2><span class="mw-headline" id="Statistical_tests">Statistical tests</span><span class="mw-editsection"></span></h2>
<p>Although the chi-squared test has been used to test for compliance with Benford's law it has low statistical power when used with small samples.
</p><p>The Kolmogorov–Smirnov test and the Kuiper test are more powerful when the sample size is small, particularly when Stephens's corrective factor is used.<sup class="reference" id="cite_ref-Stephens1970_53-0">[53]</sup> These tests may be unduly conservative when applied to discrete distributions. Values for the Benford test have been generated by Morrow.<sup class="reference" id="cite_ref-Morrow2014_54-0">[54]</sup> The critical values of the test statistics are shown below:
</p>
<dl><dd><table class="wikitable" style="text-align:center;">
<tbody><tr>
<th style="background:#EAECF0;background:linear-gradient(to top right,#EAECF0 49%,#AAA 49.5%,#AAA 50.5%,#EAECF0 51%);line-height:1.2;padding:0.1em 0.4em;">
</th>
<th>0.10
</th>
<th>0.05
</th>
<th>0.01
</th></tr>
<tr>
<td>Kuiper
</td>
<td>1.191
</td>
<td>1.321
</td>
<td>1.579
</td></tr>
<tr>
<td>Kolmogorov–Smirnov
</td>
<td>1.012
</td>
<td>1.148
</td>
<td>1.420
</td></tr></tbody></table></dd></dl>
<p>These critical values provide the minimum test statistic values required to reject the hypothesis of compliance with Benford's law at the given significance levels.
</p><p>Two alternative tests specific to this law have been published: First, the max (<span class="texhtml mvar" style="font-style:italic;">m</span>) statistic<sup class="reference" id="cite_ref-Leemis2000_55-0">[55]</sup> is given by
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle m={\sqrt {N}}\cdot \max _{k=1}^{9}\left\{\left|\Pr \left(X{\text{ has FSD}}=k\right)-\log _{10}\left(1+{\frac {1}{k}}\right)\right|\right\}.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>m</mi>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<msqrt>
<mi>N</mi>
</msqrt>
</mrow>
<mo>⋅<!-- ⋅ --></mo>
<munderover>
<mo form="prefix" movablelimits="true">max</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>9</mn>
</mrow>
</munderover>
<mrow>
<mo>{</mo>
<mrow>
<mo>|</mo>
<mrow>
<mo form="prefix" movablelimits="true">Pr</mo>
<mrow>
<mo>(</mo>
<mrow>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mtext> has FSD</mtext>
</mrow>
<mo>=</mo>
<mi>k</mi>
</mrow>
<mo>)</mo>
</mrow>
<mo>−<!-- − --></mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>k</mi>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
</mrow>
<mo>|</mo>
</mrow>
<mo>}</mo>
</mrow>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle m={\sqrt {N}}\cdot \max _{k=1}^{9}\left\{\left|\Pr \left(X{\text{ has FSD}}=k\right)-\log _{10}\left(1+{\frac {1}{k}}\right)\right|\right\}.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle m={\sqrt {N}}\cdot \max _{k=1}^{9}\left\{\left|\Pr \left(X{\text{ has FSD}}=k\right)-\log _{10}\left(1+{\frac {1}{k}}\right)\right|\right\}.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/be486814433427ae3d3a59fcd1986b59e7d70556" style="vertical-align: -2.505ex; width:58.603ex; height:6.176ex;"/></span></dd></dl>
<p>The leading factor <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\sqrt {N}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<msqrt>
<mi>N</mi>
</msqrt>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\sqrt {N}}}</annotation>
</semantics>
</math></span><img alt="\sqrt{N}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7b3d8948b2c154ccc7f7523b035ae7e254e04190" style="vertical-align: -0.671ex; width:3.999ex; height:3.009ex;"/></span> does not appear in the original formula by Leemis;<sup class="reference" id="cite_ref-Leemis2000_55-1">[55]</sup> it was added by Morrow in a later paper.<sup class="reference" id="cite_ref-Morrow2014_54-1">[54]</sup>
</p><p>Secondly, the distance (<span class="texhtml mvar" style="font-style:italic;">d</span>) statistic<sup class="reference" id="cite_ref-Cho2007_56-0">[56]</sup> is given by
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle d={\sqrt {N\cdot \sum _{l=1}^{9}\left[\Pr \left(X{\text{ has FSD}}=l\right)-\log _{10}\left(1+{\frac {1}{l}}\right)\right]^{2}}},}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>d</mi>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<msqrt>
<mi>N</mi>
<mo>⋅<!-- ⋅ --></mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>l</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>9</mn>
</mrow>
</munderover>
<msup>
<mrow>
<mo>[</mo>
<mrow>
<mo form="prefix" movablelimits="true">Pr</mo>
<mrow>
<mo>(</mo>
<mrow>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mtext> has FSD</mtext>
</mrow>
<mo>=</mo>
<mi>l</mi>
</mrow>
<mo>)</mo>
</mrow>
<mo>−<!-- − --></mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>l</mi>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
</mrow>
<mo>]</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mn>2</mn>
</mrow>
</msup>
</msqrt>
</mrow>
<mo>,</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle d={\sqrt {N\cdot \sum _{l=1}^{9}\left[\Pr \left(X{\text{ has FSD}}=l\right)-\log _{10}\left(1+{\frac {1}{l}}\right)\right]^{2}}},}</annotation>
</semantics>
</math></span><img alt="{\displaystyle d={\sqrt {N\cdot \sum _{l=1}^{9}\left[\Pr \left(X{\text{ has FSD}}=l\right)-\log _{10}\left(1+{\frac {1}{l}}\right)\right]^{2}}},}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f058284420d7cfcc8672bb7945f2c17a77c15f1" style="vertical-align: -3.005ex; width:55.101ex; height:8.009ex;"/></span></dd></dl>
<p>where FSD is the first significant digit and <span class="texhtml mvar" style="font-style:italic;">N</span> is the sample size. Morrow has determined the critical values for both these statistics, which are shown below:<sup class="reference" id="cite_ref-Morrow2014_54-2">[54]</sup>
</p>
<dl><dd><table class="wikitable" style="text-align:center;">
<tbody><tr>
<th style="background:#EAECF0;background:linear-gradient(to top right,#EAECF0 49%,#AAA 49.5%,#AAA 50.5%,#EAECF0 51%);line-height:1.2;padding:0.1em 0.4em;">
</th>
<th>0.10
</th>
<th>0.05
</th>
<th>0.01
</th></tr>
<tr>
<td>Leemis's <span class="texhtml mvar" style="font-style:italic;">m</span>
</td>
<td>0.851
</td>
<td>0.967
</td>
<td>1.212
</td></tr>
<tr>
<td>Cho &amp; Gaines's <span class="texhtml mvar" style="font-style:italic;">d</span>
</td>
<td>1.212
</td>
<td>1.330
</td>
<td>1.569
</td></tr></tbody></table></dd></dl>
<p>Morrow has also shown that for any random variable  <span class="texhtml mvar" style="font-style:italic;">X</span>  (with a continuous PDF) divided by its standard deviation (<span class="texhtml mvar" style="font-style:italic;">σ</span>), some value  <span class="texhtml mvar" style="font-style:italic;">A</span>  can be found so that the probability of the distribution of the first significant digit of the random variable <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle |X/\sigma |^{A}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mi>X</mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>σ<!-- σ --></mi>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>A</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle |X/\sigma |^{A}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle |X/\sigma |^{A}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4a8354832723d61dcd6502ae732d544ee6f86701" style="vertical-align: -0.838ex; width:7.231ex; height:3.343ex;"/></span> will differ from Benford's law by less than <span class="nowrap"><span class="texhtml mvar" style="font-style:italic;">ε</span> &gt; 0.</span><sup class="reference" id="cite_ref-Morrow2014_54-3">[54]</sup> The value of <span class="texhtml mvar" style="font-style:italic;">A</span> depends on the value of <span class="texhtml mvar" style="font-style:italic;">ε</span> and the distribution of the random variable.
</p><p>A method of accounting fraud detection based on bootstrapping and regression has been proposed.<sup class="reference" id="cite_ref-Suh2011_57-0">[57]</sup>
</p><p>If the goal is to conclude agreement with the Benford's law rather than disagreement, then the goodness-of-fit tests mentioned above are inappropriate. In this case the specific tests for equivalence should be applied. An empirical distribution is called equivalent to the Benford's law if a distance (for example total variation distance or the usual Euclidean distance) between the probability mass functions is sufficiently small. This method of testing with application to Benford's law is described in Ostrovski.<sup class="reference" id="cite_ref-58">[58]</sup>
</p>
<h2><span class="mw-headline" id="Range_of_applicability">Range of applicability</span><span class="mw-editsection"></span></h2>
<h3><span id="Distributions_known_to_obey_Benford.27s_law"></span><span class="mw-headline" id="Distributions_known_to_obey_Benford's_law">Distributions known to obey Benford's law</span><span class="mw-editsection"></span></h3>
<p>Some well-known infinite integer sequences provably satisfy Benford's law exactly (in the asymptotic limit as more and more terms of the sequence are included). Among these are the Fibonacci numbers,<sup class="reference" id="cite_ref-59">[59]</sup><sup class="reference" id="cite_ref-60">[60]</sup> the factorials,<sup class="reference" id="cite_ref-61">[61]</sup> the powers of 2,<sup class="reference" id="cite_ref-powers_62-0">[62]</sup><sup class="reference" id="cite_ref-:0_14-1">[14]</sup> and the powers of <i>almost</i> any other number.<sup class="reference" id="cite_ref-powers_62-1">[62]</sup>
</p><p>Likewise, some continuous processes satisfy Benford's law exactly (in the asymptotic limit as the process continues through time). One is an exponential growth or decay process: If a quantity is exponentially increasing or decreasing in time, then the percentage of time that it has each first digit satisfies Benford's law asymptotically (i.e. increasing accuracy as the process continues through time).
</p>
<h3><span id="Distributions_known_to_disobey_Benford.27s_law"></span><span class="mw-headline" id="Distributions_known_to_disobey_Benford's_law">Distributions known to disobey Benford's law</span><span class="mw-editsection"></span></h3>
<p>The square roots and reciprocals of successive natural numbers do not obey this law.<sup class="reference" id="cite_ref-Raimi1976_63-0">[63]</sup> Prime numbers in a finite range follow a Generalized Benford’s law, that approaches uniformity as the size of the range approaches infinity.<sup class="reference" id="cite_ref-64">[64]</sup> Lists of local telephone numbers violate Benford's law.<sup class="reference" id="cite_ref-65">[65]</sup> Benford's law is violated by the populations of all places with a population of at least 2500 individuals from five US states according to the 1960 and 1970 censuses, where only 19 % began with digit 1 but 20 % began with digit 2, because truncation at 2500 introduces statistical bias.<sup class="reference" id="cite_ref-Raimi1976_63-1">[63]</sup> The terminal digits in pathology reports violate Benford's law due to rounding.<sup class="reference" id="cite_ref-Beer2009_66-0">[66]</sup>
</p><p>Distributions that do not span several orders of magnitude will not follow Benford's law. Examples include height, weight, and IQ scores.<sup class="reference" id="cite_ref-Formann2010_9-2">[9]</sup><sup class="reference" id="cite_ref-67">[67]</sup>
</p>
<h3><span id="Criteria_for_distributions_expected_and_not_expected_to_obey_Benford.27s_law"></span><span class="mw-headline" id="Criteria_for_distributions_expected_and_not_expected_to_obey_Benford's_law">Criteria for distributions expected and not expected to obey Benford's law</span><span class="mw-editsection"></span></h3>
<p>A number of criteria, applicable particularly to accounting data, have been suggested where Benford's law can be expected to apply.<sup class="reference" id="cite_ref-Durtschi2004_68-0">[68]</sup>
</p>
<dl><dt>Distributions that can be expected to obey Benford's law</dt></dl>
<ul><li>When the mean is greater than the median and the skew is positive</li>
<li>Numbers that result from mathematical combination of numbers: e.g. quantity × price</li>
<li>Transaction level data: e.g. disbursements, sales</li></ul>
<dl><dt>Distributions that would not be expected to obey Benford's law</dt></dl>
<ul><li>Where numbers are assigned sequentially: e.g. check numbers, invoice numbers</li>
<li>Where numbers are influenced by human thought: e.g. prices set by psychological thresholds ($9.99)</li>
<li>Accounts with a large number of firm-specific numbers: e.g. accounts set up to record $100 refunds</li>
<li>Accounts with a built-in minimum or maximum</li>
<li>Distributions that do not span an order of magnitude of numbers.</li></ul>
<h3><span id="Benford.E2.80.99s_law_compliance_theorem"></span><span class="mw-headline" id="Benford’s_law_compliance_theorem">Benford’s law compliance theorem</span><span class="mw-editsection"></span></h3>
<p>Mathematically, Benford’s law applies if the distribution being tested fits the "Benford’s law compliance theorem".<sup class="reference" id="cite_ref-dspguide_17-2">[17]</sup> The derivation says that Benford's law is followed if the Fourier transform of the logarithm of the probability density function is zero for all integer values. Most notably, this is satisfied if the Fourier transform is zero (or negligible) for <i>n</i> ≥ 1. This is satisfied if the distribution is wide (since wide distribution implies a narrow Fourier transform). Smith summarizes thus (p. 716): 
</p>
<blockquote>
<p>Benford's law is followed by distributions that are wide compared with unit distance along the logarithmic scale. Likewise, the law is not followed by distributions that are narrow compared with unit distance … If the distribution is wide compared with unit distance on the log axis, it means that the spread in the set of numbers being examined is much greater than ten.
</p>
</blockquote>
<p>In short, Benford’s law requires that the numbers in the distribution being measured have a spread across at least an order of magnitude.
</p>
<h2><span class="mw-headline" id="Tests_with_common_distributions">Tests with common distributions</span><span class="mw-editsection"></span></h2>
<p>Benford's law was empirically tested against the numbers (up to the 10th digit) generated by a number of important distributions, including the uniform distribution, the exponential distribution, the normal distribution, and others.<sup class="reference" id="cite_ref-Formann2010_9-3">[9]</sup>
</p><p>The uniform distribution, as might be expected, does not obey Benford's law. In contrast, the ratio distribution of two uniform distributions is well-described by Benford's law.
</p><p>Neither the normal distribution nor the ratio distribution of two normal distributions (the Cauchy distribution) obey Benford's law. Although the half-normal distribution does not obey Benford's law, the ratio distribution of two half-normal distributions does. Neither the right-truncated normal distribution nor the ratio distribution of two right-truncated normal distributions are well described by Benford's law. This is not surprising as this distribution is weighted towards larger numbers.
</p><p>Benford's law also describes the exponential distribution and the ratio distribution of two exponential distributions well. The fit of chi-squared distribution depends on the degrees of freedom (df) with good agreement with df = 1 and decreasing agreement as the df increases. The <i>F</i>-distribution is fitted well for low degrees of freedom. With increasing dfs the fit decreases but much more slowly than the chi-squared distribution. The fit of the log-normal distribution depends on the mean and the variance of the distribution. The variance has a much greater effect on the fit than does the mean. Larger values of both parameters result in better agreement with the law. The ratio of two log normal distributions is a log normal so this distribution was not examined.
</p><p>Other distributions that have been examined include the Muth distribution, Gompertz distribution, Weibull distribution, gamma distribution, log-logistic distribution and the exponential power distribution all of which show reasonable agreement with the law.<sup class="reference" id="cite_ref-Leemis2000_55-2">[55]</sup><sup class="reference" id="cite_ref-Dümbgen2008_69-0">[69]</sup> The Gumbel distribution – a density increases with increasing value of the random variable – does not show agreement with this law.<sup class="reference" id="cite_ref-Dümbgen2008_69-1">[69]</sup>
</p>
<h2><span class="mw-headline" id="Generalization_to_digits_beyond_the_first">Generalization to digits beyond the first</span><span class="mw-editsection"></span></h2>

<p>It is possible to extend the law to digits beyond the first.<sup class="reference" id="cite_ref-Hill1995sigdig_70-0">[70]</sup> In particular, for any given number of digits, the probability of encountering a number starting with the string of digits <i>n</i> of that length –  discarding leading zeros –  is given by
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \log _{10}(n+1)-\log _{10}(n)=\log _{10}\left(1+{\frac {1}{n}}\right).}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo>+</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
<mo>−<!-- − --></mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>n</mi>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \log _{10}(n+1)-\log _{10}(n)=\log _{10}\left(1+{\frac {1}{n}}\right).}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \log _{10}(n+1)-\log _{10}(n)=\log _{10}\left(1+{\frac {1}{n}}\right).}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7b404fc2c2510ac03fada71086dde4074963fde6" style="vertical-align: -2.505ex; width:41.195ex; height:6.176ex;"/></span></dd></dl>
<p>For example, the probability that a number starts with the digits 3, 1, 4 is <span class="texhtml">log<sub>10</sub>(1 + 1/314) ≈ 0.00138</span>, as in the figure on the right. Numbers satisfying this include 3.14159..., 314285.7... and 0.00314465... .
</p><p>This result can be used to find the probability that a particular digit occurs at a given position within a number. For instance, the probability that a "2" is encountered as the second digit is<sup class="reference" id="cite_ref-Hill1995sigdig_70-1">[70]</sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \log _{10}\left(1+{\frac {1}{12}}\right)+\log _{10}\left(1+{\frac {1}{22}}\right)+\cdots +\log _{10}\left(1+{\frac {1}{92}}\right)\approx 0.109.}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mn>12</mn>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
<mo>+</mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mn>22</mn>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
<mo>+</mo>
<mo>⋯<!-- ⋯ --></mo>
<mo>+</mo>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mn>92</mn>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
<mo>≈<!-- ≈ --></mo>
<mn>0.109.</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \log _{10}\left(1+{\frac {1}{12}}\right)+\log _{10}\left(1+{\frac {1}{22}}\right)+\cdots +\log _{10}\left(1+{\frac {1}{92}}\right)\approx 0.109.}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \log _{10}\left(1+{\frac {1}{12}}\right)+\log _{10}\left(1+{\frac {1}{22}}\right)+\cdots +\log _{10}\left(1+{\frac {1}{92}}\right)\approx 0.109.}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fc7f89ded596acefd4ae8ba5132f08797fb41844" style="vertical-align: -2.505ex; width:66.586ex; height:6.176ex;"/></span></dd></dl>
<p>And the probability that <i>d</i> (<i>d</i> = 0, 1, ..., 9) is encountered as the <i>n</i>-th (<i>n</i> &gt; 1) digit is
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \sum _{k=10^{n-2}}^{10^{n-1}-1}\log _{10}\left(1+{\frac {1}{10k+d}}\right).}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
<mo>=</mo>
<msup>
<mn>10</mn>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mo>−<!-- − --></mo>
<mn>2</mn>
</mrow>
</msup>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<msup>
<mn>10</mn>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</msup>
<mo>−<!-- − --></mo>
<mn>1</mn>
</mrow>
</munderover>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>10</mn>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>+</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mrow>
<mn>10</mn>
<mi>k</mi>
<mo>+</mo>
<mi>d</mi>
</mrow>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
<mo>.</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \sum _{k=10^{n-2}}^{10^{n-1}-1}\log _{10}\left(1+{\frac {1}{10k+d}}\right).}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \sum _{k=10^{n-2}}^{10^{n-1}-1}\log _{10}\left(1+{\frac {1}{10k+d}}\right).}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4b354405a87d6e29e7d1ee2f5e4757b7e95a21fa" style="vertical-align: -3.505ex; width:28.184ex; height:8.176ex;"/></span></dd></dl>
<p>The distribution of the <i>n</i>-th digit, as <i>n</i> increases, rapidly approaches a uniform distribution with 10% for each of the ten digits, as shown below.<sup class="reference" id="cite_ref-Hill1995sigdig_70-2">[70]</sup> Four digits is often enough to assume a uniform distribution of 10% as "0" appears 10.0176% of the time in the fourth digit, while "9" appears 9.9824% of the time.
</p>
<table class="wikitable" style="text-align:right">
<tbody><tr>
<th>Digit
</th>
<th>0
</th>
<th>1
</th>
<th>2
</th>
<th>3
</th>
<th>4
</th>
<th>5
</th>
<th>6
</th>
<th>7
</th>
<th>8
</th>
<th>9
</th></tr>
<tr>
<th>1st
</th>
<td class="table-na" data-sort-value="" style="background: #ececec; color: #2C2C2C; vertical-align: middle; text-align: center;">—
</td>
<td>30.1%
</td>
<td>17.6%
</td>
<td>12.5%
</td>
<td>9.7%
</td>
<td>7.9%
</td>
<td>6.7%
</td>
<td>5.8%
</td>
<td>5.1%
</td>
<td>4.6%
</td></tr>
<tr>
<th>2nd
</th>
<td>12.0%
</td>
<td>11.4%
</td>
<td>10.9%
</td>
<td>10.4%
</td>
<td>10.0%
</td>
<td>9.7%
</td>
<td>9.3%
</td>
<td>9.0%
</td>
<td>8.8%
</td>
<td>8.5%
</td></tr>
<tr>
<th>3rd
</th>
<td>10.2%
</td>
<td>10.1%
</td>
<td>10.1%
</td>
<td>10.1%
</td>
<td>10.0%
</td>
<td>10.0%
</td>
<td>9.9%
</td>
<td>9.9%
</td>
<td>9.9%
</td>
<td>9.8%
</td></tr></tbody></table>
<h2><span class="mw-headline" id="Moments">Moments</span><span class="mw-editsection"></span></h2>
<p>Average and moments of random variables for the digits 1 to 9 following this law have been calculated:<sup class="reference" id="cite_ref-Scott2001_71-0">[71]</sup>
</p>
<ul><li>mean 3.440</li>
<li>variance 6.057</li>
<li>skewness 0.796</li>
<li>kurtosis −0.548</li></ul>
<p>For the two-digit distribution according to Benford's law these values are also known:<sup class="reference" id="cite_ref-Suh2010_72-0">[72]</sup>
</p>
<ul><li>mean 38.590</li>
<li>variance 621.832</li>
<li>skewness 0.772</li>
<li>kurtosis −0.547</li></ul>
<p>A table of the exact probabilities for the joint occurrence of the first two digits according to Benford's law is available,<sup class="reference" id="cite_ref-Suh2010_72-1">[72]</sup> as is the population correlation between the first and second digits:<sup class="reference" id="cite_ref-Suh2010_72-2">[72]</sup> <span class="nowrap"><i>ρ</i> = 0.0561</span>.
</p>
<h2><span class="mw-headline" id="In_popular_culture">In popular culture</span><span class="mw-editsection"></span></h2>
<p>Benford's law has appeared as a plot device in some twenty-first century popular entertainment. 
</p>
<ul><li>Television crime drama <i>NUMB3RS</i> used Benford's law in the 2006 episode "The Running Man" to help solve a series of high burglaries.<sup class="reference" id="cite_ref-wolfram_30-1">[30]</sup></li>
<li>The 2016 film <i>The Accountant</i> relied on Benford's law to expose theft of funds from a robotics company.</li>
<li>The 2017 Netflix series <i>Ozark</i> used Benford's law to analyze a cartel member's financial statements and uncover fraud.</li>
<li>The 2021 Jeremy Robinson novel <i>Infinite 2</i> applied Benford's law to test whether the characters are in a simulation or reality.</li></ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"></span></h2>
<ul><li>Fraud detection in predictive analytics</li>
<li>Zipf's law</li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1054258005">.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li{margin-left:0;padding-left:3.2em;text-indent:-3.2em}.mw-parser-output .refbegin-hanging-indents ul,.mw-parser-output .refbegin-hanging-indents ul li{list-style:none}@media(max-width:720px){.mw-parser-output .refbegin-hanging-indents>ul>li{padding-left:1.6em;text-indent:-1.6em}}.mw-parser-output .refbegin-columns{margin-top:0.3em}.mw-parser-output .refbegin-columns ul{margin-top:0}.mw-parser-output .refbegin-columns li{page-break-inside:avoid;break-inside:avoid-column}</style>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1097025294">.mw-parser-output .side-box{margin:4px 0;box-sizing:border-box;border:1px solid #aaa;font-size:88%;line-height:1.25em;background-color:#f9f9f9}.mw-parser-output .side-box-abovebelow,.mw-parser-output .side-box-text{padding:0.25em 0.9em}.mw-parser-output .side-box-image{padding:2px 0 2px 0.9em;text-align:center}.mw-parser-output .side-box-imageright{padding:2px 0.9em 2px 0;text-align:center}@media(min-width:500px){.mw-parser-output .side-box-flex{display:flex;align-items:center}.mw-parser-output .side-box-text{flex:1}}@media(min-width:720px){.mw-parser-output .side-box{width:238px}.mw-parser-output .side-box-right{clear:right;float:right;margin-left:1em}.mw-parser-output .side-box-left{margin-right:1em}}</style>
<ul><li>Benford Online Bibliography, an online bibliographic database on Benford's law.</li>
<li>Testing Benford's Law An open source project showing Benford's law in action against publicly available datasets.</li></ul>

</div></body>
</html>