<!DOCTYPE html>
<html>
<head>
<title>randomized_complexity</title>
</head>
<body>
<div class="mw-parser-output">
<style data-mw-deduplicate="TemplateStyles:r1045330069">.mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:#f8f9fa;border:1px solid #aaa;padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:720px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}</style><table class="sidebar nomobile nowraplinks"><tbody><tr><td class="sidebar-pretitle">Part of a series on</td></tr><tr><th class="sidebar-title-with-pretitle" style="background:#ccccff;">Probabilistic<br/>data structures</th></tr><tr><td class="sidebar-content hlist">
<ul><li>Bloom filter</li>
<li>Count–min sketch</li>
<li>Quotient filter</li>
<li>Skip list</li></ul></td>
</tr><tr><th class="sidebar-heading" style="background:#ddddff;">
Random trees</th></tr><tr><td class="sidebar-content hlist">
<ul><li>Random binary tree</li>
<li>Treap</li>
<li>Rapidly-exploring random tree</li></ul></td>
</tr><tr><th class="sidebar-heading" style="background:#ddddff;">
Related</th></tr><tr><td class="sidebar-content hlist">
<ul><li>Randomized algorithm</li>
<li>HyperLogLog</li></ul></td>
</tr><tr><td class="sidebar-navbar"><style data-mw-deduplicate="TemplateStyles:r1063604349">.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}</style></td></tr></tbody></table>
<style data-mw-deduplicate="TemplateStyles:r1033289096">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style>
<p>A <b>randomized algorithm</b> is an algorithm that employs a degree of randomness as part of its logic or procedure. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the "average case" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.
</p><p>One has to distinguish between algorithms that use the random input so that they always terminate with the correct answer, but where the expected running time is finite (Las Vegas algorithms, for example Quicksort<sup class="reference" id="cite_ref-1">[1]</sup>), and algorithms which have a chance of producing an incorrect result (Monte Carlo algorithms, for example the Monte Carlo algorithm for the MFAS problem<sup class="reference" id="cite_ref-2">[2]</sup>) or fail to produce a result either by signaling a failure or failing to terminate. In some cases, probabilistic algorithms are the only practical means of solving a problem.<sup class="reference" id="cite_ref-3">[3]</sup>
</p><p>In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior and mathematical guarantees which may depend on the existence of an ideal true random number generator.
</p>

<h2><span class="mw-headline" id="Motivation">Motivation</span><span class="mw-editsection"></span></h2>
<p>As a motivating example, consider the problem of finding an ‘<i>a</i>’ in an array of <i>n</i> elements.
</p><p><b>Input</b>: An array of <i>n</i>≥2 elements, in which half are ‘<i>a</i>’s and the other half are ‘<i>b</i>’s.
</p><p><b>Output</b>: Find an ‘<i>a</i>’ in the array.
</p><p>We give two versions of the algorithm, one Las Vegas algorithm and one Monte Carlo algorithm.
</p><p>Las Vegas algorithm:
</p>

<p>This algorithm succeeds with probability 1. The number of iterations varies and can be arbitrarily large, but the expected number of iterations is
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \lim _{n\to \infty }\sum _{i=1}^{n}{\frac {i}{2^{i}}}=2}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<munder>
<mo form="prefix" movablelimits="true">lim</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
<mo stretchy="false">→<!-- → --></mo>
<mi mathvariant="normal">∞<!-- ∞ --></mi>
</mrow>
</munder>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>n</mi>
</mrow>
</munderover>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mi>i</mi>
<msup>
<mn>2</mn>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msup>
</mfrac>
</mrow>
<mo>=</mo>
<mn>2</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \lim _{n\to \infty }\sum _{i=1}^{n}{\frac {i}{2^{i}}}=2}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \lim _{n\to \infty }\sum _{i=1}^{n}{\frac {i}{2^{i}}}=2}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/28b23b9a9c100091938f875a021b7691086adeec" style="vertical-align: -3.005ex; width:15.461ex; height:6.843ex;"/></span></dd></dl>
<p>Since it is constant, the expected run time over many calls is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \Theta (1)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \Theta (1)}</annotation>
</semantics>
</math></span><img alt="\Theta(1)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fb3ae2cd10dbd21019bb13c462144f1bdc030e49" style="vertical-align: -0.838ex; width:4.78ex; height:2.843ex;"/></span>. (See Big Theta notation)
</p><p>Monte Carlo algorithm:
</p>

<p>If an ‘<i>a</i>’ is found, the algorithm succeeds, else the algorithm fails. After <i>k</i> iterations, the probability of finding an ‘<i>a</i>’ is:
</p>

<p>This algorithm does not guarantee success, but the run time is bounded. The number of iterations is always less than or equal to k.  Taking k to be constant the run time (expected and absolute) is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \Theta (1)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \Theta (1)}</annotation>
</semantics>
</math></span><img alt="\Theta(1)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fb3ae2cd10dbd21019bb13c462144f1bdc030e49" style="vertical-align: -0.838ex; width:4.78ex; height:2.843ex;"/></span>.
</p><p>Randomized algorithms are particularly useful when faced with a malicious "adversary" or attacker who deliberately tries to feed a bad input to the algorithm (see worst-case complexity and competitive analysis (online algorithm)) such as in the Prisoner's dilemma. It is for this reason that randomness is ubiquitous in cryptography. In cryptographic applications, pseudo-random numbers cannot be used, since the adversary can predict them, making the algorithm effectively deterministic. Therefore, either a source of truly random numbers or a cryptographically secure pseudo-random number generator is required.  Another area in which randomness is inherent is quantum computing.
</p><p>In the example above, the Las Vegas algorithm always outputs the correct answer, but its running time is a random variable.  The Monte Carlo algorithm (related to the Monte Carlo method for simulation) is guaranteed to complete in an amount of time that can be bounded by a function the input size and its parameter <i>k</i>, but allows a <i>small probability of error</i>. Observe that any Las Vegas algorithm can be converted into a Monte Carlo algorithm (via Markov's inequality), by having it output an arbitrary, possibly incorrect answer if it fails to complete within a specified time. Conversely, if an efficient verification procedure exists to check whether an answer is correct, then a Monte Carlo algorithm can be converted into a Las Vegas algorithm by running the Monte Carlo algorithm repeatedly till a correct answer is obtained.
</p>
<h2><span class="mw-headline" id="Computational_complexity">Computational complexity</span><span class="mw-editsection"></span></h2>
<p>Computational complexity theory models randomized algorithms as <i>probabilistic Turing machines</i>. Both  Las Vegas and Monte Carlo algorithms are considered, and several complexity classes are studied. The most basic randomized complexity class is RP, which is the class of decision problems for which there is an efficient (polynomial time) randomized algorithm (or probabilistic Turing machine) which recognizes NO-instances with absolute certainty and recognizes YES-instances with a probability of at least 1/2. The complement class for RP is co-RP. Problem classes having (possibly nonterminating) algorithms with polynomial time average case running time whose output is always correct are said to be in ZPP.
</p><p>The class of problems for which both YES and NO-instances are allowed to be identified with some error is called BPP. This class acts as the randomized equivalent of P, i.e. BPP represents the class of efficient randomized algorithms.
</p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"></span></h2>
<p>An important line of research in randomized algorithms in number theory can be traced back to Pocklington's algorithm, from 1917, which finds square roots modulo prime numbers.<sup class="reference" id="cite_ref-4">[4]</sup>
The study of randomized algorithms in number theory was spurred by the 1977 discovery of a randomized primality test (i.e., determining the primality of a number) by Robert M. Solovay and Volker Strassen. Soon afterwards Michael O. Rabin demonstrated that the 1976 Miller's primality test can be turned into a randomized algorithm. At that time, no practical deterministic algorithm for primality was known.
</p><p>The Miller–Rabin primality test relies on a binary relation between two positive integers <i>k</i> and <i>n</i> that can be expressed by saying that <i>k</i> "is a witness to the compositeness of" <i>n</i>.  It can be shown that
</p>
<ul><li>If there is a witness to the compositeness of <i>n</i>, then <i>n</i> is composite (i.e., <i>n</i> is not prime), and</li>
<li>If <i>n</i> is composite then at least three-fourths of the natural numbers less than <i>n</i> are witnesses to its compositeness, and</li>
<li>There is a fast algorithm that, given <i>k</i> and <i>n</i>, ascertains whether <i>k</i> is a witness to the compositeness of <i>n</i>.</li></ul>
<p>Observe that this implies that the primality problem is in Co-RP.
</p><p>If one randomly chooses 100 numbers less than a composite number <i>n</i>, then the probability of failing to find such a "witness" is (1/4)<sup>100</sup> so that for most practical purposes, this is a good primality test.  If <i>n</i> is big, there may be no other test that is practical. The probability of error can be reduced to an arbitrary degree by performing enough independent tests.
</p><p>Therefore, in practice, there is no penalty associated with accepting a small probability of error, since with a little care the probability of error can be made astronomically small.  Indeed, even though a deterministic polynomial-time primality test has since been found (see AKS primality test), it has not replaced the older probabilistic tests in cryptographic software nor is it expected to do so for the foreseeable future.
</p>
<h2><span class="mw-headline" id="Examples">Examples</span><span class="mw-editsection"></span></h2>
<h3><span class="mw-headline" id="Quicksort">Quicksort</span><span class="mw-editsection"></span></h3>
<p>Quicksort is a familiar, commonly used algorithm in which randomness can be useful. Many deterministic versions of this algorithm require <i>O</i>(<i>n</i><sup>2</sup>) time to sort <i>n</i> numbers for some well-defined class of degenerate inputs (such as an already sorted array), with the specific class of inputs that generate this behavior defined by the protocol for pivot selection. However, if the algorithm selects pivot elements uniformly at random, it has a provably high probability of finishing in <i>O</i>(<i>n</i> log <i>n</i>) time regardless of the characteristics of the input.
</p>
<h3><span class="mw-headline" id="Randomized_incremental_constructions_in_geometry">Randomized incremental constructions in geometry</span><span class="mw-editsection"></span></h3>
<p>In computational geometry, a standard technique to build a structure like a convex hull or Delaunay triangulation is to randomly permute the input points and then insert them one by one into the existing structure. The randomization ensures that the expected number of changes to the structure caused by an insertion is small, and so the expected running time of the algorithm can be bounded from above. This technique is known as randomized incremental construction.<sup class="reference" id="cite_ref-5">[5]</sup>
</p>
<h3><span class="mw-headline" id="Min_cut">Min cut</span><span class="mw-editsection"></span></h3>
<link href="mw-data:TemplateStyles:r1033289096" rel="mw-deduplicated-inline-style"/>
<p><b>Input</b>: A graph <i>G</i>(<i>V</i>,<i>E</i>)
</p><p><b>Output</b>: A cut partitioning the vertices into <i>L</i> and <i>R</i>, with the minimum number of edges between <i>L</i> and <i>R</i>.
</p><p>Recall that the contraction of two nodes, <i>u</i> and <i>v</i>, in a (multi-)graph yields a new node <i>u</i> ' with edges that are the union of the edges incident on either <i>u</i> or <i>v</i>, except from any edge(s) connecting <i>u</i> and <i>v</i>. Figure 1 gives an example of contraction of vertex <i>A</i> and <i>B</i>.
After contraction, the resulting graph may have parallel edges, but contains no self loops.
</p>


<p>Karger's<sup class="reference" id="cite_ref-6">[6]</sup> basic algorithm:
</p>
<pre><b>begin</b>
    i = 1
    <b>repeat</b>
        <b>repeat</b>
            Take a random edge (u,v) ∈ E in G
            replace u and v with the contraction u'
        <b>until</b> only 2 nodes remain
        obtain the corresponding cut result C<sub>i</sub>
        i = i + 1
    <b>until</b> i = m
    output the minimum cut among C<sub>1</sub>, C<sub>2</sub>, ..., C<sub>m</sub>.
<b>end</b>
</pre>
<p>In each execution of the outer loop, the algorithm repeats the inner loop until only 2 nodes remain, the corresponding cut is obtained. The run time of one execution is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(n)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(n)}</annotation>
</semantics>
</math></span><img alt="O(n)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/34109fe397fdcff370079185bfdb65826cb5565a" style="vertical-align: -0.838ex; width:4.977ex; height:2.843ex;"/></span>, and <i>n</i> denotes the number of vertices.
After <i>m</i> times executions of the outer loop, we output the minimum cut among all the results. The figure 2 gives an
example of one execution of the algorithm. After execution, we get a cut of size 3.
</p>
<style data-mw-deduplicate="TemplateStyles:r1110004140">.mw-parser-output .math_theorem{margin:1em 2em;padding:0.5em 1em 0.4em;border:1px solid #aaa;overflow:hidden}@media(max-width:500px){.mw-parser-output .math_theorem{margin:1em 0em;padding:0.5em 0.5em 0.4em}}</style>
<style data-mw-deduplicate="TemplateStyles:r1037201836">.mw-parser-output .math_proof{border:thin solid #aaa;margin:1em 2em;padding:0.5em 1em 0.4em;text-align:justify}@media(max-width:500px){.mw-parser-output .math_proof{margin:1em 0;padding:0.5em 0.5em 0.4em}}</style>
<link href="mw-data:TemplateStyles:r1110004140" rel="mw-deduplicated-inline-style"/>
<link href="mw-data:TemplateStyles:r1037201836" rel="mw-deduplicated-inline-style"/>
<h4><span class="mw-headline" id="Analysis_of_algorithm">Analysis of algorithm</span><span class="mw-editsection"></span></h4>
<p>The probability that the algorithm succeeds is 1 − the probability that all attempts fail. By independence, the probability that all attempts fail is
</p>
<p>By lemma 1, the probability that <span class="texhtml"><i>C</i><sub><i>i</i></sub> = <i>C</i></span> is the probability that no edge of <i>C</i> is selected during iteration <i>i</i>. Consider the inner loop and let <span class="texhtml"><i>G</i><sub><i>j</i></sub></span> denote the graph after <i>j</i> edge contractions, where <span class="texhtml"><i>j</i> ∈ {0, 1, …, <i>n</i> − 3}</span>. <span class="texhtml"><i>G</i><sub><i>j</i></sub></span> has <span class="texhtml"><i>n</i> − <i>j</i></span> vertices. We use the chain rule of conditional possibilities.
The probability that the edge chosen at iteration <i>j</i> is not in <i>C</i>, given that no edge of <i>C</i> has been chosen before, is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle 1-{\frac {k}{|E(G_{j})|}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mn>1</mn>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mi>k</mi>
<mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mi>E</mi>
<mo stretchy="false">(</mo>
<msub>
<mi>G</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
</mrow>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle 1-{\frac {k}{|E(G_{j})|}}}</annotation>
</semantics>
</math></span><img alt="1-{\frac  {k}{|E(G_{j})|}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ef9c91c421134a1e01d64a4f8937e1895aadc59f" style="vertical-align: -2.671ex; width:12.454ex; height:6.176ex;"/></span>. Note that <span class="texhtml"><i>G</i><sub><i>j</i></sub></span> still has min cut of size <i>k</i>, so by Lemma 2, it still has at least <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\frac {(n-j)k}{2}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo>−<!-- − --></mo>
<mi>j</mi>
<mo stretchy="false">)</mo>
<mi>k</mi>
</mrow>
<mn>2</mn>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\frac {(n-j)k}{2}}}</annotation>
</semantics>
</math></span><img alt="{\frac  {(n-j)k}{2}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/50518ee4af22931cb0bde886fc34cb2a048e271c" style="vertical-align: -1.838ex; width:9.05ex; height:5.676ex;"/></span> edges.
</p><p>Thus, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle 1-{\frac {k}{|E(G_{j})|}}\geq 1-{\frac {2}{n-j}}={\frac {n-j-2}{n-j}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mn>1</mn>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mi>k</mi>
<mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
<mi>E</mi>
<mo stretchy="false">(</mo>
<msub>
<mi>G</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>j</mi>
</mrow>
</msub>
<mo stretchy="false">)</mo>
<mrow class="MJX-TeXAtom-ORD">
<mo stretchy="false">|</mo>
</mrow>
</mrow>
</mfrac>
</mrow>
<mo>≥<!-- ≥ --></mo>
<mn>1</mn>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>2</mn>
<mrow>
<mi>n</mi>
<mo>−<!-- − --></mo>
<mi>j</mi>
</mrow>
</mfrac>
</mrow>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mi>n</mi>
<mo>−<!-- − --></mo>
<mi>j</mi>
<mo>−<!-- − --></mo>
<mn>2</mn>
</mrow>
<mrow>
<mi>n</mi>
<mo>−<!-- − --></mo>
<mi>j</mi>
</mrow>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle 1-{\frac {k}{|E(G_{j})|}}\geq 1-{\frac {2}{n-j}}={\frac {n-j-2}{n-j}}}</annotation>
</semantics>
</math></span><img alt="1-{\frac  {k}{|E(G_{j})|}}\geq 1-{\frac  {2}{n-j}}={\frac  {n-j-2}{n-j}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d66862f86b105833468ab3619994021555dd8081" style="vertical-align: -2.671ex; width:38.715ex; height:6.176ex;"/></span>.
</p><p>So by the chain rule, the probability of finding the min cut <i>C</i> is
</p>
<p>Cancellation gives <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \Pr[C_{i}=C]\geq {\frac {2}{n(n-1)}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo form="prefix" movablelimits="true">Pr</mo>
<mo stretchy="false">[</mo>
<msub>
<mi>C</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>i</mi>
</mrow>
</msub>
<mo>=</mo>
<mi>C</mi>
<mo stretchy="false">]</mo>
<mo>≥<!-- ≥ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>2</mn>
<mrow>
<mi>n</mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \Pr[C_{i}=C]\geq {\frac {2}{n(n-1)}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle \Pr[C_{i}=C]\geq {\frac {2}{n(n-1)}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f9a5ee0a8aef637a2522ae343d168d48bb739389" style="vertical-align: -2.671ex; width:23.651ex; height:6.009ex;"/></span>. Thus the probability that the algorithm succeeds is at least <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle 1-\left(1-{\frac {2}{n(n-1)}}\right)^{m}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mn>1</mn>
<mo>−<!-- − --></mo>
<msup>
<mrow>
<mo>(</mo>
<mrow>
<mn>1</mn>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>2</mn>
<mrow>
<mi>n</mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
</mfrac>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>m</mi>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle 1-\left(1-{\frac {2}{n(n-1)}}\right)^{m}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle 1-\left(1-{\frac {2}{n(n-1)}}\right)^{m}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/619ace45ff61354cc374c7ce6be9ae79fafff277" style="vertical-align: -2.671ex; width:22.54ex; height:6.343ex;"/></span>. For <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle m={\frac {n(n-1)}{2}}\ln n}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>m</mi>
<mo>=</mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mrow>
<mi>n</mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo>−<!-- − --></mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mrow>
<mn>2</mn>
</mfrac>
</mrow>
<mi>ln</mi>
<mo>⁡<!-- ⁡ --></mo>
<mi>n</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle m={\frac {n(n-1)}{2}}\ln n}</annotation>
</semantics>
</math></span><img alt="{\displaystyle m={\frac {n(n-1)}{2}}\ln n}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/193a8af166ca8c7f076040adb4d6b7d255392fb9" style="vertical-align: -1.838ex; width:18.685ex; height:5.676ex;"/></span>, this is equivalent to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle 1-{\frac {1}{n}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mn>1</mn>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>n</mi>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle 1-{\frac {1}{n}}}</annotation>
</semantics>
</math></span><img alt="1-{\frac  {1}{n}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2aa9ee64f95306042fd5ae4367891eb1a0d72cc1" style="vertical-align: -1.838ex; width:6.234ex; height:5.176ex;"/></span>. The algorithm finds the min cut with probability <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle 1-{\frac {1}{n}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mn>1</mn>
<mo>−<!-- − --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mfrac>
<mn>1</mn>
<mi>n</mi>
</mfrac>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle 1-{\frac {1}{n}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle 1-{\frac {1}{n}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2aa9ee64f95306042fd5ae4367891eb1a0d72cc1" style="vertical-align: -1.838ex; width:6.234ex; height:5.176ex;"/></span>, in time <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(mn)=O(n^{3}\log n)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mi>m</mi>
<mi>n</mi>
<mo stretchy="false">)</mo>
<mo>=</mo>
<mi>O</mi>
<mo stretchy="false">(</mo>
<msup>
<mi>n</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>3</mn>
</mrow>
</msup>
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(mn)=O(n^{3}\log n)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle O(mn)=O(n^{3}\log n)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0545706ab60a6ab301cf83adf20f578d60f7f628" style="vertical-align: -0.838ex; width:21.288ex; height:3.176ex;"/></span>.
</p>
<h2><span class="mw-headline" id="Derandomization">Derandomization</span><span class="mw-editsection"></span></h2>
<p>Randomness can be viewed as a resource, like space and time. Derandomization is then the process of <i>removing</i> randomness (or using as little of it as possible). It is not currently known if all algorithms can be derandomized without significantly increasing their running time. For instance, in computational complexity, it is unknown whether P = BPP, i.e., we do not know whether we can take an arbitrary randomized algorithm that runs in polynomial time with a small error probability and derandomize it to run in polynomial time without using randomness.
</p><p>There are specific methods that can be employed to derandomize particular randomized algorithms:
</p>
<ul><li>the method of conditional probabilities, and its generalization, pessimistic estimators</li>
<li>discrepancy theory (which is used to derandomize geometric algorithms)</li>
<li>the exploitation of limited independence in the random variables used by the algorithm, such as the pairwise independence used in universal hashing</li>
<li>the use of expander graphs (or dispersers in general) to <i>amplify</i> a limited amount of initial randomness (this last approach is also referred to as generating pseudorandom bits from a random source, and leads to the related topic of pseudorandomness)</li>
<li>changing the randomized algorithm to use a hash function as a source of randomness for the algorithm's tasks, and then derandomizing the algorithm by brute-forcing all possible parameters (seeds) of the hash function. This technique is usually used to exhaustively search a sample space and making the algorithm deterministic (e.g. randomized graph algorithms)</li></ul>
<h2><span class="mw-headline" id="Where_randomness_helps">Where randomness helps</span><span class="mw-editsection"></span></h2>
<p>When the model of computation is restricted to Turing machines, it is currently an open question whether the ability to make random choices allows some problems to be solved in polynomial time that cannot be solved in polynomial time without this ability; this is the question of whether P = BPP. However, in other contexts, there are specific examples of problems where randomization yields strict improvements.
</p>
<ul><li>Based on the initial motivating example: given an exponentially long string of 2<sup><i>k</i></sup> characters, half a's and half b's, a random-access machine requires 2<sup><i>k</i>−1</sup> lookups in the worst-case to find the index of an <i>a</i>; if it is permitted to make random choices, it can solve this problem in an expected polynomial number of lookups.</li>
<li>The natural way of carrying out a numerical computation in embedded systems or cyber-physical systems is to provide a result that approximates the correct one with high probability (or Probably Approximately Correct Computation (PACC)). The hard problem associated with the evaluation of the discrepancy loss between the approximated and the correct computation can be effectively addressed by resorting to randomization<sup class="reference" id="cite_ref-7">[7]</sup></li>
<li>In communication complexity, the equality of two strings can be verified to some reliability using <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \log n}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>log</mi>
<mo>⁡<!-- ⁡ --></mo>
<mi>n</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \log n}</annotation>
</semantics>
</math></span><img alt="\log n" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/317ab5292da7c7935aec01a570461fe0613b21d5" style="vertical-align: -0.671ex; width:4.754ex; height:2.509ex;"/></span> bits of communication with a randomized protocol. Any deterministic protocol requires <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \Theta (n)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi mathvariant="normal">Θ<!-- Θ --></mi>
<mo stretchy="false">(</mo>
<mi>n</mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \Theta (n)}</annotation>
</semantics>
</math></span><img alt="\Theta (n)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a6351206e27071559aa4472579095994f650d76b" style="vertical-align: -0.838ex; width:5.012ex; height:2.843ex;"/></span> bits if defending against a strong opponent.<sup class="reference" id="cite_ref-8">[8]</sup></li>
<li>The volume of a convex body can be estimated by a randomized algorithm to arbitrary precision in polynomial time.<sup class="reference" id="cite_ref-9">[9]</sup> Bárány and Füredi showed that no deterministic algorithm can do the same.<sup class="reference" id="cite_ref-10">[10]</sup> This is true unconditionally, i.e. without relying on any complexity-theoretic assumptions, assuming the convex body can be queried only as a black box.</li>
<li>A more complexity-theoretic example of a place where randomness appears to help is the class IP. IP consists of all languages that can be accepted (with high probability) by a polynomially long interaction between an all-powerful prover and a verifier that implements a BPP algorithm. IP = PSPACE.<sup class="reference" id="cite_ref-11">[11]</sup> However, if it is required that the verifier be deterministic, then IP = NP.</li>
<li>In a chemical reaction network (a finite set of reactions like A+B → 2C + D operating on a finite number of molecules), the ability to ever reach a given target state from an initial state is decidable, while even approximating the probability of ever reaching a given target state (using the standard concentration-based probability for which reaction will occur next) is undecidable.  More specifically, a limited Turing machine  can be simulated with arbitrarily high probability of running correctly for all time, only if a random chemical reaction network is used. With a simple nondeterministic chemical reaction network (any possible reaction can happen next), the computational power is limited to primitive recursive functions.<sup class="reference" id="cite_ref-12">[12]</sup></li></ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"></span></h2>
<ul><li>Probabilistic analysis of algorithms</li>
<li>Atlantic City algorithm</li>
<li>Monte Carlo algorithm</li>
<li>Las Vegas algorithm</li>
<li>Bogosort</li>
<li>Principle of deferred decision</li>
<li>Randomized algorithms as zero-sum games</li>
<li>Probabilistic roadmap</li>
<li>HyperLogLog</li>
<li>count–min sketch</li>
<li>approximate counting algorithm</li>
<li>Karger's algorithm</li></ul>
<h2><span class="mw-headline" id="Notes">Notes</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"></span></h2>
<ul><li>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. <i>Introduction to Algorithms</i>, Second Edition. MIT Press and McGraw–Hill, 1990. <link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/>ISBN 0-262-03293-7. Chapter 5: Probabilistic Analysis and Randomized Algorithms, pp. 91–122.</li>
<li>Dirk Draheim. "<i>Semantics of the Probabilistic Typed Lambda Calculus (Markov Chain Semantics, Termination Behavior, and Denotational Semantics).</i>" Springer, 2017.</li>
<li>Jon Kleinberg and Éva Tardos. <i>Algorithm Design</i>. Chapter 13: "Randomized algorithms".</li>
<li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation journal cs1" id="CITEREFFallis2000">Fallis, D. (2000). "The reliability of randomized algorithms". <i>The British Journal for the Philosophy of Science</i>. <b>51</b> (2): 255–271. doi:10.1093/bjps/51.2.255.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+British+Journal+for+the+Philosophy+of+Science&amp;rft.atitle=The+reliability+of+randomized+algorithms&amp;rft.volume=51&amp;rft.issue=2&amp;rft.pages=255-271&amp;rft.date=2000&amp;rft_id=info%3Adoi%2F10.1093%2Fbjps%2F51.2.255&amp;rft.aulast=Fallis&amp;rft.aufirst=D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandomized+algorithm"></span></li>
<li>M. Mitzenmacher and E. Upfal. <i>Probability and Computing: Randomized Algorithms and Probabilistic Analysis</i>. Cambridge University Press, New York (NY), 2005.</li>
<li>Rajeev Motwani and P. Raghavan. <i>Randomized Algorithms</i>. Cambridge University Press, New York (NY), 1995.</li>
<li>Rajeev Motwani and P. Raghavan. Randomized Algorithms. A survey on Randomized Algorithms.</li>
<li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation cs2" id="CITEREFChristos_Papadimitriou1993">Christos Papadimitriou (1993), <i>Computational Complexity</i> (1st ed.), Addison Wesley, ISBN <bdi>978-0-201-53082-7</bdi></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Computational+Complexity&amp;rft.edition=1st&amp;rft.pub=Addison+Wesley&amp;rft.date=1993&amp;rft.isbn=978-0-201-53082-7&amp;rft.au=Christos+Papadimitriou&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandomized+algorithm"></span> Chapter 11: Randomized computation, pp. 241–278.</li>
<li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation journal cs1" id="CITEREFRabin1980">Rabin, Michael O. (1980). "Probabilistic algorithm for testing primality". <i>Journal of Number Theory</i>. <b>12</b>: 128–138. doi:<span class="cs1-lock-free" title="Freely accessible">10.1016/0022-314X(80)90084-0</span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Number+Theory&amp;rft.atitle=Probabilistic+algorithm+for+testing+primality&amp;rft.volume=12&amp;rft.pages=128-138&amp;rft.date=1980&amp;rft_id=info%3Adoi%2F10.1016%2F0022-314X%2880%2990084-0&amp;rft.aulast=Rabin&amp;rft.aufirst=Michael+O.&amp;rft_id=%2F%2Fdoi.org%2F10.1016%252F0022-314X%252880%252990084-0&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ARandomized+algorithm"></span></li>
<li>A. A. Tsay, W. S. Lovejoy, David R. Karger, <i>Random Sampling in Cut, Flow, and Network Design Problems</i>, Mathematics of Operations Research, 24(2):383–413, 1999.</li></ul>
<!-- 
NewPP limit report
Parsed by mw2309
Cached time: 20221224012816
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.309 seconds
Real time usage: 0.439 seconds
Preprocessor visited node count: 1909/1000000
Post‐expand include size: 38788/2097152 bytes
Template argument size: 5675/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 5/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 46807/5000000 bytes
Lua time usage: 0.169/10.000 seconds
Lua memory usage: 6486227/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  323.622      1 -total
 42.50%  137.555      1 Template:Reflist
 29.02%   93.906      4 Template:Cite_journal
 14.78%   47.826      1 Template:Short_description
 12.63%   40.887      8 Template:Citation
 12.62%   40.847      1 Template:Probabilistic
 12.12%   39.225      1 Template:Sidebar
  8.13%   26.305      2 Template:Pagetype
  6.49%   21.007      1 Template:Distinguish-redirect
  5.10%   16.515      1 Template:Isbn
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:495383-0!canonical and timestamp 20221224012816 and revision id 1083506589.
 -->
</div></body>
</html>