<!DOCTYPE html>
<html>
<head>
<title>strongly_NP-hard</title>
</head>
<body>
<div class="mw-parser-output"><p>In computational complexity, <b>strong NP-completeness</b> is a property of computational problems that is a special case of NP-completeness. A general computational problem may have numerical parameters.  For example, the input to the bin packing problem is a list of objects of specific sizes and a size for the bins that must contain the objects—these object sizes and bin size are numerical parameters.
</p><p>A problem is said to be strongly NP-complete (NP-complete in the strong sense), if it remains NP-complete even when all of its numerical parameters are bounded by a polynomial in the length of the input.<sup class="reference" id="cite_ref-1">[1]</sup> A problem is said to be strongly NP-hard if a strongly NP-complete problem has a polynomial reduction to it; in combinatorial optimization, particularly,  the phrase "strongly NP-hard" is reserved for problems that are not known to have a polynomial reduction to another strongly NP-complete problem.
</p><p>Normally numerical parameters to a problem are given in positional notation, so a problem of input size <i>n</i> might contain parameters whose size is exponential in <i>n</i>.  If we redefine the problem to have the parameters given in unary notation, then the parameters must be bounded by the input size.  Thus strong NP-completeness or NP-hardness may also be defined as the NP-completeness or NP-hardness of this unary version of the problem.
</p><p>For example, bin packing is strongly NP-complete while the 0-1 Knapsack problem is only weakly NP-complete.  Thus the version of bin packing where the object and bin sizes are integers bounded by a polynomial remains NP-complete, while the corresponding version of the Knapsack problem can be solved in pseudo-polynomial time by dynamic programming.
</p><p>From a theoretical perspective any strongly NP-hard optimization problem with a polynomially bounded objective function cannot have a fully polynomial-time approximation scheme (or FPTAS) unless P = NP.<sup class="reference" id="cite_ref-vvv_2-0">[2]</sup>
<sup class="reference" id="cite_ref-3">[3]</sup> However, the converse fails: e.g. if P does not equal NP,  knapsack with two constraints is not strongly NP-hard, but has no FPTAS even when the optimal objective is polynomially bounded.<sup class="reference" id="cite_ref-4">[4]</sup>
</p><p>Some strongly NP-complete problems may still be easy to solve <i>on average</i>, but it's more likely that difficult instances will be encountered in practice.
</p>
<h2><span class="mw-headline" id="Strong_and_weak_NP-hardness_vs._strong_and_weak_polynomial-time_algorithms">Strong and weak NP-hardness vs. strong and weak polynomial-time algorithms</span><span class="mw-editsection"></span></h2>
<p>Assuming P ≠ NP, the following are true for computational problems on integers:<sup class="reference" id="cite_ref-5">[5]</sup>
</p>
<ul><li>If a problem is weakly NP-hard, then it does not have a weakly polynomial time algorithm (polynomial in the number of integers and the <i>number of bits</i> in the largest integer), but it may have a pseudopolynomial time algorithm (polynomial in the number of integers and the <i>magnitude</i> of the largest integer). An example is the partition problem. Both weak NP-hardness and weak polynomial-time correspond to encoding the input agents in binary coding.</li></ul>
<ul><li>If a problem is strongly NP-hard, then it does not even have a pseudo-polynomial time algorithm. It also does not have a fully-polynomial time approximation scheme. An example is the 3-partition problem. Both strong NP-hardness and pseudo-polynomial time correspond to encoding the input agents in unary coding.</li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style>
<!-- 
NewPP limit report
Parsed by mw1352
Cached time: 20221214144341
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.122 seconds
Real time usage: 0.148 seconds
Preprocessor visited node count: 310/1000000
Post‐expand include size: 11120/2097152 bytes
Template argument size: 86/2097152 bytes
Highest expansion depth: 8/100
Expensive parser function count: 0/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 16292/5000000 bytes
Lua time usage: 0.069/10.000 seconds
Lua memory usage: 3358089/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  127.602      1 -total
 97.97%  125.008      1 Template:Reflist
 61.55%   78.545      1 Template:Cite_journal
 12.57%   16.040      3 Template:Cite_book
  4.16%    5.302      1 Template:Cite_web
  2.25%    2.871      1 Template:Main_other
  1.96%    2.502      1 Template:Strong_and_weak_NP_hardness
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:6319351-0!canonical and timestamp 20221214144341 and revision id 1107142190.
 -->
</div></body>
</html>