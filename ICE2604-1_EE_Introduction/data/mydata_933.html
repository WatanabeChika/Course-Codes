<!DOCTYPE html>
<html>
<head>
<title>smoothsort</title>
</head>
<body>
<div class="mw-parser-output">
<style data-mw-deduplicate="TemplateStyles:r1066479718">.mw-parser-output .infobox-subbox{padding:0;border:none;margin:-3px;width:auto;min-width:100%;font-size:100%;clear:none;float:none;background-color:transparent}.mw-parser-output .infobox-3cols-child{margin:auto}.mw-parser-output .infobox .navbar{font-size:100%}body.skin-minerva .mw-parser-output .infobox-header,body.skin-minerva .mw-parser-output .infobox-subheader,body.skin-minerva .mw-parser-output .infobox-above,body.skin-minerva .mw-parser-output .infobox-title,body.skin-minerva .mw-parser-output .infobox-image,body.skin-minerva .mw-parser-output .infobox-full-data,body.skin-minerva .mw-parser-output .infobox-below{text-align:center}</style><table class="infobox"><caption class="infobox-title">Smoothsort</caption><tbody><tr><td class="infobox-image" colspan="2"><img alt="An animation depicting smoothsort's operation, showing the heap being built and then disassembled," data-file-height="226" data-file-width="295" decoding="async" height="226" src="//upload.wikimedia.org/wikipedia/commons/a/a5/Smoothsort.gif" width="295"/></td></tr><tr><th class="infobox-label" scope="row">Class</th><td class="infobox-data">Sorting algorithm</td></tr><tr><th class="infobox-label" scope="row">Data structure</th><td class="infobox-data">Array</td></tr><tr><th class="infobox-label" scope="row">Worst-case performance</th><td class="infobox-data"><span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span></td></tr><tr><th class="infobox-label" scope="row">Best-case performance</th><td class="infobox-data"><span class="texhtml"><i>O</i>(<i>n</i>)</span></td></tr><tr><th class="infobox-label" scope="row">Average performance</th><td class="infobox-data"><span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span></td></tr><tr><th class="infobox-label" scope="row">Worst-case space complexity</th><td class="infobox-data"><span class="texhtml"><i>O</i>(<i>n</i>)</span> total, <span class="texhtml"><i>O</i>(1)</span> auxiliary</td></tr></tbody></table>
<p>In computer science, <b>smoothsort</b> is a comparison-based sorting algorithm. A variant of heapsort, it was invented and published by Edsger Dijkstra in 1981.<sup class="reference" id="cite_ref-EWD-796a_1-0">[1]</sup> Like heapsort, smoothsort is an in-place algorithm with an upper bound of <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span>,<sup class="reference" id="cite_ref-hertel_2-0">[2]</sup> but it is not a stable sort.<sup class="reference" id="cite_ref-3">[3]</sup><sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><span title="The material near this tag may rely on a self-published source. (January 2016)">self-published source?</span></i>]</sup><sup class="reference" id="cite_ref-4">[4]</sup>  The advantage of smoothsort is that it comes closer to <span class="texhtml"><i>O</i>(<i>n</i>)</span> time if the input is already sorted to some degree, whereas heapsort averages <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span> regardless of the initial sorted state.
</p>

<h2><span class="mw-headline" id="Overview">Overview</span><span class="mw-editsection"></span></h2>
<p>Like heapsort, smoothsort organizes the input into a priority queue and then repeatedly extracts the maximum.  Also like heapsort, the priority queue is an implicit heap data structure (a heap-ordered implicit binary tree), which occupies a prefix of the array.  Each extraction shrinks the prefix and adds the extracted element to a growing sorted suffix.  When the prefix has shrunk to nothing, the array is completely sorted.
</p><p>Heapsort maps the binary tree to the array using a top-down breadth-first traversal of the tree; the array begins with the root of the tree, then its two children, then four grandchildren, and so on.  Every element has a well-defined depth below the root of the tree, and every element except the root has its parent earlier in the array.  Its height above the leaves, however, depends on the size of the array.  This has the disadvantage that every element must be moved as part of the sorting process: it must pass through the root before being moved to its final location.
</p><p>Smoothsort uses a different mapping, a bottom-up depth-first post-order traversal.  A left child is followed by the subtree rooted at its sibling, and a right child is followed by its parent.  Every element has a well-defined height above the leaves, and every non-leaf element has its <i>children</i> earlier in the array.  Its depth below the root, however, depends on the size of the array.  The algorithm is organized so the root is at the end of the heap, and at the moment that an element is extracted from the heap it is already in its final location and does not need to be moved.  Also, a sorted array is already a valid heap, and many sorted intervals are valid heap-ordered subtrees.
</p><p>More formally, every position <span class="texhtml mvar" style="font-style:italic;">i</span> is the root of a unique subtree, whose nodes occupy a contiguous interval that ends at <span class="texhtml mvar" style="font-style:italic;">i</span>.  An initial prefix of the array (including the whole array), might be such an interval corresponding to a subtree, but in general decomposes as a union of a number of successive such subtree intervals, which Dijkstra calls "stretches". Any subtree without a parent (i.e. rooted at a position whose parent lies beyond the prefix under consideration) gives a stretch in the decomposition of that interval, which decomposition is therefore unique. When a new node is appended to the prefix, one of two cases occurs: either the position is a leaf and adds a stretch of length 1 to the decomposition, or it combines with the last two stretches, becoming the parent of their respective roots, thus replacing the two stretches by a new stretch containing their union plus the new (root) position.
</p><p>Dijkstra noted<sup class="reference" id="cite_ref-EWD-796a_1-1">[1]</sup> that the obvious rule would be to combine stretches if and only if they have equal size, in which case all subtrees would be perfect binary trees of size <span class="texhtml">2<sup><i>k</i></sup>−1</span>.  However, he chose a different rule, which gives more possible tree sizes.  This has the same asymptotic efficiency,<sup class="reference" id="cite_ref-hertel_2-1">[2]</sup> but gains a small constant factor in efficiency by requiring fewer stretches to cover each interval.
</p><p>The rule Dijkstra uses is that the last two stretches are combined if and only if their sizes are consecutive Leonardo numbers <span class="texhtml"><i>L</i>(<i>i</i>+1)</span> and <span class="texhtml"><i>L</i>(<i>i</i>)</span> (in that order), which numbers are recursively defined, in a manner very similar to the Fibonacci numbers, as:
</p>
<ul><li><span class="texhtml"><i>L</i>(0) = <i>L</i>(1) = 1</span></li>
<li><span class="texhtml"><i>L</i>(<i>k</i>+2) = <i>L</i>(<i>k</i>+1) + <i>L</i>(<i>k</i>) + 1</span></li></ul>
<p>As a consequence, the size of any subtree is a Leonardo number. The sequence of stretch sizes decomposing the first <span class="texhtml mvar" style="font-style:italic;">n</span> positions, for any <span class="texhtml mvar" style="font-style:italic;">n</span>, can be found in a greedy manner: the first size is the largest Leonardo number not exceeding <span class="texhtml mvar" style="font-style:italic;">n</span>, and the remainder (if any) is decomposed recursively. The sizes of stretches are decreasing, strictly so except possibly for two final sizes 1, and avoiding successive Leonardo numbers except possibly for the final two sizes.
</p><p>In addition to each stretch being a heap-ordered tree, the roots of the trees are maintained in sorted order.  This effectively adds a third child (which Dijkstra calls a "stepson") to each root linking it to the preceding root.  This combines all of the trees together into one global heap. with the global maximum at the end.
</p><p>Although the location of each node's stepson is fixed, the link only exists for tree roots, meaning that links are removed whenever trees are merged.  This is different from ordinary children, which are linked as long as the parent exists.
</p><p>In the first (heap growing) phase of sorting, an increasingly large initial part of the array is reorganized so that the subtree for each of its stretches is a max-heap: the entry at any non-leaf position is at least as large as the entries at the positions that are its children.  In addition, all roots are at least as large as their stepsons.
</p><p>In the second (heap shrinking) phase, the maximal node is detached from the end of the array (without needing to move it) and the heap invariants are re-established among its children.  (Specifically, among the newly created stepsons.)
</p><p>Practical implementation frequently needs to compute Leonardo numbers <span class="texhtml"><i>L</i>(<i>k</i>)</span>.  Dijkstra provides clever code which uses a fixed number of integer variables to efficiently compute the values needed at the time they are needed.  Alternatively, if there is a finite bound <span class="texhtml mvar" style="font-style:italic;">N</span> on the size of arrays to be sorted, a precomputed table of Leonardo numbers can be stored in <span class="texhtml"><i>O</i>(log <i>N</i>)</span> space.
</p>
<h2><span class="mw-headline" id="Operations">Operations</span><span class="mw-editsection"></span></h2>
<p>While the two phases of the sorting procedure are opposite to each other as far as the evolution of the sequence-of-heaps structure is concerned, they are implemented using one core primitive, equivalent to the 
"sift down" operation in a binary max-heap.
</p>
<h3><span class="mw-headline" id="Sifting_down">Sifting down</span><span class="mw-editsection"></span></h3>
<p>The core sift-down operation (which Dijkstra calls "trinkle") restores the heap invariant when it is possibly violated only at the root node.  If the root node is less than any of its children, it is swapped with its greatest child and the process repeated with the root node in its new subtree.
</p><p>The difference between smoothsort and a binary max-heap is that the root of each stretch must be ordered with respect to a third "stepson": the root of the preceding stretch.  So the sift-down procedure starts with a series of four-way comparisons (the root node and three children) until the stepson is not the maximal element, then a series of three-way comparisons (the root plus two children) until the root node finds its final home and the invariants are re-established.
</p><p>Each tree is a full binary tree: each node has two children or none. There is no need to deal with the special case of one child which occurs in a standard implicit binary heap.  (But the special case of stepson links more than makes up for this saving.)
</p><p>Because there are <span class="texhtml"><i>O</i>(log <i>n</i>)</span> stretches, each of which is a tree of depth <span class="texhtml"><i>O</i>(log <i>n</i>)</span>, the time to perform each sifting-down operation is bounded by <span class="texhtml"><i>O</i>(log <i>n</i>)</span>.
</p>
<h3><span class="mw-headline" id="Growing_the_heap_region_by_incorporating_an_element_to_the_right">Growing the heap region by incorporating an element to the right</span><span class="mw-editsection"></span></h3>
<p>When an additional element is considered for incorporation into the sequence of stretches (list of disjoint heap structures) it either forms a new one-element stretch, or it combines the two rightmost stretches by becoming the parent of both their roots and forming a new stretch that replaces the two in the sequence. Which of the two happens depends only on the sizes of the stretches currently present (and ultimately only on the index of the element added); Dijkstra stipulated that stretches are combined if and only if their sizes are <span class="texhtml"><i>L</i>(<i>k</i>+1)</span> and <span class="texhtml"><i>L</i>(<i>k</i>)</span> for some <span class="texhtml mvar" style="font-style:italic;">k</span>, i.e., consecutive Leonardo numbers; the new stretch will have size <span class="texhtml"><i>L</i>(<i>k</i>+2)</span>.
</p><p>In either case, the new element must be sifted down to its correct place in the heap structure.  Even if the new node is a one-element stretch, it must still be sorted relative to the preceding stretch's root.
</p>
<h4><span class="mw-headline" id="Optimization">Optimization</span><span class="mw-editsection"></span></h4>
<p>Dijkstra's algorithm saves work by observing that the full heap invariant is required at the end of the growing phase, but it is not required at every intermediate step.  In particular, the requirement that an element be greater than its stepson is only important for the elements which are the final tree roots.
</p><p>Therefore, when an element is added, compute the position of its future parent.  If this is within the range of remaining values to be sorted, act as if there is no stepson and only sift down within the current tree.
</p>
<h3><span class="mw-headline" id="Shrinking_the_heap_region_by_separating_the_rightmost_element_from_it">Shrinking the heap region by separating the rightmost element from it</span><span class="mw-editsection"></span></h3>
<p>During this phase, the form of the sequence of stretches goes through the changes of the growing phase in reverse. No work at all is needed when separating off a leaf node, but for a non-leaf node its two children become roots of new stretches, and need to be moved to their proper place in the sequence of roots of stretches. This can be obtained by applying sift-down twice: first for the left child, and then for the right child (whose stepson was the left child).
</p><p>Because half of all nodes in a full binary tree are leaves, this performs an average of one sift-down operation per node.
</p>
<h4><span class="mw-headline" id="Optimization_2">Optimization</span><span class="mw-editsection"></span></h4>
<p>It is already known that the newly exposed roots are correctly ordered with respect to their normal children; it is only the ordering relative to their stepsons which is in question.  Therefore, while shrinking the heap, the first step of sifting down can be simplified to a single comparison with the stepson.  If a swap occurs, subsequent steps must do the full four-way comparison.
</p>
<h2><span class="mw-headline" id="Analysis">Analysis</span><span class="mw-editsection"></span></h2>
<p>Smoothsort takes <span class="texhtml"><i>O</i>(<i>n</i>)</span> time to process a presorted array, <span class="texhtml"><i>O</i>(<i>n</i> log  <i>n</i>)</span> in the worst case, and achieves nearly-linear performance on many nearly-sorted inputs.  However, it does not handle all nearly-sorted sequences optimally.  Using the count of inversions as a measure of un-sortedness (the number of pairs of indices <span class="texhtml mvar" style="font-style:italic;">i</span> and <span class="texhtml mvar" style="font-style:italic;">j</span> with <span class="texhtml"><i>i</i> &lt; <i>j</i></span> and <span class="texhtml"><i>A</i>[<i>i</i>] &gt; <i>A</i>[<i>j</i>]</span>; for randomly sorted input this is approximately <span class="texhtml"><i>n</i><sup>2</sup>/4</span>), there are possible input sequences with <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span> inversions which cause it to take <span class="texhtml">Ω(<i>n</i> log <i>n</i>)</span> time, whereas other adaptive sorting algorithms can solve these cases in <span class="texhtml"><i>O</i>(<i>n</i> log log <i>n</i>)</span> time.<sup class="reference" id="cite_ref-hertel_2-2">[2]</sup>
</p><p>The smoothsort algorithm needs to be able to hold in memory the sizes of all of the trees in the Leonardo heap.  Since they are sorted by order and all orders are distinct, this is usually done using a bit vector indicating which orders are present.  Moreover, since the largest order is at most <span class="texhtml"><i>O</i>(log <i>n</i>)</span>, these bits can be encoded in <span class="texhtml"><i>O</i>(1)</span> machine words, assuming a transdichotomous machine model.
</p><p>Note that <span class="texhtml"><i>O</i>(1)</span> machine words is not the same thing as <i>one</i> machine word.  A 32-bit vector would only suffice for sizes less than <span class="texhtml"><i>L</i>(32) = 7049155</span>.  A 64-bit vector will do for sizes less than <span class="texhtml"><i>L</i>(64) = 34335360355129 ≈ 2<sup>45</sup></span>.  In general, it takes <span class="texhtml">1/log<sub>2</sub>(<i>φ</i>) ≈ 1.44</span> bits of vector per bit of size.
</p>
<h2><span class="mw-headline" id="Poplar_sort">Poplar sort</span><span class="mw-editsection"></span></h2>
<p>A simpler algorithm inspired by smoothsort is <b>poplar sort</b>.<sup class="reference" id="cite_ref-5">[5]</sup>  Named after the rows of trees of decreasing size often seen in Dutch polders, it performs fewer comparisons than smoothsort for inputs that are not mostly sorted, but cannot achieve linear time for sorted inputs.
</p><p>The significant change made by poplar sort in that the roots of the various trees are <i>not</i> kept in sorted order; there are no "stepson" links tying them together into a single heap.  Instead, each time the heap is shrunk in the second phase, the roots are searched to find the maximum entry.
</p><p>Because there are <span class="texhtml mvar" style="font-style:italic;">n</span> shrinking steps, each of which must search <span class="texhtml"><i>O</i>(log <i>n</i>)</span> tree roots for the maximum, the best-case run time for poplar sort is  <span class="texhtml"><i>O</i>(<i>n</i> log <i>n</i>)</span>.
</p><p>The authors also suggest using perfect binary trees rather than Leonardo trees to provide further simplification, but this is a less significant change.
</p><p>The same structure has been proposed as a general-purpose priority queue under the name <b>post-order heap</b>,<sup class="reference" id="cite_ref-6">[6]</sup> achieving <span class="texhtml"><i>O</i>(1)</span> amortized insertion time in a structure simpler than an implicit binomial heap.
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"></span></h2>
<p>The musl C library uses smoothsort for its implementation of <code>qsort()</code>.<sup class="reference" id="cite_ref-7">[7]</sup><sup class="reference" id="cite_ref-8">[8]</sup>
</p>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"></span></h2>
<ul><li>Commented transcription of EWD796a, 16-Aug-1981</li>
<li>Detailed modern explanation of Smoothsort</li>
<li>wikibooks:Algorithm Implementation/Sorting/Smoothsort</li>
<li>Description and example implementation of Poplar heap</li>
<li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation journal cs1" id="CITEREFNoshitaNakatani1985">Noshita, Kohei; Nakatani, Yoshinobu (April 1985). "On the Nested Heap Structure in Smoothsort". <i>Mathematical Foundations of Computer Science and Their Applications<span style="font-weight: normal"> (Japanese: <span lang="ja">数理解析研究所講究録</span>)</span></i>. <b>556</b>: 1–16.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Mathematical+Foundations+of+Computer+Science+and+Their+Applications%3Cspan+style%3D%22font-weight%3A+normal%22%3E+%28Japanese%3A+%3Cspan+lang%3D%22ja%22%3E%E6%95%B0%E7%90%86%E8%A7%A3%E6%9E%90%E7%A0%94%E7%A9%B6%E6%89%80%E8%AC%9B%E7%A9%B6%E9%8C%B2%3C%2Fspan%3ECategory%3AArticles+containing+Japanese-language+text%29%3C%2Fspan%3E&amp;rft.atitle=On+the+Nested+Heap+Structure+in+Smoothsort&amp;rft.volume=556&amp;rft.pages=1-16&amp;rft.date=1985-04&amp;rft.aulast=Noshita&amp;rft.aufirst=Kohei&amp;rft.au=Nakatani%2C+Yoshinobu&amp;rft_id=http%3A%2F%2Fhdl.handle.net%2F2433%2F98975&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASmoothsort"></span></li></ul>

<!-- 
NewPP limit report
Parsed by mw2363
Cached time: 20221224011645
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.406 seconds
Real time usage: 0.523 seconds
Preprocessor visited node count: 3703/1000000
Post‐expand include size: 50237/2097152 bytes
Template argument size: 5560/2097152 bytes
Highest expansion depth: 14/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 31209/5000000 bytes
Lua time usage: 0.263/10.000 seconds
Lua memory usage: 14644921/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  466.296      1 -total
 26.19%  122.101      3 Template:Cite_journal
 25.82%  120.408      1 Template:Reflist
 22.59%  105.340      1 Template:Nihongo
 13.94%   65.009      1 Template:Cite_EWD
 13.40%   62.473      1 Template:Cite_book
 12.72%   59.328      1 Template:Sorting
 12.22%   56.963      1 Template:Navbox
 11.39%   53.108      1 Template:Short_description
  7.43%   34.664      1 Template:Infobox_Algorithm
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:100450-0!canonical and timestamp 20221224011644 and revision id 1098544685.
 -->
</div></body>
</html>