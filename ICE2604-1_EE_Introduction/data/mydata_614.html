<!DOCTYPE html>
<html>
<head>
<title>metaheuristic</title>
</head>
<body>
<div class="mw-parser-output">
<p>In computer science and mathematical optimization, a <b>metaheuristic</b> is a higher-level procedure or heuristic designed to find, generate, or select a heuristic (partial search algorithm) that may provide a sufficiently good solution to an optimization problem, especially with incomplete or imperfect information or limited computation capacity.<sup class="reference" id="cite_ref-Bala2015_1-0">[1]</sup><sup class="reference" id="cite_ref-Bianchi2009_2-0">[2]</sup> Metaheuristics sample a subset of solutions which is otherwise too large to be completely enumerated or otherwise explored. Metaheuristics may make relatively few assumptions about the optimization problem being solved and so may be usable for a variety of problems.<sup class="reference" id="cite_ref-blum03metaheuristics_3-0">[3]</sup>
</p><p>Compared to optimization algorithms and iterative methods, metaheuristics do not guarantee that a globally optimal solution can be found on some class of problems.<sup class="reference" id="cite_ref-blum03metaheuristics_3-1">[3]</sup> Many metaheuristics implement some form of stochastic optimization, so that the solution found is dependent on the set of random variables generated.<sup class="reference" id="cite_ref-Bianchi2009_2-1">[2]</sup> In combinatorial optimization, by searching over a large set of feasible solutions, metaheuristics can often find good solutions with less computational effort than optimization algorithms, iterative methods, or simple heuristics.<sup class="reference" id="cite_ref-blum03metaheuristics_3-2">[3]</sup> As such, they are useful  approaches for optimization problems.<sup class="reference" id="cite_ref-Bianchi2009_2-2">[2]</sup> Several books and survey papers have been published on the subject.<sup class="reference" id="cite_ref-Bianchi2009_2-3">[2]</sup><sup class="reference" id="cite_ref-blum03metaheuristics_3-3">[3]</sup><sup class="reference" id="cite_ref-goldberg89genetic_4-0">[4]</sup><sup class="reference" id="cite_ref-glover03handbook_5-0">[5]</sup><sup class="reference" id="cite_ref-talbi09metaheuristics_6-0">[6]</sup>
</p><p>Most literature on metaheuristics is experimental in nature, describing empirical results based on computer experiments with the algorithms. But some formal theoretical results are also available, often on convergence and the possibility of finding the global optimum.<sup class="reference" id="cite_ref-blum03metaheuristics_3-4">[3]</sup> Many metaheuristic methods have been published with claims of novelty and practical efficacy. While the field also features high-quality research, many of the publications have been of poor quality; flaws include vagueness, lack of conceptual elaboration, poor experiments, and ignorance of previous literature.<sup class="reference" id="cite_ref-Sörensen2013_7-0">[7]</sup>
</p>

<h2><span class="mw-headline" id="Properties">Properties</span><span class="mw-editsection"></span></h2>
<p>These are properties that characterize most metaheuristics:<sup class="reference" id="cite_ref-blum03metaheuristics_3-5">[3]</sup>
</p>
<ul><li>Metaheuristics are strategies that guide the search process.</li>
<li>The goal is to efficiently explore the search space in order to find near–optimal solutions.</li>
<li>Techniques which constitute metaheuristic algorithms range from simple local search procedures to complex learning processes.</li>
<li>Metaheuristic algorithms are approximate and usually non-deterministic.</li>
<li>Metaheuristics are not problem-specific.</li></ul>
<h2><span class="mw-headline" id="Classification">Classification</span><span class="mw-editsection"></span></h2>

<p>There are a wide variety of metaheuristics<sup class="reference" id="cite_ref-Bianchi2009_2-4">[2]</sup> and a number of properties with respect to which to classify them.<sup class="reference" id="cite_ref-blum03metaheuristics_3-6">[3]</sup>
</p>
<h3><span class="mw-headline" id="Local_search_vs._global_search">Local search vs. global search</span><span class="mw-editsection"></span></h3>
<p>One approach is to characterize the type of search strategy.<sup class="reference" id="cite_ref-blum03metaheuristics_3-7">[3]</sup> One type of search strategy is an improvement on simple local search algorithms. A well known local search algorithm is the hill climbing method which is used to find local optimums. However, hill climbing does not guarantee finding global optimum solutions.
</p><p>Many metaheuristic ideas were proposed to improve local search heuristic in order to find better solutions. Such metaheuristics include simulated annealing, tabu search, iterated local search, variable neighborhood search, and GRASP.<sup class="reference" id="cite_ref-blum03metaheuristics_3-8">[3]</sup> These metaheuristics can both be classified as local search-based or global search metaheuristics.
</p><p>Other global search metaheuristic that are not local search-based are usually population-based metaheuristics. Such metaheuristics include ant colony optimization, evolutionary computation, particle swarm optimization, genetic algorithm, and rider optimization algorithm<sup class="reference" id="cite_ref-9">[9]</sup>
</p>
<h3><span class="mw-headline" id="Single-solution_vs._population-based">Single-solution vs. population-based</span><span class="mw-editsection"></span></h3>
<p>Another classification dimension is single solution vs population-based searches.<sup class="reference" id="cite_ref-blum03metaheuristics_3-9">[3]</sup><sup class="reference" id="cite_ref-talbi09metaheuristics_6-1">[6]</sup> Single solution approaches focus on modifying and improving a single candidate solution; single solution metaheuristics include simulated annealing, iterated local search, variable neighborhood search, and guided local search.<sup class="reference" id="cite_ref-talbi09metaheuristics_6-2">[6]</sup> Population-based approaches maintain and improve multiple candidate solutions, often using population characteristics to guide the search; population based metaheuristics include evolutionary computation, genetic algorithms, and particle swarm optimization.<sup class="reference" id="cite_ref-talbi09metaheuristics_6-3">[6]</sup> Another category of metaheuristics  is Swarm intelligence which is a collective behavior of decentralized, self-organized agents in a population or swarm. Ant colony optimization,<sup class="reference" id="cite_ref-M._Dorigo,_Optimization,_Learning_and_Natural_Algorithms_10-0">[10]</sup> particle swarm optimization,<sup class="reference" id="cite_ref-talbi09metaheuristics_6-4">[6]</sup> social cognitive optimization are examples of this category.
</p>
<h3><span class="mw-headline" id="Hybridization_and_memetic_algorithms">Hybridization and memetic algorithms</span><span class="mw-editsection"></span></h3>
<p>A hybrid metaheuristic is one that combines a metaheuristic with other optimization approaches, such as algorithms from mathematical programming, constraint programming, and machine learning. Both components of a hybrid metaheuristic may run concurrently and exchange information to guide the search.
</p><p>On the other hand, Memetic algorithms<sup class="reference" id="cite_ref-moscato89evolution_11-0">[11]</sup> represent the synergy of evolutionary or any population-based approach with separate individual learning or local improvement procedures for problem search. An example of memetic algorithm is the use of a local search algorithm instead of a basic mutation operator in evolutionary algorithms.
</p>
<h3><span class="mw-headline" id="Parallel_metaheuristics">Parallel metaheuristics</span><span class="mw-editsection"></span></h3>
<p>A parallel metaheuristic is one that uses the techniques of parallel programming to run multiple metaheuristic searches in parallel; these may range from simple distributed schemes to concurrent search runs that interact to improve the overall solution.
</p>
<h3><span class="mw-headline" id="Nature-inspired_and_metaphor-based_metaheuristics">Nature-inspired and metaphor-based metaheuristics</span><span class="mw-editsection"></span></h3>
<style data-mw-deduplicate="TemplateStyles:r1033289096">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style>
<p>A very active area of research is the design of nature-inspired metaheuristics. Many recent metaheuristics, especially evolutionary computation-based algorithms, are inspired by natural systems. Nature acts as a source of concepts, mechanisms and principles for designing of artificial computing systems to deal with complex computational problems. Such metaheuristics include simulated annealing, evolutionary algorithms, ant colony optimization and particle swarm optimization. A large number of more recent metaphor-inspired metaheuristics have started to attract criticism in the research community for hiding their lack of novelty behind an elaborate metaphor.<sup class="reference" id="cite_ref-Sörensen2013_7-1">[7]</sup>
</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"></span></h2>
<p>Metaheuristics are used for combinatorial optimization in which an optimal solution is sought over a discrete search-space. An example problem is the travelling salesman problem where the search-space of candidate solutions grows faster than exponentially as the size of the problem increases, which makes an exhaustive search for the optimal solution infeasible. Additionally, multidimensional combinatorial problems, including most design problems in engineering<sup class="reference" id="cite_ref-12">[12]</sup><sup class="reference" id="cite_ref-13">[13]</sup><sup class="reference" id="cite_ref-14">[14]</sup> such as form-finding and behavior-finding, suffer from the curse of dimensionality, which also makes them infeasible for exhaustive search or analytical methods. Metaheuristics are also widely used for jobshop scheduling and job selection problems.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><span title="This claim needs references to reliable sources. (September 2019)">citation needed</span></i>]</sup> Popular metaheuristics for combinatorial problems include simulated annealing by Kirkpatrick et al.,<sup class="reference" id="cite_ref-kirkpatrick83optimization_15-0">[15]</sup> genetic algorithms by Holland et al.,<sup class="reference" id="cite_ref-holland75adaptation_16-0">[16]</sup> scatter search<sup class="reference" id="cite_ref-glover77scattersearch_17-0">[17]</sup> and tabu search<sup class="reference" id="cite_ref-glover86future_18-0">[18]</sup> by Glover. Literature review on metaheuristic optimization,<sup class="reference" id="cite_ref-19">[19]</sup>
suggested that it was Fred Glover who coined the word metaheuristics.<sup class="reference" id="cite_ref-20">[20]</sup>
</p>
<h2><span class="mw-headline" id="Metaheuristic_Optimization_Frameworks">Metaheuristic Optimization Frameworks</span><span class="mw-editsection"></span></h2>
<p>A MOF can be defined as ‘‘a set of software tools that provide a correct and reusable implementation of a set of metaheuristics, and the basic mechanisms to accelerate the implementation of its partner subordinate heuristics (possibly including solution encodings and technique-specific operators), which are necessary to solve a particular problem instance using techniques provided’’.<sup class="reference" id="cite_ref-Parejo_et_al.2012_21-0">[21]</sup>
</p><p>There are many candidate optimization tools which can be considered as a MOF of varying feature: Comet, EvA2, evolvica, Evolutionary::Algorithm, GAPlayground, jaga, JCLEC, JGAP, jMetal, n-genes, Open Beagle, Opt4j, ParadisEO/EO, Pisa, Watchmaker, FOM, Hypercube, HotFrame, Templar, EasyLocal, iOpt, OptQuest, JDEAL, Optimization Algorithm Toolkit, HeuristicLab, MAFRA, Localizer, GALIB, DREAM, Discropt, MALLBA, MAGMA, Metaheuristics.jl, UOF<sup class="reference" id="cite_ref-Parejo_et_al.2012_21-1">[21]</sup> and OptaPlanner.
</p>
<h2><span class="mw-headline" id="Contributions">Contributions</span><span class="mw-editsection"></span></h2>
<p>Many different metaheuristics are in existence and new variants are continually being proposed. Some of the most significant contributions to the field are:
</p>
<ul><li>1952: Robbins and Monro work on stochastic optimization methods.<sup class="reference" id="cite_ref-robbins52stochastic_22-0">[22]</sup></li>
<li>1954: Barricelli carry out the first simulations of the evolution process and use them on general optimization problems.<sup class="reference" id="cite_ref-barricelli54esempi_23-0">[23]</sup></li>
<li>1963: Rastrigin proposes random search.<sup class="reference" id="cite_ref-rastrigin63convergence_24-0">[24]</sup></li>
<li>1965: Matyas proposes random optimization.<sup class="reference" id="cite_ref-matyas65random_25-0">[25]</sup></li>
<li>1965: Nelder and Mead propose a simplex heuristic, which was shown by Powell to converge to non-stationary points on some problems.<sup class="reference" id="cite_ref-nelder65simplex_26-0">[26]</sup></li>
<li>1965: Ingo Rechenberg discovers the first Evolution Strategies algorithm.<sup class="reference" id="cite_ref-rechenberg65ES_27-0">[27]</sup></li>
<li>1966: Fogel et al. propose evolutionary programming.<sup class="reference" id="cite_ref-fogel66artificial_28-0">[28]</sup></li>
<li>1970: Hastings proposes the Metropolis–Hastings algorithm.<sup class="reference" id="cite_ref-hastings70monte_29-0">[29]</sup></li>
<li>1970: Cavicchio proposes adaptation of control parameters for an optimizer.<sup class="reference" id="cite_ref-cavicchio70adaptive_30-0">[30]</sup></li>
<li>1970: Kernighan and Lin propose a graph partitioning method, related to variable-depth search and prohibition-based (tabu) search.<sup class="reference" id="cite_ref-kernighan1970efficient_31-0">[31]</sup></li>
<li>1975: Holland proposes the genetic algorithm.<sup class="reference" id="cite_ref-holland75adaptation_16-1">[16]</sup></li>
<li>1977: Glover proposes scatter search.<sup class="reference" id="cite_ref-glover77scattersearch_17-1">[17]</sup></li>
<li>1978: Mercer and Sampson propose a metaplan for tuning an optimizer's parameters by using another optimizer.<sup class="reference" id="cite_ref-mercer78adaptive_32-0">[32]</sup></li>
<li>1980: Smith describes genetic programming.<sup class="reference" id="cite_ref-smith80learning_33-0">[33]</sup></li>
<li>1983: Kirkpatrick et al. propose simulated annealing.<sup class="reference" id="cite_ref-kirkpatrick83optimization_15-1">[15]</sup></li>
<li>1986: Glover proposes tabu search, first mention of the term <i>metaheuristic</i>.<sup class="reference" id="cite_ref-glover86future_18-1">[18]</sup></li>
<li>1989: Moscato proposes memetic algorithms.<sup class="reference" id="cite_ref-moscato89evolution_11-1">[11]</sup></li>
<li>1990: Moscato and Fontanari,<sup class="reference" id="cite_ref-34">[34]</sup> and Dueck and Scheuer,<sup class="reference" id="cite_ref-35">[35]</sup> independently proposed a deterministic update rule for simulated annealing which accelerated the search. This led to the threshold accepting metaheuristic.</li>
<li>1992: Dorigo introduces ant colony optimization in his PhD thesis.<sup class="reference" id="cite_ref-M._Dorigo,_Optimization,_Learning_and_Natural_Algorithms_10-1">[10]</sup></li>
<li>1995: Wolpert and Macready prove the no free lunch theorems.<sup class="reference" id="cite_ref-wolpert95nofreelunch_36-0">[36]</sup><sup class="reference" id="cite_ref-Igel2003_37-0">[37]</sup><sup class="reference" id="cite_ref-Auger2010_38-0">[38]</sup><sup class="reference" id="cite_ref-Droste2002_39-0">[39]</sup></li></ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"></span></h2>
<ul><li>Stochastic search</li>
<li>Meta-optimization</li>
<li>Matheuristics</li>
<li>Hyper-heuristics</li>
<li>Swarm intelligence</li>
<li>Genetic algorithms</li>
<li>Genetic programming</li>
<li>Simulated annealing</li>
<li>Workforce modeling</li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"></span></h2>
<ul><li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation book cs1" id="CITEREFSörensenSevauxGlover2017">Sörensen, Kenneth; Sevaux, Marc; Glover, Fred (2017-01-16). "A History of Metaheuristics" <span class="cs1-format">(PDF)</span>.  In Martí, Rafael; Panos, Pardalos; Resende, Mauricio (eds.). <i>Handbook of Heuristics</i>. Springer. ISBN <bdi>978-3-319-07123-7</bdi>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+History+of+Metaheuristics&amp;rft.btitle=Handbook+of+Heuristics&amp;rft.pub=Springer&amp;rft.date=2017-01-16&amp;rft.isbn=978-3-319-07123-7&amp;rft.aulast=S%C3%B6rensen&amp;rft.aufirst=Kenneth&amp;rft.au=Sevaux%2C+Marc&amp;rft.au=Glover%2C+Fred&amp;rft_id=http%3A%2F%2Fleeds-faculty.colorado.edu%2Fglover%2F468%2520-%2520A%2520History%2520of%2520Metaheuristics%2520w%2520Sorensen%2520%2526%2520Sevaux.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMetaheuristic"></span></li>
<li>Ashish Sharma (2022), Nature Inspired Algorithms with Randomized Hypercomputational Perspective. <i>Information Sciences.</i> https://doi.org/10.1016/j.ins.2022.05.020.</li></ul>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"></span></h2>
<ul><li><link href="mw-data:TemplateStyles:r1067248974" rel="mw-deduplicated-inline-style"/><cite class="citation web cs1" id="CITEREFFred_Glover_and_Kenneth_Sörensen">Fred Glover and Kenneth Sörensen (ed.). "Metaheuristics". <i>Scholarpedia</i>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Scholarpedia&amp;rft.atitle=Metaheuristics&amp;rft_id=http%3A%2F%2Fwww.scholarpedia.org%2Farticle%2FMetaheuristics&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMetaheuristic"></span></li>
<li>EU/ME forum for researchers in the field.</li></ul>

<!-- 
NewPP limit report
Parsed by mw2378
Cached time: 20221220214223
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.438 seconds
Real time usage: 0.529 seconds
Preprocessor visited node count: 2781/1000000
Post‐expand include size: 130573/2097152 bytes
Template argument size: 2304/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 4/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 123526/5000000 bytes
Lua time usage: 0.254/10.000 seconds
Lua memory usage: 6765871/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  452.027      1 -total
 53.56%  242.126      1 Template:Reflist
 34.24%  154.796     25 Template:Cite_journal
 13.21%   59.729      1 Template:Optimization_algorithms
 12.84%   58.042      1 Template:Short_description
 12.79%   57.808      1 Template:Navbox_with_collapsible_groups
  7.49%   33.866      8 Template:Cite_book
  7.05%   31.864      2 Template:Pagetype
  5.88%   26.586      1 Template:Citation_needed
  5.28%   23.868      1 Template:Fix
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:774458-0!canonical and timestamp 20221220214223 and revision id 1115204184.
 -->
</div></body>
</html>