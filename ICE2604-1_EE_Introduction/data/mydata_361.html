<!DOCTYPE html>
<html>
<head>
<title>external_merge</title>
</head>
<body>
<div class="mw-parser-output">
<p><b>External sorting</b> is a class of sorting algorithms that can handle massive amounts of data. External sorting is required when the data being sorted do not fit into the main memory of a computing device (usually RAM) and instead they must reside in the slower external memory, usually a disk drive. Thus, external sorting algorithms are external memory algorithms and thus applicable in the external memory model of computation.
</p><p>External sorting algorithms generally fall into two types, distribution sorting, which resembles quicksort, and external merge sort, which resembles merge sort. The latter typically uses a hybrid sort-merge strategy.  In the sorting phase, chunks of data small enough to fit in main memory are read, sorted, and written out to a temporary file.  In the merge phase, the sorted subfiles are combined into a single larger file.
</p>

<h2><span class="mw-headline" id="Model">Model</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1033289096">.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}</style>
<p>External sorting algorithms can be analyzed in the external memory model. In this model, a cache or internal memory of size <span class="texhtml mvar" style="font-style:italic;">M</span> and an unbounded external memory are divided into blocks of size <span class="texhtml mvar" style="font-style:italic;">B</span>, and the running time of an algorithm is determined by the number of memory transfers between internal and external memory. Like their cache-oblivious counterparts, asymptotically optimal external sorting algorithms achieve a running time (in Big O notation) of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O\left({\tfrac {N}{B}}\log _{\tfrac {M}{B}}{\tfrac {N}{B}}\right)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mrow>
<mo>(</mo>
<mrow>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="false" scriptlevel="0">
<mfrac>
<mi>N</mi>
<mi>B</mi>
</mfrac>
</mstyle>
</mrow>
<msub>
<mi>log</mi>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="false" scriptlevel="0">
<mfrac>
<mi>M</mi>
<mi>B</mi>
</mfrac>
</mstyle>
</mrow>
</msub>
<mo>⁡<!-- ⁡ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="false" scriptlevel="0">
<mfrac>
<mi>N</mi>
<mi>B</mi>
</mfrac>
</mstyle>
</mrow>
</mrow>
<mo>)</mo>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O\left({\tfrac {N}{B}}\log _{\tfrac {M}{B}}{\tfrac {N}{B}}\right)}</annotation>
</semantics>
</math></span><img alt="{\displaystyle O\left({\tfrac {N}{B}}\log _{\tfrac {M}{B}}{\tfrac {N}{B}}\right)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/496ff85a291f2a81b89ac3b14929c774ad5c0035" style="vertical-align: -2.671ex; width:16.713ex; height:6.343ex;"/></span>.
</p>
<h2><span class="mw-headline" id="External_merge_sort">External merge sort</span><span class="mw-editsection"></span></h2>
<p>One example of external sorting is the external merge sort algorithm, which is a K-way merge algorithm. It sorts chunks that each fit in RAM, then merges the sorted chunks together.<sup class="reference" id="cite_ref-1">[1]</sup><sup class="reference" id="cite_ref-2">[2]</sup>
</p><p>The algorithm first sorts <span class="texhtml mvar" style="font-style:italic;">M</span> items at a time and puts the sorted lists back into external memory. It then recursively does a <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\tfrac {M}{B}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="false" scriptlevel="0">
<mfrac>
<mi>M</mi>
<mi>B</mi>
</mfrac>
</mstyle>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\tfrac {M}{B}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\tfrac {M}{B}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/860c96a1698eb35072253cf373ad394015efe974" style="vertical-align: -1.171ex; width:2.563ex; height:3.509ex;"/></span>-way merge on those sorted lists. To do this merge, <span class="texhtml mvar" style="font-style:italic;">B</span> elements from each sorted list are loaded into internal memory, and the minimum is repeatedly outputted.
</p><p>For example, for sorting 900 megabytes of data using only 100 megabytes of RAM:
</p>
<ol><li>Read 100 MB of the data in main memory and sort by some conventional method, like quicksort.</li>
<li>Write the sorted data to disk.</li>
<li>Repeat steps 1 and 2 until all of the data is in sorted 100 MB chunks (there are 900MB / 100MB = 9 chunks), which now need to be merged into one single output file.</li>
<li>Read the first 10 MB (= 100MB / (9 chunks + 1)) of each sorted chunk into input buffers in main memory and allocate the remaining 10 MB for an output buffer.  (In practice, it might provide better performance to make the output buffer larger and the input buffers slightly smaller.)</li>
<li>Perform a 9-way merge and store the result in the output buffer. Whenever the output buffer fills, write it to the final sorted file and empty it. Whenever any of the 9 input buffers empties, fill it with the next 10 MB of its associated 100 MB sorted chunk until no more data from the chunk is available. This is the key step that makes external merge sort work externally—because the merge algorithm only makes one pass sequentially through each of the chunks, each chunk does not have to be loaded completely; rather, sequential parts of the chunk can be loaded as needed.</li></ol>
<p>Historically, instead of a sort, sometimes a replacement-selection algorithm<sup class="reference" id="cite_ref-3">[3]</sup> was used to perform the initial distribution, to produce on average half as many output chunks of double the length.
</p>
<h3><span class="mw-headline" id="Additional_passes">Additional passes</span><span class="mw-editsection"></span></h3>
<p>The previous example is a two-pass sort: first sort, then merge. The sort ends with a single <i>k</i>-way merge, rather than a series of two-way merge passes as in a typical in-memory merge sort. This is because each merge pass reads and writes <i>every value</i> from and to disk, so reducing the number of passes more than compensates for the additional cost of a <i>k</i>-way merge.
</p><p>The limitation to single-pass merging is that as the number of chunks increases, memory will be divided into more buffers, so each buffer is smaller. Eventually, the reads become so small that more time is spent on disk seeks than data transfer.  A typical magnetic hard disk drive might have a 10 ms access time and 100 MB/s data transfer rate, so each seek takes as much time as transferring 1 MB of data.
</p><p>Thus, for sorting, say, 50 GB in 100 MB of RAM, using a single 500-way merge pass isn't efficient: we can only read 100 MB / 501 ≈ 200 KB from each chunk at once, so 5/6 of the disk's time is spent seeking.  Using two merge passes solves the problem.  Then the sorting process might look like this:
</p>
<ol><li>Run the initial chunk-sorting pass as before to create 500×100 MB sorted chunks.</li>
<li>Run a first merge pass combining 25×100 MB chunks at a time, resulting in 20×2.5 GB sorted chunks.</li>
<li>Run a second merge pass to merge the 20×2.5 GB sorted chunks into a single 50 GB sorted result</li></ol>
<p>Although this requires an additional pass over the data, each read is now 4 MB long, so only 1/5 of the disk's time is spent seeking.  The improvement in data transfer efficiency during the merge passes (16.6% to 80% is almost a 5× improvement) more than makes up for the doubled number of merge passes.
</p><p>Variations include using an intermediate medium like solid-state disk for some stages, even if there isn't enough of it to hold the full dataset. Repeating the example above with 20 GB of temporary storage on SSD, the first pass could merge 200×100 MB sorted chunks read from that temporary space to write 20GB sorted chunks to HDD. The 200-way merge with 500kb buffers is reasonably efficient due to the much greater random-read throughput of SSDs, the first pass can benefit from an SSD's higher read/write bandwidth, and the second pass has fewer larger chunks to merge and therefore fewer HDD seeks. SSDs can also be used as read buffers in a merge phase, allowing fewer larger reads from HDD storage. Given the relatively low cost of SSD space, it can be an economical tool for sorting large inputs with very limited memory.
</p><p>Like in-memory sorts, efficient external sorts require O(<i>n</i> log <i>n</i>) time: linear increases in data size require logarithmic increases in the number of passes, and each pass takes a linear number of reads and writes. Using the large memory sizes provided by modern computers the logarithmic factor grows very slowly. Under reasonable assumptions at least 500 GB of data stored on a hard drive can be sorted using 1 GB of main memory before a third pass becomes advantageous, and many times that much data can be sorted before a fourth pass becomes useful.<sup class="reference" id="cite_ref-4">[4]</sup> Media with high random-read performance like solid-state drives (SSDs) also increase the amount that can be sorted before additional passes improve performance.
</p><p>Main memory size is important. Doubling memory dedicated to sorting halves the number of chunks <i>and</i> the number of reads per chunk, reducing the number of seeks required by about three-quarters. The ratio of RAM to disk storage on servers often makes it convenient to do huge sorts on a cluster of machines<sup class="reference" id="cite_ref-5">[5]</sup> rather than on one machine with multiple passes.
</p>
<h2><span class="mw-headline" id="External_distribution_sort">External distribution sort</span><span class="mw-editsection"></span></h2>
<p>External distribution sort is analogous to quicksort. The algorithm finds approximately <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\tfrac {M}{B}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="false" scriptlevel="0">
<mfrac>
<mi>M</mi>
<mi>B</mi>
</mfrac>
</mstyle>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\tfrac {M}{B}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\tfrac {M}{B}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/860c96a1698eb35072253cf373ad394015efe974" style="vertical-align: -1.171ex; width:2.563ex; height:3.509ex;"/></span> pivots and uses them to divide the <span class="texhtml mvar" style="font-style:italic;">N</span> elements into approximately equally sized subarrays, each of whose elements are all smaller than the next, and then recurse until the sizes of the subarrays are less than the block size. When the subarrays are less than the block size, sorting can be done quickly because all reads and writes are done in the cache, and in the external memory model requires <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O(1)}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O(1)}</annotation>
</semantics>
</math></span><img alt="O(1)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e66384bc40452c5452f33563fe0e27e803b0cc21" style="vertical-align: -0.838ex; width:4.745ex; height:2.843ex;"/></span> operations.
</p><p>However, finding exactly <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\tfrac {M}{B}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="false" scriptlevel="0">
<mfrac>
<mi>M</mi>
<mi>B</mi>
</mfrac>
</mstyle>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\tfrac {M}{B}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\tfrac {M}{B}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/860c96a1698eb35072253cf373ad394015efe974" style="vertical-align: -1.171ex; width:2.563ex; height:3.509ex;"/></span> pivots would not be fast enough to make the external distribution sort asymptotically optimal. Instead, we find slightly fewer pivots. To find these pivots, the algorithm splits the <span class="texhtml mvar" style="font-style:italic;">N</span> input elements into <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\tfrac {N}{M}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="false" scriptlevel="0">
<mfrac>
<mi>N</mi>
<mi>M</mi>
</mfrac>
</mstyle>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\tfrac {N}{M}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\tfrac {N}{M}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b5d5ee01c2981ba95017bd5bdb588cb2316ca3e3" style="vertical-align: -1.171ex; width:2.563ex; height:3.509ex;"/></span> chunks, and takes every <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\sqrt {\tfrac {M}{16B}}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<msqrt>
<mstyle displaystyle="false" scriptlevel="0">
<mfrac>
<mi>M</mi>
<mrow>
<mn>16</mn>
<mi>B</mi>
</mrow>
</mfrac>
</mstyle>
</msqrt>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\sqrt {\tfrac {M}{16B}}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\sqrt {\tfrac {M}{16B}}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/224e39237bb73c547dcdd5569ceb993dd36bc062" style="vertical-align: -1.838ex; width:6.051ex; height:4.843ex;"/></span> elements, and recursively uses the median of medians algorithm to find <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\sqrt {\tfrac {M}{B}}}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mrow class="MJX-TeXAtom-ORD">
<msqrt>
<mstyle displaystyle="false" scriptlevel="0">
<mfrac>
<mi>M</mi>
<mi>B</mi>
</mfrac>
</mstyle>
</msqrt>
</mrow>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\sqrt {\tfrac {M}{B}}}}</annotation>
</semantics>
</math></span><img alt="{\displaystyle {\sqrt {\tfrac {M}{B}}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d831ae852c648eaefbe9c1336a15bdb6cc2d8ef1" style="vertical-align: -1.838ex; width:4.887ex; height:4.843ex;"/></span> pivots.<sup class="reference" id="cite_ref-Aggarwal88_6-0">[6]</sup>
</p><p>There is a duality, or fundamental similarity, between merge- and distribution-based algorithms.<sup class="reference" id="cite_ref-7">[7]</sup>
</p>
<h2><span class="mw-headline" id="Performance">Performance</span><span class="mw-editsection"></span></h2>
<p>The Sort Benchmark, created by computer scientist Jim Gray, compares external sorting algorithms implemented using finely tuned hardware and software.  Winning implementations use several techniques:
</p>
<ul><li><b>Using parallelism</b>
<ul><li>Multiple disk drives can be used in parallel in order to improve sequential read and write speed.  This can be a very cost-efficient improvement: a Sort Benchmark winner in the cost-centric Penny Sort category uses six hard drives in an otherwise midrange machine.<sup class="reference" id="cite_ref-8">[8]</sup></li>
<li>Sorting software can use multiple threads, to speed up the process on modern multicore computers.</li>
<li>Software can use asynchronous I/O so that one run of data can be sorted or merged while other runs are being read from or written to disk.</li>
<li>Multiple machines connected by fast network links can each sort part of a huge dataset in parallel.<sup class="reference" id="cite_ref-9">[9]</sup></li></ul></li>
<li><b>Increasing hardware speed</b>
<ul><li>Using more RAM for sorting can reduce the number of disk seeks and avoid the need for more passes.</li>
<li>Fast external memory like solid-state drives can speed sorts, either if the data is small enough to fit entirely on SSDs or, more rarely, to accelerate sorting SSD-sized chunks in a three-pass sort.</li>
<li><i>Many</i> other factors can affect hardware's maximum sorting speed: CPU speed and number of cores, RAM access latency, input/output bandwidth, disk read/write speed, disk seek time, and others. "Balancing" the hardware to minimize bottlenecks is an important part of designing an efficient sorting system.</li>
<li>Cost-efficiency as well as absolute speed can be critical, especially in cluster environments where lower node costs allow purchasing more nodes.</li></ul></li>
<li><b>Increasing software speed</b>
<ul><li>Some Sort Benchmark entrants use a variation on radix sort for the first phase of sorting: they separate data into one of many "bins" based on the beginning of its value.  Sort Benchmark data is random and especially well-suited to this optimization.</li>
<li>Compacting the input, intermediate files, and output can reduce time spent on I/O, but is not allowed in the Sort Benchmark.</li>
<li>Because the Sort Benchmark sorts long (100-byte) records using short (10-byte) keys, sorting software sometimes rearranges the keys separately from the values to reduce memory I/O volume.</li></ul></li></ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"></span></h2>
<ul><li>Mainframe sort merge</li>
<li>External memory algorithm</li>
<li>Funnelsort</li>
<li>Cache-oblivious distribution sort</li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"></span></h2>
<style data-mw-deduplicate="TemplateStyles:r1011085734">.mw-parser-output .reflist{font-size:90%;margin-bottom:0.5em;list-style-type:decimal}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}</style>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"></span></h2>
<ul><li>STXXL, an algorithm toolkit including external mergesort</li>
<li>An external mergesort example</li>
<li>A K-Way Merge Implementation</li>
<li>External-Memory Sorting in Java</li>
<li>A sample pennysort implementation using Judy Arrays</li>
<li>Sort Benchmark</li></ul>
<!-- 
NewPP limit report
Parsed by mw2314
Cached time: 20221224055803
Cache expiry: 1814400
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.152 seconds
Real time usage: 0.245 seconds
Preprocessor visited node count: 1327/1000000
Post‐expand include size: 7287/2097152 bytes
Template argument size: 1336/2097152 bytes
Highest expansion depth: 16/100
Expensive parser function count: 1/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 16165/5000000 bytes
Lua time usage: 0.057/10.000 seconds
Lua memory usage: 2790244/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  181.273      1 -total
 57.24%  103.768      1 Template:Reflist
 28.51%   51.690      1 Template:Short_description
 24.81%   44.965      1 Template:Cite_journal
 24.56%   44.514      4 Template:ISBN
 17.98%   32.595      4 Template:Catalog_lookup_link
 14.90%   27.008      2 Template:Pagetype
 10.69%   19.380      1 Template:See_also
  7.73%   14.004      7 Template:Main_other
  6.32%   11.465      1 Template:SDcat
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:1478246-0!canonical and timestamp 20221224055803 and revision id 1101440289.
 -->
</div></body>
</html>